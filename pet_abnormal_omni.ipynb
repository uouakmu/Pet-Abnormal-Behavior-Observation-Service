{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db64bf87",
   "metadata": {},
   "source": [
    "## Train\n",
    "---\n",
    "- ì‚¬ìš© ë°ì´í„°ì…‹\n",
    "    - 4_Animal_Skin\n",
    "    - 5_Animal_Eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_919331/1507650764.py:437: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 1/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 01/30:   0%|                                                         | 0/1563 [00:00<?, ?it/s]/tmp/ipykernel_919331/1507650764.py:466: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  [Skin ] Epoch 01/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:10<00:00, 12.02it/s, acc=53.20%, loss=1.4556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 01/30:   0%|                                                         | 0/2871 [00:00<?, ?it/s]/tmp/ipykernel_919331/1507650764.py:508: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "  [Eyes ] Epoch 01/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:50<00:00, 16.83it/s, acc=43.11%, loss=1.7032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 2/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 02/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:25<00:00, 10.77it/s, acc=65.56%, loss=1.2165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 02/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [03:01<00:00, 15.79it/s, acc=51.57%, loss=1.5037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 3/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 03/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:45<00:00,  9.45it/s, acc=71.90%, loss=1.0924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 03/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:49<00:00, 16.96it/s, acc=55.01%, loss=1.4199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 4/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 04/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:11<00:00, 11.89it/s, acc=75.96%, loss=1.0091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 04/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:52<00:00, 16.69it/s, acc=57.56%, loss=1.3545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 5/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 05/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:16<00:00, 11.45it/s, acc=79.38%, loss=0.9477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 05/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:50<00:00, 16.83it/s, acc=59.80%, loss=1.3000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 6/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 06/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:21<00:00, 11.06it/s, acc=82.12%, loss=0.8968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 06/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:58<00:00, 16.12it/s, acc=61.33%, loss=1.2595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 7/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 07/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:47<00:00,  9.33it/s, acc=84.26%, loss=0.8521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 07/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [02:50<00:00, 16.80it/s, acc=62.96%, loss=1.2230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 8/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 08/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [02:28<00:00, 10.54it/s, acc=86.23%, loss=0.8188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 08/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [03:06<00:00, 15.37it/s, acc=63.95%, loss=1.1935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 9/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 09/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [03:46<00:00,  6.90it/s, acc=87.78%, loss=0.7876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 09/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [03:13<00:00, 14.81it/s, acc=65.07%, loss=1.1674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 10/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [03:42<00:00,  7.03it/s, acc=89.42%, loss=0.7577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [03:26<00:00, 13.92it/s, acc=66.08%, loss=1.1422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 11/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1563/1563 [03:38<00:00,  7.17it/s, acc=91.06%, loss=0.7276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/2] Eyes Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Eyes ] Epoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2871/2871 [03:24<00:00, 14.06it/s, acc=66.89%, loss=1.1195]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Epoch 12/30 =========\n",
      "\n",
      "[1/2] Skin Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  [Skin ] Epoch 12/30:  12%|â–ˆâ–ˆâ–Œ                   | 181/1563 [00:29<02:21,  9.75it/s, acc=93.02%, loss=0.6966]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 24\n",
    "LR = 1e-4\n",
    "NUM_IMAGES_PER_SAMPLE = 5          # ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•˜ëŠ” ì‚¬ì§„ ìˆ˜\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CLASS DEFINITIONS\n",
    "# ê·œì¹™: dog_ ì ‘ë‘ â†’ dog classes, cat_ ì ‘ë‘ â†’ cat classes\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# â”€â”€ 4_Animal_Skin â”€â”€\n",
    "SKIN_CLASSES = [\n",
    "    \"cat_normal\", \"cat_ê²°ì ˆ,ì¢…ê´´\", \"cat_ë†í¬,ì—¬ë“œë¦„\",\n",
    "    \"cat_ë¹„ë“¬,ê°ì§ˆ,ìƒí”¼ì„±ì”ê³ ë¦¬\", \"dog_normal\",\n",
    "    \"dog_ê²°ì ˆ,ì¢…ê´´\", \"dog_ë†í¬,ì—¬ë“œë¦„\", \"dog_ë¯¸ë€,ê¶¤ì–‘\",\n",
    "    \"dog_ë¹„ë“¬,ê°ì§ˆ,ìƒí”¼ì„±ì”ê³ ë¦¬\", \"dog_ë¹„ë“¬,ê°ì§ˆ,ìƒí”¼ì„±ì”ê³ ë¦¬\",\n",
    "]\n",
    "\n",
    "# â”€â”€ 5_Animal_Eyes â”€â”€\n",
    "EYES_CLASSES = [\n",
    "    \"cat_normal\", \"cat_ê°ë§‰ê¶¤ì–‘\", \"cat_ê°ë§‰ë¶€ê³¨í¸\",\n",
    "    \"cat_ê²°ë§‰ì—¼\", \"cat_ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì—¼\", \"cat_ì•ˆê²€ì—¼\",\n",
    "    \"dog_normal\", \"dog_ê²°ë§‰ì—¼\", \"dog_ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_ìƒ\",\n",
    "    \"dog_ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_í•˜\", \"dog_ë°±ë‚´ì¥_ë¹„ì„±ìˆ™\", \"dog_ë°±ë‚´ì¥_ì„±ìˆ™\",\n",
    "    \"dog_ë°±ë‚´ì¥_ì´ˆê¸°\", \"dog_ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_ìƒ\", \"dog_ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_í•˜\",\n",
    "    \"dog_ìƒ‰ì†Œì¹¨ì°©ì„±ê°ë§‰ì—¼\", \"dog_ì•ˆê²€ë‚´ë°˜ì¦\", \"dog_ì•ˆê²€ì—¼\",\n",
    "    \"dog_ì•ˆê²€ì¢…ì–‘\", \"dog_ìœ ë£¨ì¦\", \"dog_í•µê²½í™”\"\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ìœ ì‚¬ í´ë˜ìŠ¤ ê·¸ë£¹ ì •ì˜ (Eyes ì „ìš©)\n",
    "# ë™ì¼ ì§ˆí™˜ ë‚´ ì„¸ë¶„ë¥˜ëŠ” Hierarchical Loss ê°€ì¤‘ì¹˜ë¡œ í˜¼ë™ íŒ¨ë„í‹°ë¥¼ ì¤Œ\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EYES_SIMILAR_GROUPS = [\n",
    "    [\"dog_ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_ìƒ\", \"dog_ë¹„ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_í•˜\"],\n",
    "    [\"dog_ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_ìƒ\", \"dog_ê¶¤ì–‘ì„±ê°ë§‰ì§ˆí™˜_í•˜\"],\n",
    "    [\"dog_ë°±ë‚´ì¥_ì´ˆê¸°\", \"dog_ë°±ë‚´ì¥_ë¹„ì„±ìˆ™\", \"dog_ë°±ë‚´ì¥_ì„±ìˆ™\"],\n",
    "]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# LOSS: Hierarchical-Aware CE\n",
    "# ===============================\n",
    "# ê°™ì€ ì§ˆí™˜ ê·¸ë£¹ ë‚´ ì˜¤ë¶„ë¥˜ì— extra_penalty ë¥¼ ê³±í•´\n",
    "# ëª¨ë¸ì´ ìƒ/í•˜, ì´ˆê¸°/ì„±ìˆ™ êµ¬ë¶„ì„ ë” ì—´ì‹¬íˆ í•™ìŠµí•˜ê²Œ ë§Œë“ ë‹¤.\n",
    "\n",
    "class HierarchicalWeightedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss + Label Smoothing + ìœ ì‚¬ í´ë˜ìŠ¤ í˜¼ë™ í˜ë„í‹°\n",
    "\n",
    "    Args:\n",
    "        class_names    : í•™ìŠµ taskì— í•´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸\n",
    "        similar_groups : ìœ ì‚¬ í´ë˜ìŠ¤ ë¬¶ìŒ [[cls_a, cls_b], ...]\n",
    "        class_weights  : í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • weight í…ì„œ\n",
    "        smoothing      : label smoothing Îµ\n",
    "        extra_penalty  : ê°™ì€ ê·¸ë£¹ ë‚´ ì˜¤ë¶„ë¥˜ ì‹œ loss ë°°ìœ¨\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        class_names,\n",
    "        similar_groups=None,\n",
    "        class_weights=None,\n",
    "        smoothing=LABEL_SMOOTHING,\n",
    "        extra_penalty=1.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.smoothing      = smoothing\n",
    "        self.extra_penalty  = extra_penalty\n",
    "        self.num_classes    = len(class_names)\n",
    "        self.class_names    = class_names\n",
    "        self.name_to_idx    = {n: i for i, n in enumerate(class_names)}\n",
    "\n",
    "        # ìœ ì‚¬ ê·¸ë£¹ â†’ (idx_i, idx_j) pair set\n",
    "        self.penalty_pairs = set()\n",
    "        if similar_groups:\n",
    "            for group in similar_groups:\n",
    "                idxs = [self.name_to_idx[n] for n in group if n in self.name_to_idx]\n",
    "                for i in range(len(idxs)):\n",
    "                    for j in range(i + 1, len(idxs)):\n",
    "                        self.penalty_pairs.add((idxs[i], idxs[j]))\n",
    "                        self.penalty_pairs.add((idxs[j], idxs[i]))\n",
    "\n",
    "        self.register_buffer(\"weight\", class_weights)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        logits  : (B, C)\n",
    "        targets : (B,)  long\n",
    "        \"\"\"\n",
    "        B, C = logits.shape\n",
    "        device = logits.device\n",
    "\n",
    "        # â”€â”€ Label Smoothing â”€â”€\n",
    "        log_prob = F.log_softmax(logits, dim=-1)\n",
    "        smooth_loss = -log_prob.mean(dim=-1)                              # (B,)\n",
    "        nll_loss    = F.nll_loss(log_prob, targets, weight=self.weight, reduction=\"none\")  # (B,)\n",
    "        base_loss   = (1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss  # (B,)\n",
    "\n",
    "        # â”€â”€ Hierarchical Penalty â”€â”€\n",
    "        if self.penalty_pairs:\n",
    "            pred_classes = logits.argmax(dim=-1)          # (B,)\n",
    "            penalty_mask = torch.ones(B, device=device)\n",
    "            for b in range(B):\n",
    "                t = targets[b].item()\n",
    "                p = pred_classes[b].item()\n",
    "                if (t, p) in self.penalty_pairs:\n",
    "                    penalty_mask[b] = self.extra_penalty\n",
    "            base_loss = base_loss * penalty_mask\n",
    "\n",
    "        return base_loss.mean()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# CLASS WEIGHT COMPUTATION\n",
    "# ===============================\n",
    "\n",
    "def compute_class_weights(sample_counts: dict, class_names: list) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse-frequency ë°©ì‹ìœ¼ë¡œ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•œë‹¤.\n",
    "    sample_counts: {class_name: n_samples}\n",
    "    \"\"\"\n",
    "    counts = torch.tensor(\n",
    "        [sample_counts.get(n, 1) for n in class_names], dtype=torch.float\n",
    "    )\n",
    "    weights = 1.0 / counts\n",
    "    weights = weights / weights.sum() * len(class_names)   # normalize\n",
    "    return weights\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# MODEL DEFINITIONS\n",
    "# ===============================\n",
    "\n",
    "class AnomalyMultiBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    ì´ìƒ ì¦ìƒ Omni ëª¨ë¸\n",
    "    â”œâ”€â”€ skin_backbone  â†’ Skin ë¶„ë¥˜ (í”¼ë¶€ì§ˆí™˜)\n",
    "    â””â”€â”€ eyes_backbone  â†’ Eyes ë¶„ë¥˜ (ì•ˆêµ¬ì§ˆí™˜)\n",
    "\n",
    "    ê° backbone ì€ ResNet50 (ImageNet pretrained) ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°,\n",
    "    ë§ˆì§€ë§‰ fc ë¥¼ task-specific head ë¡œ êµì²´í•œë‹¤.\n",
    "\n",
    "    Eyes ì˜ ê²½ìš° ìœ ì‚¬ í´ë˜ìŠ¤ í˜¼ë™ì„ ì¤„ì´ê¸° ìœ„í•´:\n",
    "      1) Dropout + ë” ê¹Šì€ head\n",
    "      2) Feature Attention (Channel Squeeze-Excitation)\n",
    "    ì„ ì¶”ê°€í•œë‹¤.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_skin_classes: int, num_eyes_classes: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # â”€â”€ Skin Backbone (ResNet50 pretrained) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        skin_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        skin_feat_dim = skin_base.fc.in_features          # 2048\n",
    "        skin_base.fc = nn.Identity()\n",
    "        self.skin_backbone = skin_base\n",
    "        self.skin_head = nn.Sequential(\n",
    "            nn.Linear(skin_feat_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_skin_classes),\n",
    "        )\n",
    "\n",
    "        # â”€â”€ Eyes Backbone (ResNet50 pretrained + SE attention) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        eyes_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        eyes_feat_dim = eyes_base.fc.in_features\n",
    "        eyes_base.fc = nn.Identity()\n",
    "        self.eyes_backbone = eyes_base\n",
    "\n",
    "        # Squeeze-Excitation: ì±„ë„ ì¤‘ìš”ë„ ì¬ë³´ì • â†’ ë¯¸ì„¸í•œ ë³‘ë³€ êµ¬ë¶„ë ¥ í–¥ìƒ\n",
    "        self.eyes_se = SqueezeExcitation(eyes_feat_dim, reduction=16)\n",
    "\n",
    "        # ë” ê¹Šì€ classifier head\n",
    "        self.eyes_head = nn.Sequential(\n",
    "            nn.Linear(eyes_feat_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_eyes_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, task: str = \"skin\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x    : (B, 3, 224, 224)  â€” ë‹¨ì¼ ì´ë¯¸ì§€ ë˜ëŠ” ì•™ìƒë¸” í›„ í‰ê·  logit ìš©\n",
    "        task : \"skin\" | \"eyes\"\n",
    "        \"\"\"\n",
    "        if task == \"skin\":\n",
    "            feat = self.skin_backbone(x)\n",
    "            return self.skin_head(feat)\n",
    "\n",
    "        elif task == \"eyes\":\n",
    "            feat = self.eyes_backbone(x)           # (B, 2048)\n",
    "            feat = self.eyes_se(feat)              # channel attention\n",
    "            return self.eyes_head(feat)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {task!r}. Choose 'skin' or 'eyes'.\")\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    \"\"\"\n",
    "    1-D Squeeze-Excitation for feature vectors (after global avg pool).\n",
    "    feat : (B, C)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# INFERENCE: 5-Image Ensemble\n",
    "# ===============================\n",
    "\n",
    "def predict_anomaly(\n",
    "    model: AnomalyMultiBackbone,\n",
    "    images: list,           # list of PIL.Image (5ì¥)\n",
    "    task: str,              # \"skin\" | \"eyes\"\n",
    "    pet_type: str,          # \"dog\" | \"cat\"\n",
    "    class_names: list,\n",
    "    device=DEVICE,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    5ì¥ì˜ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ í‰ê·  softmax í™•ë¥ ë¡œ ìµœì¢… ì˜ˆì¸¡ì„ ë°˜í™˜í•œë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"predicted_class\": str,\n",
    "            \"confidence\": float,\n",
    "            \"top3\": [(class_name, prob), ...]\n",
    "        }\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # ë°˜ë ¤ë™ë¬¼ ì¢…ì— ë§ëŠ” class indexë§Œ ì„ íƒ\n",
    "    valid_idxs = [\n",
    "        i for i, n in enumerate(class_names) if n.startswith(pet_type + \"_\")\n",
    "    ]\n",
    "    valid_names = [class_names[i] for i in valid_idxs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        probs_accum = torch.zeros(len(class_names), device=device)\n",
    "\n",
    "        for img in images:\n",
    "            tensor = transform(img).unsqueeze(0).to(device)   # (1, 3, 224, 224)\n",
    "            logits = model(tensor, task=task)                  # (1, C)\n",
    "\n",
    "            # í•´ë‹¹ pet_type ì™¸ class ë§ˆìŠ¤í‚¹ (âˆ’inf â†’ softmax â‰ˆ 0)\n",
    "            mask = torch.full((len(class_names),), float(\"-inf\"), device=device)\n",
    "            mask[valid_idxs] = logits[0][valid_idxs]\n",
    "\n",
    "            probs = F.softmax(mask, dim=-1)\n",
    "            probs_accum += probs\n",
    "\n",
    "        probs_accum /= len(images)    # í‰ê·  ì•™ìƒë¸”\n",
    "\n",
    "    # ìœ íš¨ class ì¤‘ top-k\n",
    "    valid_probs = [(valid_names[i], probs_accum[valid_idxs[i]].item())\n",
    "                   for i in range(len(valid_idxs))]\n",
    "    valid_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return {\n",
    "        \"predicted_class\": valid_probs[0][0],\n",
    "        \"confidence\":      valid_probs[0][1],\n",
    "        \"top3\":            valid_probs[:3],\n",
    "    }\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# DATASETS\n",
    "# ===============================\n",
    "\n",
    "class AnomalyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°ì…‹ êµ¬ì¡°:\n",
    "        root_dir/\n",
    "            dog_ê²°ë§‰ì—¼/  img001.jpg ...\n",
    "            cat_normal/  img001.jpg ...\n",
    "            ...\n",
    "\n",
    "    task      : \"skin\" | \"eyes\"\n",
    "    pet_type  : \"dog\" | \"cat\" | \"all\"\n",
    "    \"\"\"\n",
    "\n",
    "    TRANSFORM = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    TRANSFORM_VAL = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        class_names: list,\n",
    "        task: str,\n",
    "        is_train: bool = True,\n",
    "    ):\n",
    "        self.class_names = class_names\n",
    "        self.task        = task\n",
    "        self.transform   = self.TRANSFORM if is_train else self.TRANSFORM_VAL\n",
    "        self.name_to_idx = {n: i for i, n in enumerate(class_names)}\n",
    "\n",
    "        self.samples = []   # [(img_path, label_idx), ...]\n",
    "\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            label_idx = self.name_to_idx[class_name]\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(class_dir, fname), label_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        return self.transform(img), label\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sample_counts(root_dir: str, class_names: list) -> dict:\n",
    "        counts = {}\n",
    "        for cn in class_names:\n",
    "            d = os.path.join(root_dir, cn)\n",
    "            if os.path.isdir(d):\n",
    "                counts[cn] = len([\n",
    "                    f for f in os.listdir(d)\n",
    "                    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "                ])\n",
    "            else:\n",
    "                counts[cn] = 1\n",
    "        return counts\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# TRAIN FUNCTION\n",
    "# ===============================\n",
    "\n",
    "def train(\n",
    "    skin_root: str = \"files/4_Animal_Skin\",\n",
    "    eyes_root: str = \"files/5_Animal_Eyes\",\n",
    "):\n",
    "    # â”€â”€ í´ë˜ìŠ¤ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    skin_classes = SKIN_CLASSES\n",
    "    eyes_classes = EYES_CLASSES\n",
    "\n",
    "    num_skin  = len(skin_classes)\n",
    "    num_eyes  = len(eyes_classes)\n",
    "\n",
    "    # â”€â”€ ëª¨ë¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model = AnomalyMultiBackbone(num_skin, num_eyes)\n",
    "\n",
    "    # â”€â”€ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (ë¶ˆê· í˜• ë³´ì •) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    skin_counts  = AnomalyDataset.get_sample_counts(skin_root, skin_classes)\n",
    "    eyes_counts  = AnomalyDataset.get_sample_counts(eyes_root, eyes_classes)\n",
    "\n",
    "    skin_weights = compute_class_weights(skin_counts, skin_classes).to(DEVICE)\n",
    "    eyes_weights = compute_class_weights(eyes_counts, eyes_classes).to(DEVICE)\n",
    "\n",
    "    # â”€â”€ Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    skin_criterion = HierarchicalWeightedLoss(\n",
    "        class_names   = skin_classes,\n",
    "        class_weights = skin_weights,\n",
    "        smoothing     = LABEL_SMOOTHING,\n",
    "    )\n",
    "    eyes_criterion = HierarchicalWeightedLoss(\n",
    "        class_names    = eyes_classes,\n",
    "        similar_groups = EYES_SIMILAR_GROUPS,\n",
    "        class_weights  = eyes_weights,\n",
    "        smoothing      = LABEL_SMOOTHING,\n",
    "        extra_penalty  = 1.5,\n",
    "    )\n",
    "\n",
    "    # â”€â”€ Optimizer & Scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # â”€â”€ Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n========= Epoch {epoch + 1}/{EPOCHS} =========\\n\")\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 1ï¸âƒ£  Skin Training\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"[1/2] Skin Training\")\n",
    "        model.to(DEVICE)\n",
    "        model.train()\n",
    "\n",
    "        skin_dataset = AnomalyDataset(skin_root, skin_classes, task=\"skin\", is_train=True)\n",
    "        skin_loader  = DataLoader(\n",
    "            skin_dataset,\n",
    "            batch_size  = BATCH_SIZE,\n",
    "            shuffle     = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "\n",
    "        skin_loss_sum, skin_correct, skin_total = 0.0, 0, 0\n",
    "\n",
    "        skin_pbar = tqdm(skin_loader, desc=f\"  [Skin ] Epoch {epoch+1:02d}/{EPOCHS}\", ncols=110, leave=True)\n",
    "        for images, labels in skin_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(images, task=\"skin\")\n",
    "                loss    = skin_criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            skin_loss_sum += loss.item() * images.size(0)\n",
    "            skin_correct  += (outputs.argmax(1) == labels).sum().item()\n",
    "            skin_total    += images.size(0)\n",
    "\n",
    "            skin_pbar.set_postfix(\n",
    "                loss=f\"{skin_loss_sum / skin_total:.4f}\",\n",
    "                acc=f\"{100 * skin_correct / skin_total:.2f}%\"\n",
    "            )\n",
    "\n",
    "        del skin_loader, skin_dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 2ï¸âƒ£  Eyes Training\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(\"[2/2] Eyes Training\")\n",
    "\n",
    "        eyes_dataset = AnomalyDataset(eyes_root, eyes_classes, task=\"eyes\", is_train=True)\n",
    "        eyes_loader  = DataLoader(\n",
    "            eyes_dataset,\n",
    "            batch_size  = BATCH_SIZE,\n",
    "            shuffle     = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory  = True,\n",
    "        )\n",
    "\n",
    "        eyes_loss_sum, eyes_correct, eyes_total = 0.0, 0, 0\n",
    "\n",
    "        eyes_pbar = tqdm(eyes_loader, desc=f\"  [Eyes ] Epoch {epoch+1:02d}/{EPOCHS}\", ncols=110, leave=True)\n",
    "        for images, labels in eyes_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(images, task=\"eyes\")\n",
    "                loss    = eyes_criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            eyes_loss_sum += loss.item() * images.size(0)\n",
    "            eyes_correct  += (outputs.argmax(1) == labels).sum().item()\n",
    "            eyes_total    += images.size(0)\n",
    "\n",
    "            eyes_pbar.set_postfix(\n",
    "                loss=f\"{eyes_loss_sum / eyes_total:.4f}\",\n",
    "                acc=f\"{100 * eyes_correct / eyes_total:.2f}%\"\n",
    "            )\n",
    "\n",
    "        del eyes_loader, eyes_dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # â”€â”€ LR Scheduler Step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        scheduler.step()\n",
    "\n",
    "        # â”€â”€ Checkpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"epoch\": epoch + 1},\n",
    "            f\"anomaly_checkpoint_epoch_{epoch + 1}.pth\",\n",
    "        )\n",
    "\n",
    "    print(\"\\nTraining Finished.\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# ENTRY POINT\n",
    "# ===============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23710186",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55420461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import resnet34, ResNet34_Weights\n",
    "# from PIL import Image\n",
    "# from PIL import ImageFile\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# # =========================\n",
    "# # 0. ì„¤ì •\n",
    "# # =========================\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# SKIN_ROOT = \"files/4_Animal_Skin\"\n",
    "# EYES_ROOT = \"files/5_Animal_Eyes\"\n",
    "# WORK_DIR = \"files/work/disease_dataset\"\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# EPOCHS = 50\n",
    "# LR_INITIAL = 1e-4\n",
    "# DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# NUM_WORKERS = 16\n",
    "\n",
    "# PATIENCE = 10\n",
    "# MIN_DELTA = 0.001\n",
    "\n",
    "# MIXUP_ALPHA = 0\n",
    "\n",
    "# print(f\"ğŸ¯ Device: {DEVICE}\")\n",
    "# print(f\"âš¡ Batch Size: {BATCH_SIZE}, Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# # =========================\n",
    "# # 1. Dataset Preparation\n",
    "# # =========================\n",
    "# def collect_samples(root):\n",
    "#     samples = []\n",
    "#     for class_dir in sorted(os.listdir(root)):\n",
    "#         class_path = os.path.join(root, class_dir)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "#         for file in os.listdir(class_path):\n",
    "#             if file.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "#                 samples.append((class_dir, os.path.join(class_path, file)))\n",
    "#     print(f\"  â†’ {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "#     return samples\n",
    "\n",
    "# def split_and_copy(samples, task_name):\n",
    "#     random.shuffle(samples)\n",
    "#     class_samples = defaultdict(list)\n",
    "#     for label, path in samples:\n",
    "#         class_samples[label].append(path)\n",
    "\n",
    "#     for split in [\"train\",\"val\",\"test\"]:\n",
    "#         os.makedirs(os.path.join(WORK_DIR,split,task_name),exist_ok=True)\n",
    "\n",
    "#     for label, paths in class_samples.items():\n",
    "#         n = len(paths)\n",
    "#         n_train = int(n*0.8)\n",
    "#         n_val = int(n*0.1)\n",
    "\n",
    "#         splits = {\n",
    "#             \"train\": paths[:n_train],\n",
    "#             \"val\": paths[n_train:n_train+n_val],\n",
    "#             \"test\": paths[n_train+n_val:]\n",
    "#         }\n",
    "\n",
    "#         for split_name, split_paths in splits.items():\n",
    "#             for src in tqdm(split_paths, desc=f\"{task_name}/{split_name}/{label}\", leave=False):\n",
    "#                 dst_dir = os.path.join(WORK_DIR,split_name,task_name,label)\n",
    "#                 os.makedirs(dst_dir,exist_ok=True)\n",
    "#                 shutil.copy(src, os.path.join(dst_dir, os.path.basename(src)))\n",
    "\n",
    "# def prepare_dataset():\n",
    "#     if os.path.exists(WORK_DIR):\n",
    "#         shutil.rmtree(WORK_DIR)\n",
    "\n",
    "#     print(\"\\nğŸ“¦ Collecting Skin Disease...\")\n",
    "#     skin = collect_samples(SKIN_ROOT)\n",
    "#     split_and_copy(skin,\"skin\")\n",
    "\n",
    "#     print(\"\\nğŸ“¦ Collecting Eye Disease...\")\n",
    "#     eyes = collect_samples(EYES_ROOT)\n",
    "#     split_and_copy(eyes,\"eyes\")\n",
    "\n",
    "#     print(\"\\nâœ… Dataset ready\")\n",
    "\n",
    "# # =========================\n",
    "# # 2. Dataset Class\n",
    "# # =========================\n",
    "# class ImageDataset(Dataset):\n",
    "#     def __init__(self, task_dir, augment=False):\n",
    "#         self.samples = []\n",
    "#         self.label_to_id = {}\n",
    "\n",
    "#         for label in sorted(os.listdir(task_dir)):\n",
    "#             label_dir = os.path.join(task_dir,label)\n",
    "#             if not os.path.isdir(label_dir):\n",
    "#                 continue\n",
    "#             self.label_to_id[label] = len(self.label_to_id)\n",
    "#             for file in os.listdir(label_dir):\n",
    "#                 if file.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "#                     self.samples.append((os.path.join(label_dir,file),label))\n",
    "\n",
    "#         print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "\n",
    "#         if augment:\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((256,256)),\n",
    "#                 transforms.RandomCrop(224),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.RandomRotation(15),\n",
    "#                 transforms.ColorJitter(0.3,0.3,0.3),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "#             ])\n",
    "#         else:\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((224,224)),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "#             ])\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "\n",
    "#     def __getitem__(self,idx):\n",
    "#         path,label = self.samples[idx]\n",
    "#         img = Image.open(path).convert(\"RGB\")\n",
    "#         img = self.transform(img)\n",
    "#         return img, self.label_to_id[label]\n",
    "\n",
    "# # =========================\n",
    "# # 3. Disease Omni Model\n",
    "# # =========================\n",
    "# class DiseaseOmni(nn.Module):\n",
    "#     def __init__(self, num_skin, num_eyes):\n",
    "#         super().__init__()\n",
    "\n",
    "#         skin_backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_s = skin_backbone.fc.in_features\n",
    "#         skin_backbone.fc = nn.Identity()\n",
    "#         self.skin_backbone = skin_backbone\n",
    "#         self.skin_head = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(in_features_s, num_skin)\n",
    "#         )\n",
    "\n",
    "#         eyes_backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_e = eyes_backbone.fc.in_features\n",
    "#         eyes_backbone.fc = nn.Identity()\n",
    "#         self.eyes_backbone = eyes_backbone\n",
    "#         self.eyes_head = nn.Sequential(\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(in_features_e, num_eyes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self,x,task):\n",
    "#         if task==\"skin\":\n",
    "#             feat = self.skin_backbone(x)\n",
    "#             return self.skin_head(feat)\n",
    "#         elif task==\"eyes\":\n",
    "#             feat = self.eyes_backbone(x)\n",
    "#             return self.eyes_head(feat)\n",
    "#         else:\n",
    "#             raise ValueError(\"Task must be 'skin' or 'eyes'\")\n",
    "\n",
    "# # =========================\n",
    "# # 4. Early Stopping\n",
    "# # =========================\n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, patience=10, min_delta=0.001):\n",
    "#         self.patience = patience\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.best_score = None\n",
    "#         self.early_stop = False\n",
    "        \n",
    "#     def __call__(self, val_acc):\n",
    "#         if self.best_score is None:\n",
    "#             self.best_score = val_acc\n",
    "#         elif val_acc < self.best_score + self.min_delta:\n",
    "#             self.counter += 1\n",
    "#             print(f\"  âš ï¸  EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_score = val_acc\n",
    "#             self.counter = 0\n",
    "\n",
    "# # =========================\n",
    "# # 5. Training\n",
    "# # =========================\n",
    "# def train():\n",
    "#     prepare_dataset()\n",
    "\n",
    "#     print(\"\\nğŸ”„ Loading datasets...\")\n",
    "#     skin_train = ImageDataset(os.path.join(WORK_DIR,\"train\",\"skin\"), augment=True)\n",
    "#     skin_val   = ImageDataset(os.path.join(WORK_DIR,\"val\",\"skin\"), augment=False)\n",
    "\n",
    "#     eyes_train = ImageDataset(os.path.join(WORK_DIR,\"train\",\"eyes\"), augment=True)\n",
    "#     eyes_val   = ImageDataset(os.path.join(WORK_DIR,\"val\",\"eyes\"), augment=False)\n",
    "\n",
    "#     # Eyes sampler (ë¶ˆê· í˜• ë³´ì •)\n",
    "#     eyes_labels = [eyes_train.label_to_id[label] for _, label in eyes_train.samples]\n",
    "#     class_sample_count = np.array([eyes_labels.count(i) for i in range(len(eyes_train.label_to_id))])\n",
    "#     weights = 1. / class_sample_count\n",
    "#     samples_weight = np.array([weights[t] for t in eyes_labels])\n",
    "#     samples_weight = torch.from_numpy(samples_weight).double()\n",
    "#     eyes_sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "#     skin_train_loader = DataLoader(\n",
    "#         skin_train, BATCH_SIZE, True,\n",
    "#         num_workers=NUM_WORKERS, pin_memory=True,\n",
    "#         persistent_workers=True\n",
    "#     )\n",
    "\n",
    "#     eyes_train_loader = DataLoader(\n",
    "#         eyes_train, BATCH_SIZE, sampler=eyes_sampler,\n",
    "#         num_workers=NUM_WORKERS, pin_memory=True,\n",
    "#         persistent_workers=True\n",
    "#     )\n",
    "\n",
    "#     skin_val_loader = DataLoader(skin_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2)\n",
    "#     eyes_val_loader = DataLoader(eyes_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2)\n",
    "\n",
    "#     print(\"\\nâš–ï¸  Computing class weights...\")\n",
    "#     skin_labels = [skin_train.label_to_id[label] for _, label in skin_train.samples]\n",
    "#     skin_weights = compute_class_weight('balanced', classes=np.arange(len(skin_train.label_to_id)), y=skin_labels)\n",
    "#     skin_weights_tensor = torch.tensor(skin_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "#     eyes_weights = compute_class_weight('balanced', classes=np.arange(len(eyes_train.label_to_id)), y=eyes_labels)\n",
    "#     eyes_weights_tensor = torch.tensor(eyes_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "#     model = DiseaseOmni(\n",
    "#         len(skin_train.label_to_id),\n",
    "#         len(eyes_train.label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR_INITIAL, weight_decay=1e-4)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "\n",
    "#     criterion_skin = nn.CrossEntropyLoss(weight=skin_weights_tensor, label_smoothing=0.05)\n",
    "#     criterion_eyes = nn.CrossEntropyLoss(weight=eyes_weights_tensor, label_smoothing=0.05)\n",
    "\n",
    "#     early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "#     best_avg_acc = 0\n",
    "#     history = []\n",
    "\n",
    "#     for epoch in range(EPOCHS):\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "#         print(f\"{'='*60}\")\n",
    "\n",
    "#         model.train()\n",
    "#         loss_s_total, loss_e_total = 0, 0\n",
    "\n",
    "#         # ------------------ Skin ------------------\n",
    "#         loop = tqdm(skin_train_loader, desc=f\"Skin Train {epoch+1}\", leave=False)\n",
    "\n",
    "#         for imgs, labels in loop:\n",
    "#             imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(imgs, \"skin\")\n",
    "#             loss = criterion_skin(logits, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_s_total += loss.item()\n",
    "\n",
    "#             # ğŸ”¥ ì‹¤ì‹œê°„ loss í‘œì‹œ\n",
    "#             loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#         # ------------------ Eyes ------------------\n",
    "#         loop = tqdm(eyes_train_loader, desc=f\"Eyes Train {epoch+1}\", leave=False)\n",
    "\n",
    "#         for imgs, labels in loop:\n",
    "#             imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             logits = model(imgs, \"eyes\")\n",
    "#             loss = criterion_eyes(logits, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             loss_e_total += loss.item()\n",
    "\n",
    "#             loop.set_postfix(loss=loss.item())\n",
    "\n",
    "#         # ------------------ Validation ------------------\n",
    "#         model.eval()\n",
    "#         correct_s = correct_e = total_s = total_e = 0\n",
    "\n",
    "#         with torch.no_grad():\n",
    "\n",
    "#             for imgs, labels in tqdm(skin_val_loader, \n",
    "#                                     desc=f\"Skin Val {epoch+1}\", \n",
    "#                                     leave=False):\n",
    "#                 imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "#                 pred = model(imgs,\"skin\").argmax(-1)\n",
    "#                 correct_s += (pred==labels).sum().item()\n",
    "#                 total_s += labels.size(0)\n",
    "\n",
    "#             for imgs, labels in tqdm(eyes_val_loader, \n",
    "#                                     desc=f\"Eyes Val {epoch+1}\", \n",
    "#                                     leave=False):\n",
    "#                 imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "#                 pred = model(imgs,\"eyes\").argmax(-1)\n",
    "#                 correct_e += (pred==labels).sum().item()\n",
    "#                 total_e += labels.size(0)\n",
    "\n",
    "#         acc_s = correct_s/total_s\n",
    "#         acc_e = correct_e/total_e\n",
    "#         avg_acc = (acc_s + acc_e)/2\n",
    "\n",
    "#         print(f\"Skin Acc: {acc_s:.4f} | Eyes Acc: {acc_e:.4f} | Avg: {avg_acc:.4f}\")\n",
    "\n",
    "#         if avg_acc > best_avg_acc:\n",
    "#             best_avg_acc = avg_acc\n",
    "#             torch.save(model.state_dict(), \"pet_disease_omni_best.pth\")\n",
    "#             print(\"ğŸ’¾ Saved best model\")\n",
    "\n",
    "#         early_stopping(avg_acc)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"ğŸ›‘ Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     print(f\"\\nğŸ‰ Training Finished | Best Avg Acc: {best_avg_acc:.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032db127",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import resnet34, ResNet34_Weights\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# from collections import defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# # =========================\n",
    "# # ëª¨ë¸ í´ë˜ìŠ¤ (trainê³¼ ë™ì¼)\n",
    "# # =========================\n",
    "# class AbnormalityOmni(nn.Module):\n",
    "#     def __init__(self, num_classes_per_task):\n",
    "#         super().__init__()\n",
    "#         self.trunk = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features = self.trunk.fc.in_features\n",
    "#         self.trunk.fc = nn.Identity()\n",
    "        \n",
    "#         self.pet_head = nn.Linear(in_features, 2)  # dog/cat\n",
    "        \n",
    "#         self.task_heads = nn.ModuleDict({\n",
    "#             task: nn.Linear(in_features, num_classes) \n",
    "#             for task, num_classes in num_classes_per_task.items()\n",
    "#         })\n",
    "    \n",
    "#     def forward(self, x, task):\n",
    "#         if x.dim() == 5:  # 1x5x3x224x224\n",
    "#             B = x.size(0)\n",
    "#             x = x.view(-1, *x.shape[2:])\n",
    "        \n",
    "#         feats = self.trunk(x)\n",
    "#         if x.size(0) != feats.size(0):\n",
    "#             feats = feats.view(B, -1, feats.size(-1)).mean(1)\n",
    "        \n",
    "#         pet_logits = self.pet_head(feats)\n",
    "#         task_logits = self.task_heads[task](feats)\n",
    "        \n",
    "#         return {\n",
    "#             'pet': F.softmax(pet_logits, dim=-1),\n",
    "#             task: task_logits\n",
    "#         }\n",
    "\n",
    "# # =========================\n",
    "# # Test Dataset (train ì˜ì¡´ì„± ì œê±°)\n",
    "# # =========================\n",
    "# class TestImageDataset(Dataset):\n",
    "#     def __init__(self, task_dir, label_to_id):\n",
    "#         self.samples = []\n",
    "#         self.id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "        \n",
    "#         for label in os.listdir(task_dir):\n",
    "#             if label not in label_to_id:\n",
    "#                 continue\n",
    "#             label_dir = os.path.join(task_dir, label)\n",
    "#             for file in os.listdir(label_dir):\n",
    "#                 if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "#                     self.samples.append((\n",
    "#                         os.path.join(label_dir, file),\n",
    "#                         label_to_id[label]\n",
    "#                     ))\n",
    "        \n",
    "#         print(f\"  ğŸ“Š Test {os.path.basename(task_dir)}: {len(self.samples)} samples\")\n",
    "        \n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         path, label_id = self.samples[idx]\n",
    "#         img = Image.open(path).convert(\"RGB\")\n",
    "#         img = self.transform(img)\n",
    "#         return img, label_id\n",
    "\n",
    "# # =========================\n",
    "# # Test í•¨ìˆ˜\n",
    "# # =========================\n",
    "# def test():\n",
    "#     DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     BATCH_SIZE = 32  # TestëŠ” í¬ê²Œ\n",
    "    \n",
    "#     print(\"ğŸ” Loading best model...\")\n",
    "#     checkpoint = torch.load(\"pet_abnormality_omni_best.pth\", map_location=DEVICE)\n",
    "    \n",
    "#     num_classes_per_task = checkpoint[\"num_classes_per_task\"]\n",
    "#     label_to_id = checkpoint[\"label_to_id\"]\n",
    "    \n",
    "#     # ëª¨ë¸ ë³µì›\n",
    "#     model = AbnormalityOmni(num_classes_per_task).to(DEVICE)\n",
    "#     model.load_state_dict(checkpoint[\"model\"])\n",
    "#     model.eval()\n",
    "    \n",
    "#     print(\"ğŸ“¦ Loading TEST datasets...\")\n",
    "#     TEST_DIR = os.path.join(\"files\", \"work\", \"abnormality_omni\", \"test\")\n",
    "#     tasks = [\"skin\", \"eye\"]\n",
    "    \n",
    "#     test_loaders = {}\n",
    "#     for task in tasks:\n",
    "#         test_loaders[task] = DataLoader(\n",
    "#             TestImageDataset(\n",
    "#                 os.path.join(TEST_DIR, task),\n",
    "#                 label_to_id[task]\n",
    "#             ),\n",
    "#             BATCH_SIZE, False, num_workers=8, pin_memory=True\n",
    "#         )\n",
    "    \n",
    "#     # =========================\n",
    "#     # Evaluation\n",
    "#     # =========================\n",
    "#     results = {}\n",
    "#     all_preds = {}\n",
    "#     all_labels = {}\n",
    "    \n",
    "#     for task in tasks:\n",
    "#         print(f\"\\nğŸ” Testing {task.upper()}...\")\n",
    "#         correct, total = 0, 0\n",
    "#         all_preds[task] = []\n",
    "#         all_labels[task] = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for imgs, labels in tqdm(test_loaders[task], desc=f\"Test {task}\"):\n",
    "#                 imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "#                 outputs = model(imgs, task)\n",
    "#                 pred = outputs[task].argmax(dim=-1)\n",
    "                \n",
    "#                 correct += (pred == labels).sum().item()\n",
    "#                 total += labels.size(0)\n",
    "                \n",
    "#                 all_preds[task].extend(pred.cpu().numpy())\n",
    "#                 all_labels[task].extend(labels.cpu().numpy())\n",
    "        \n",
    "#         acc = correct / total\n",
    "#         results[task] = acc\n",
    "#         print(f\"  â†’ Accuracy: {acc:.4f} ({acc*100:.1f}%)\")\n",
    "    \n",
    "#     # Average\n",
    "#     avg_acc = sum(results.values()) / len(tasks)\n",
    "#     print(f\"\\nğŸ“Š FINAL TEST Results:\")\n",
    "#     for task, acc in results.items():\n",
    "#         print(f\"  {task.capitalize()}: {acc:.4f} ({acc*100:.1f}%)\")\n",
    "#     print(f\"  Average: {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "    \n",
    "#     # =========================\n",
    "#     # Detailed Metrics + Confusion Matrix\n",
    "#     # =========================\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "    \n",
    "#     for i, task in enumerate(tasks):\n",
    "#         plt.subplot(1, len(tasks), i+1)\n",
    "        \n",
    "#         # Classification Report\n",
    "#         print(f\"\\n{task.upper()} Classification Report:\")\n",
    "#         print(classification_report(\n",
    "#             all_labels[task], all_preds[task], \n",
    "#             target_names=list(label_to_id[task].keys()),\n",
    "#             digits=4\n",
    "#         ))\n",
    "        \n",
    "#         # Confusion Matrix\n",
    "#         cm = confusion_matrix(all_labels[task], all_preds[task])\n",
    "#         sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#         plt.title(f'{task.capitalize()} Confusion Matrix')\n",
    "#         plt.ylabel('True'); plt.xlabel('Pred')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('abnormality_test_results.png', dpi=150, bbox_inches='tight')\n",
    "#     print(\"âœ… Saved: abnormality_test_results.png\")\n",
    "    \n",
    "#     # Pet prediction accuracy (bonus)\n",
    "#     print(\"\\nğŸ” Pet (dog/cat) Accuracy Check...\")\n",
    "#     pet_correct, pet_total = 0, 0\n",
    "#     # ì „ì²´ testì—ì„œ pet ì˜ˆì¸¡ ìƒ˜í”Œë§ ì²´í¬ (ì‹¤ì œ appì²˜ëŸ¼)\n",
    "#     with torch.no_grad():\n",
    "#         for task in tasks:\n",
    "#             for imgs, _ in test_loaders[task][:5]:  # ìƒ˜í”Œ\n",
    "#                 imgs = imgs.to(DEVICE)\n",
    "#                 pet_probs = model(imgs[:1], task)['pet']  # single img test\n",
    "#                 pet_pred = pet_probs.argmax().item()\n",
    "#                 pet_total += 1\n",
    "#                 # ì‹¤ì œ pet ë¼ë²¨ ì—†ìœ¼ë¯€ë¡œ confidenceë§Œ\n",
    "#                 pet_correct += float(pet_probs.max() > 0.8)\n",
    "    \n",
    "#     pet_acc = pet_correct / pet_total\n",
    "#     print(f\"  Pet classification conf>0.8: {pet_acc:.4f}\")\n",
    "    \n",
    "#     # Save results\n",
    "#     torch.save({\n",
    "#         'test_results': results,\n",
    "#         'avg_acc': avg_acc,\n",
    "#         'predictions': all_preds,\n",
    "#         'labels': all_labels\n",
    "#     }, \"abnormality_test_results.pth\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
