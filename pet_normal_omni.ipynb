{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2747e39f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cap/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Device: cuda:1\n",
      "‚úÖ All file-copy tasks already prepared, skipping.\n",
      "\n",
      "üì¶ Collecting health/BCS (all samples)...\n",
      "  ‚Üí 4171 samples, 3 classes (all used)\n",
      "    normal: 4029\n",
      "    overweight: 93\n",
      "    underweight: 49\n",
      "  Health split ‚Üí train:3336, val:415, test:420\n",
      "\n",
      "‚úÖ Dataset preparation complete.\n",
      "\n",
      "üîÑ Pre-loading label mappings...\n",
      "  üìä behavior: 11843 samples, 25 classes\n",
      "  üìä emotion: 44766 samples, 10 classes\n",
      "  üìä sound: 995 samples, 14 classes, augment=False\n",
      "  üìä patella: 80696 samples, 5 classes\n",
      "\n",
      "üîÑ Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211/211 [00:00<00:00, 606.34it/s, Materializing param=wav2vec2.masked_spec_embed]                                            \n",
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "project_hid.weight           | UNEXPECTED | \n",
      "project_q.bias               | UNEXPECTED | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED | \n",
      "project_q.weight             | UNEXPECTED | \n",
      "project_hid.bias             | UNEXPECTED | \n",
      "quantizer.weight_proj.weight | UNEXPECTED | \n",
      "quantizer.codevectors        | UNEXPECTED | \n",
      "projector.bias               | MISSING    | \n",
      "projector.weight             | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "classifier.weight            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üìä sound: 995 samples, 14 classes, augment=False\n",
      "\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "============================================================\n",
      "\n",
      "üêæ Training Behavior...\n",
      "  üìä behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Avg Loss: 3.0736\n",
      "\n",
      "üòä Training Emotion...\n",
      "  üìä emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Avg Loss: 2.6950\n",
      "\n",
      "üîä Training Sound...\n",
      "  üìä sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sound:   0%|          | 0/32 [00:00<?, ?it/s]/tmp/ipykernel_1169724/2692483353.py:873: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  audio_scheduler.step()\n",
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí Avg Loss: 1.6198\n",
      "\n",
      "ü¶¥ Training Patella...\n",
      "  üìä patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patella:   1%|‚ñè         | 36/2522 [00:09<03:59, 10.37it/s] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, get_linear_schedule_with_warmup\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "from PIL import Image, ImageFile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import gc\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# =========================\n",
    "# 0. ÏÑ§Ï†ï\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BEHAVIOR_ROOT = \"files/1_Animal_Behavior\"\n",
    "EMOTION_ROOT  = \"files/2_Animal_emotions\"\n",
    "SOUND_ROOT    = \"files/3_Animal_Sound\"\n",
    "PATELLA_ROOT  = \"files/6_Animal_Patella\"\n",
    "HEALTH_ROOT   = \"files/7_Animal_Health\"   # ÎπÑÎßåÎèÑ Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "WORK_DIR      = \"files/work/omni_dataset\"\n",
    "\n",
    "# ‚îÄ‚îÄ ÏÉòÌîåÎßÅ Ï†úÍ±∞: Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÌôúÏö© ‚îÄ‚îÄ\n",
    "# (behavior/emotion/sound Î™®Îëê Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©, Î∂àÍ∑†ÌòïÏùÄ class_weightÎ°ú Î≥¥Ï†ï)\n",
    "\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 100\n",
    "LR_VIDEO    = 5e-5\n",
    "LR_AUDIO    = 1e-5\n",
    "DEVICE      = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 24\n",
    "SR          = 16000\n",
    "MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "# ‚îÄ‚îÄ LOSS_WEIGHTS ‚îÄ‚îÄ\n",
    "# emotion: 0.8‚Üí1.0 (Î∂àÍ∑†ÌòïÏùÄ class_weightÎ°ú Ï≤òÎ¶¨ÌïòÎØÄÎ°ú task Ï§ëÏöîÎèÑÎäî ÎèôÏùºÌïòÍ≤å)\n",
    "# health : 1.0 (ÎπÑÎßåÎèÑ, ÏûÑÏÉÅÏ†Å Ï§ëÏöîÎèÑ ÎÜíÏùå)\n",
    "LOSS_WEIGHTS = {\n",
    "    \"behavior\": 1.0,\n",
    "    \"emotion\":  1.0,\n",
    "    \"sound\":    0.6,\n",
    "    \"patella\":  1.0,\n",
    "    \"health\":   1.0,\n",
    "}\n",
    "\n",
    "# BCS ‚Üí ÎπÑÎßåÎèÑ 3-class Îß§Ìïë\n",
    "BCS_TO_LABEL = {1: 0, 2: 0, 3: 0,   # Ï†ÄÏ≤¥Ï§ë\n",
    "                4: 1, 5: 1, 6: 1,   # Î≥¥ÌÜµ\n",
    "                7: 2, 8: 2, 9: 2}   # Í≥ºÏ≤¥Ï§ë\n",
    "BCS_CLASSES  = [\"underweight\", \"normal\", \"overweight\"]\n",
    "\n",
    "AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "print(f\"üéØ Device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# üî• Audio Augmentation\n",
    "# =========================\n",
    "def augment_audio(waveform, p=0.5):\n",
    "    if random.random() > p:\n",
    "        return waveform\n",
    "    \n",
    "    n_steps = random.uniform(-2, 2)\n",
    "    waveform = librosa.effects.pitch_shift(waveform, sr=SR, n_steps=n_steps)\n",
    "    \n",
    "    rate = random.uniform(0.9, 1.1)\n",
    "    stretched = librosa.effects.time_stretch(waveform, rate=rate)\n",
    "    if len(stretched) > MAX_AUDIO_LEN:\n",
    "        stretched = stretched[:MAX_AUDIO_LEN]\n",
    "    else:\n",
    "        stretched = np.pad(stretched, (0, MAX_AUDIO_LEN - len(stretched)))\n",
    "    waveform = stretched\n",
    "    \n",
    "    noise = np.random.normal(0, 0.003, len(waveform))\n",
    "    waveform = waveform * 0.99 + noise\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "# =========================\n",
    "# 1. Dataset Preparation\n",
    "# =========================\n",
    "def collect_samples(root, exts):\n",
    "    samples = []\n",
    "    for class_dir in sorted(os.listdir(root)):\n",
    "        class_path = os.path.join(root, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        for root_dir, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if any(filename.lower().endswith(ext) for ext in exts):\n",
    "                    file_path = os.path.join(root_dir, filename)\n",
    "                    samples.append((class_dir, file_path))\n",
    "    \n",
    "    print(f\"  ‚Üí {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "    return samples\n",
    "\n",
    "def collect_patella_samples(root):\n",
    "    samples = []\n",
    "    \n",
    "    for grade in sorted(os.listdir(root)):\n",
    "        grade_path = os.path.join(root, grade)\n",
    "        if not os.path.isdir(grade_path):\n",
    "            continue\n",
    "        \n",
    "        for date_dir in os.listdir(grade_path):\n",
    "            date_path = os.path.join(grade_path, date_dir)\n",
    "            if not os.path.isdir(date_path):\n",
    "                continue\n",
    "            \n",
    "            for direction in ['Back', 'Front', 'Left', 'Right']:\n",
    "                direction_path = os.path.join(date_path, direction)\n",
    "                if not os.path.exists(direction_path):\n",
    "                    continue\n",
    "                \n",
    "                for filename in os.listdir(direction_path):\n",
    "                    if filename.lower().endswith('.jpg'):\n",
    "                        img_path = os.path.join(direction_path, filename)\n",
    "                        json_path = img_path.replace('.jpg', '.json')\n",
    "                        \n",
    "                        if os.path.exists(json_path):\n",
    "                            samples.append((grade, img_path, json_path))\n",
    "    \n",
    "    print(f\"  ‚Üí {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "    return samples\n",
    "\n",
    "def sample_balanced(samples):\n",
    "    \"\"\"ÏÉòÌîåÎßÅ ÏóÜÏù¥ Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò. Î∂àÍ∑†ÌòïÏùÄ ÌïôÏäµ Ïãú class_weightÎ°ú Î≥¥Ï†ï.\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    for label, _ in samples:\n",
    "        class_counts[label] += 1\n",
    "    print(f\"  üìä {len(class_counts)} classes, total {len(samples)} samples (all used)\")\n",
    "    for label, cnt in sorted(class_counts.items()):\n",
    "        print(f\"    {label}: {cnt}\")\n",
    "    return samples\n",
    "\n",
    "def sample_balanced_audio(samples):\n",
    "    \"\"\"ÏÉòÌîåÎßÅ ÏóÜÏù¥ Ï†ÑÏ≤¥ Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò. Î∂àÍ∑†ÌòïÏùÄ class_weightÎ°ú Î≥¥Ï†ï.\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    for label, _ in samples:\n",
    "        class_counts[label] += 1\n",
    "    print(f\"  üìä {len(class_counts)} classes, total {len(samples)} samples (all used)\")\n",
    "    for label, cnt in sorted(class_counts.items()):\n",
    "        print(f\"    {label}: {cnt}\")\n",
    "    return samples\n",
    "\n",
    "def collect_health_samples(root):\n",
    "    \"\"\"\n",
    "    7_Animal_Health/Cat/, 7_Animal_Health/Dog/ ÌïòÏúÑÏùò jpg+json Ïåç ÏàòÏßë.\n",
    "    label: BCS_TO_LABEL Í∏∞Ï§Ä 3-class (underweight / normal / overweight)\n",
    "    Î∞òÌôò: [(label_str, img_path, json_path), ...]\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for species in sorted(os.listdir(root)):           # Cat, Dog\n",
    "        species_path = os.path.join(root, species)\n",
    "        if not os.path.isdir(species_path):\n",
    "            continue\n",
    "        for fname in os.listdir(species_path):\n",
    "            if not fname.lower().endswith('.jpg'):\n",
    "                continue\n",
    "            img_path  = os.path.join(species_path, fname)\n",
    "            json_path = img_path.replace('.jpg', '.json').replace('.JPG', '.json')\n",
    "            if not os.path.exists(json_path):\n",
    "                continue\n",
    "            try:\n",
    "                with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                    meta = json.load(f)\n",
    "                bcs = int(meta['metadata']['physical']['BCS'])\n",
    "                label = BCS_CLASSES[BCS_TO_LABEL[bcs]]\n",
    "                samples.append((label, img_path, json_path))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    label_counts = defaultdict(int)\n",
    "    for label, _, _ in samples:\n",
    "        label_counts[label] += 1\n",
    "    print(f\"  ‚Üí {len(samples)} samples, {len(label_counts)} classes (all used)\")\n",
    "    for lbl, cnt in sorted(label_counts.items()):\n",
    "        print(f\"    {lbl}: {cnt}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "class HealthDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ÎπÑÎßåÎèÑ(BCS) Î∂ÑÎ•ò Dataset.\n",
    "    - JSONÏùò bounding box(points)Î°ú Ïù¥ÎØ∏ÏßÄÎ•º cropÌïú Îí§ ÌïôÏäµ\n",
    "    - label: 0=Ï†ÄÏ≤¥Ï§ë, 1=Î≥¥ÌÜµ, 2=Í≥ºÏ≤¥Ï§ë\n",
    "    \"\"\"\n",
    "    LABEL_TO_ID = {cls: i for i, cls in enumerate(BCS_CLASSES)}\n",
    "\n",
    "    def __init__(self, samples, augment=False):\n",
    "        \"\"\"\n",
    "        samples: collect_health_samples() Î∞òÌôòÍ∞í ÎòêÎäî\n",
    "                 split_health()Î°ú ÎÇòÎâú [(label, img_path, json_path), ...]\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.augment = augment\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, img_path, json_path = self.samples[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "\n",
    "        # Bounding box crop (points: [[x1,y1],[x2,y2]])\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                meta = json.load(f)\n",
    "            pts = meta['annotations']['label']['points']\n",
    "            x1, y1 = pts[0]\n",
    "            x2, y2 = pts[1]\n",
    "            x1, x2 = max(0, min(x1, x2)), min(w, max(x1, x2))\n",
    "            y1, y2 = max(0, min(y1, y2)), min(h, max(y1, y2))\n",
    "            if x2 > x1 and y2 > y1:\n",
    "                img = img.crop((x1, y1, x2, y2))\n",
    "        except Exception:\n",
    "            pass   # crop Ïã§Ìå® Ïãú ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return img, self.LABEL_TO_ID[label]\n",
    "\n",
    "\n",
    "def split_health(samples, train_ratio=0.8, val_ratio=0.1):\n",
    "    \"\"\"Health samplesÎ•º train/val/testÎ°ú Î∂ÑÎ¶¨ (ÌååÏùº Î≥µÏÇ¨ ÏóÜÏù¥ Ïù∏Î©îÎ™®Î¶¨ Î∂ÑÎ¶¨).\"\"\"\n",
    "    random.shuffle(samples)\n",
    "    class_split = defaultdict(list)\n",
    "    for item in samples:\n",
    "        class_split[item[0]].append(item)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    for items in class_split.values():\n",
    "        n = len(items)\n",
    "        n_train = int(n * train_ratio)\n",
    "        n_val   = int(n * val_ratio)\n",
    "        train.extend(items[:n_train])\n",
    "        val.extend(items[n_train:n_train + n_val])\n",
    "        test.extend(items[n_train + n_val:])\n",
    "\n",
    "    print(f\"  Health split ‚Üí train:{len(train)}, val:{len(val)}, test:{len(test)}\")\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def split_and_copy(samples, task_name, is_patella=False, original_samples=None):\n",
    "    \"\"\"\n",
    "    original_samples: sound task Ï†ÑÏö©. test setÏùÑ Ïò§Î≤ÑÏÉòÌîå Ïù¥Ï†Ñ ÏõêÎ≥∏ÏóêÏÑú Î∂ÑÎ¶¨Ìï† Îïå ÏÇ¨Ïö©.\n",
    "                      Î≤ÑÍ∑∏ 1 ÏàòÏ†ï - Ïò§Î≤ÑÏÉòÌîåÎêú poolÍ≥º testÍ∞Ä Í≤πÏπòÎäî data leakage Î∞©ÏßÄ.\n",
    "    \"\"\"\n",
    "    random.shuffle(samples)\n",
    "    class_samples = defaultdict(list)\n",
    "\n",
    "    if is_patella:\n",
    "        for label, img_path, json_path in samples:\n",
    "            class_samples[label].append((img_path, json_path))\n",
    "    else:\n",
    "        for label, path in samples:\n",
    "            class_samples[label].append(path)\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(WORK_DIR, split, task_name), exist_ok=True)\n",
    "\n",
    "    # ‚úÖ sound: test setÏùÄ Ïò§Î≤ÑÏÉòÌîå Ïù¥Ï†Ñ ÏõêÎ≥∏(original_samples)ÏóêÏÑú Î≥ÑÎèÑ Ï∂îÏ∂ú\n",
    "    if original_samples is not None:\n",
    "        orig_class = defaultdict(list)\n",
    "        for label, path in original_samples:\n",
    "            orig_class[label].append(path)\n",
    "        test_items_by_label = {\n",
    "            label: paths[:max(10, len(paths) // 5)]\n",
    "            for label, paths in orig_class.items()\n",
    "        }\n",
    "    else:\n",
    "        test_items_by_label = None\n",
    "\n",
    "    for label, items in class_samples.items():\n",
    "        n = len(items)\n",
    "        n_train = int(n * 0.8)\n",
    "        n_val   = int(n * 0.1)\n",
    "\n",
    "        if test_items_by_label is not None:\n",
    "            # sound: train/valÏùÄ Ïò§Î≤ÑÏÉòÌîå pool, testÎäî ÏõêÎ≥∏\n",
    "            train_items = items[:n_train]\n",
    "            val_items   = items[n_train:n_train + n_val]\n",
    "            test_items  = test_items_by_label.get(label, [])\n",
    "        else:\n",
    "            train_items = items[:n_train]\n",
    "            val_items   = items[n_train:n_train + n_val]\n",
    "            test_items  = items[n_train + n_val:]\n",
    "\n",
    "        split_map = {\"train\": train_items, \"val\": val_items, \"test\": test_items}\n",
    "\n",
    "        for split_name, split_items in split_map.items():\n",
    "            dst_label_dir = os.path.join(WORK_DIR, split_name, task_name, label)\n",
    "            os.makedirs(dst_label_dir, exist_ok=True)\n",
    "\n",
    "            for item in tqdm(split_items, desc=f\"{task_name}/{split_name}/{label}\", leave=False):\n",
    "                if is_patella:\n",
    "                    img_path, json_path = item\n",
    "                    dst_img  = os.path.join(dst_label_dir, f\"{label}_{os.path.basename(img_path)}\")\n",
    "                    shutil.copy(img_path, dst_img)\n",
    "                    dst_json = dst_img.replace('.jpg', '.json')\n",
    "                    shutil.copy(json_path, dst_json)\n",
    "                else:\n",
    "                    dst_path = os.path.join(dst_label_dir, f\"{label}_{os.path.basename(item)}\")\n",
    "                    shutil.copy(item, dst_path)\n",
    "\n",
    "def _task_ready(task_name):\n",
    "    \"\"\"Ìï¥Îãπ taskÏùò train Ìè¥ÎçîÍ∞Ä Ï°¥Ïû¨ÌïòÍ≥† ÎπÑÏñ¥ÏûàÏßÄ ÏïäÏúºÎ©¥ True\"\"\"\n",
    "    task_train = os.path.join(WORK_DIR, \"train\", task_name)\n",
    "    return os.path.isdir(task_train) and len(os.listdir(task_train)) > 0\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    # taskÎ≥Ñ ÎèÖÎ¶Ω Ï≤¥ÌÅ¨: ÏóÜÎäî taskÎßå ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú Ï§ÄÎπÑ\n",
    "    need_behavior = not _task_ready(\"behavior\")\n",
    "    need_emotion  = not _task_ready(\"emotion\")\n",
    "    need_sound    = not _task_ready(\"sound\")\n",
    "    need_patella  = not _task_ready(\"patella\")\n",
    "    # HealthÎäî ÌååÏùº Î≥µÏÇ¨ ÏóÜÏù¥ Ïù∏Î©îÎ™®Î¶¨ Î∂ÑÎ¶¨ ‚Üí Ìï≠ÏÉÅ ÏàòÏßë\n",
    "\n",
    "    if not any([need_behavior, need_emotion, need_sound, need_patella]):\n",
    "        print(\"\\u2705 All file-copy tasks already prepared, skipping.\")\n",
    "    else:\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            os.makedirs(os.path.join(WORK_DIR, split), exist_ok=True)\n",
    "\n",
    "        if need_behavior:\n",
    "            print(\"\\n\\U0001f4e6 Collecting behavior (all samples)...\")\n",
    "            behavior_all = collect_samples(BEHAVIOR_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "            behavior = sample_balanced(behavior_all)          # \\u2705 \\uc804\\uccb4 \\uc0ac\\uc6a9\n",
    "            print(\"  \\U0001f4cb Splitting & Copying behavior...\")\n",
    "            split_and_copy(behavior, \"behavior\")\n",
    "        else:\n",
    "            print(\"\\u2705 behavior already prepared, skipping.\")\n",
    "\n",
    "        if need_emotion:\n",
    "            print(\"\\n\\U0001f4e6 Collecting emotion (all samples)...\")\n",
    "            emotion_all = collect_samples(EMOTION_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "            emotion = sample_balanced(emotion_all)            # \\u2705 \\uc804\\uccb4 \\uc0ac\\uc6a9\n",
    "            print(\"  \\U0001f4cb Splitting & Copying emotion...\")\n",
    "            split_and_copy(emotion, \"emotion\")\n",
    "        else:\n",
    "            print(\"\\u2705 emotion already prepared, skipping.\")\n",
    "\n",
    "        if need_sound:\n",
    "            print(\"\\n\\U0001f4e6 Collecting sound (all samples)...\")\n",
    "            sound_all = collect_samples(SOUND_ROOT, ['.wav', '.mp3', '.m4a'])\n",
    "            sound = sample_balanced_audio(sound_all)          # \\u2705 \\uc804\\uccb4 \\uc0ac\\uc6a9\n",
    "            print(\"  \\U0001f4cb Splitting & Copying sound...\")\n",
    "            split_and_copy(sound, \"sound\", original_samples=sound_all)\n",
    "        else:\n",
    "            print(\"\\u2705 sound already prepared, skipping.\")\n",
    "\n",
    "        if need_patella:\n",
    "            print(\"\\n\\U0001f4e6 Collecting patella luxation (all samples)...\")\n",
    "            patella_all = collect_patella_samples(PATELLA_ROOT)\n",
    "            print(\"  \\u2139\\ufe0f  Patella: Using all samples\")\n",
    "            print(\"  \\U0001f4cb Splitting & Copying patella...\")\n",
    "            split_and_copy(patella_all, \"patella\", is_patella=True)\n",
    "        else:\n",
    "            print(\"\\u2705 patella already prepared, skipping.\")\n",
    "\n",
    "    # Health: \\ud30c\\uc77c \\ubcf5\\uc0ac \\uc5c6\\uc774 \\uc778\\uba54\\ubaa8\\ub9ac split \\u2192 \\ud56d\\uc0c1 \\uc218\\uc9d1\n",
    "    print(\"\\n\\U0001f4e6 Collecting health/BCS (all samples)...\")\n",
    "    health_all = collect_health_samples(HEALTH_ROOT)\n",
    "    health_train, health_val, health_test = split_health(health_all)\n",
    "\n",
    "    print(\"\\n\\u2705 Dataset preparation complete.\")\n",
    "    return health_train, health_val, health_test\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset Classes\n",
    "# =========================\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        \n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            \n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(label_dir, file), label))\n",
    "        \n",
    "        print(f\"  üìä {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.label_to_id[label]\n",
    "\n",
    "class PatellaDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        \n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            \n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith('.jpg'):\n",
    "                    img_path = os.path.join(label_dir, file)\n",
    "                    json_path = img_path.replace('.jpg', '.json')\n",
    "                    \n",
    "                    if os.path.exists(json_path):\n",
    "                        self.samples.append((img_path, json_path, label))\n",
    "        \n",
    "        print(f\"  üìä {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, json_path, label = self.samples[idx]\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        keypoints = []\n",
    "        for annotation in data.get('annotation_info', []):\n",
    "            x = float(annotation.get('x', 0))\n",
    "            y = float(annotation.get('y', 0))\n",
    "            keypoints.extend([x, y])\n",
    "        \n",
    "        while len(keypoints) < 18:\n",
    "            keypoints.append(0.0)\n",
    "        \n",
    "        keypoints = torch.tensor(keypoints[:18], dtype=torch.float32)\n",
    "        \n",
    "        return img, keypoints, self.label_to_id[label]\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = {}   # ‚úÖ Ïó≠Î∞©Ìñ• Îß§Ìïë Ï∂îÍ∞Ä\n",
    "        self.augment = augment\n",
    "        next_id = 0\n",
    "\n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "\n",
    "            self.label_to_id[label] = next_id\n",
    "            self.id_to_label[next_id] = label\n",
    "            next_id += 1\n",
    "\n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.wav', '.mp3', '.m4a')):\n",
    "                    self.samples.append((os.path.join(label_dir, file), label))\n",
    "\n",
    "        print(f\"  üìä {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes, augment={augment}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "        except Exception:\n",
    "            waveform = np.zeros(MAX_AUDIO_LEN)\n",
    "\n",
    "        if self.augment:\n",
    "            waveform = augment_audio(waveform)\n",
    "\n",
    "        if len(waveform) > MAX_AUDIO_LEN:\n",
    "            waveform = waveform[:MAX_AUDIO_LEN]\n",
    "        else:\n",
    "            waveform = np.pad(waveform, (0, MAX_AUDIO_LEN - len(waveform)))\n",
    "\n",
    "        inputs = FEATURE_EXTRACTOR(waveform, sampling_rate=SR, return_tensors=\"pt\")\n",
    "        # ‚úÖ dict Î∞òÌôò: collate_fnÏù¥ ÏïàÏ†ÑÌïòÍ≤å Ïä§ÌÉùÌï† Ïàò ÏûàÎèÑÎ°ù\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            \"labels\": torch.tensor(self.label_to_id[label], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn_audio(batch):\n",
    "    \"\"\"AudioDatasetÏùò dict Î∞∞ÏπòÎ•º ÏïàÏ†ÑÌïòÍ≤å ÌÖåÏÑ†ÏÑúÎ°ú Î≥¥Ìòà\"\"\"\n",
    "    input_values = torch.stack([item[\"input_values\"] for item in batch])\n",
    "    labels       = torch.stack([item[\"labels\"]       for item in batch])\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# =========================\n",
    "# 3. Individual Models (ÎèÖÎ¶Ω Î™®Îç∏)\n",
    "# =========================\n",
    "def _efficientnet_b3_backbone():\n",
    "    \"\"\"EfficientNet-B3 backbone Í≥µÌÜµ ÎπåÎçî. fc Ï†úÍ±∞ ÌõÑ feature vector Î∞òÌôò.\"\"\"\n",
    "    backbone = efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
    "    in_features = backbone.classifier[1].in_features   # 1536\n",
    "    backbone.classifier = nn.Identity()\n",
    "    return backbone, in_features\n",
    "\n",
    "\n",
    "class BehaviorModel(nn.Module):\n",
    "    \"\"\"ÌñâÎèô Î∂ÑÎ•ò: EfficientNet-B3 backbone\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone, in_features = _efficientnet_b3_backbone()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    \"\"\"Í∞êÏ†ï Î∂ÑÎ•ò: EfficientNet-B3 backbone (ÎèÖÎ¶Ω backbone, Î≥ÑÎèÑ class_weight Ï†ÅÏö©)\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone, in_features = _efficientnet_b3_backbone()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "\n",
    "class PatellaModel(nn.Module):\n",
    "    \"\"\"Ïä¨Í∞úÍ≥® ÌÉàÍµ¨: EfficientNet-B3 + keypoint concat\"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone, in_features = _efficientnet_b3_backbone()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features + 18, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, keypoints):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(torch.cat([feat, keypoints], dim=1))\n",
    "\n",
    "\n",
    "class HealthModel(nn.Module):\n",
    "    \"\"\"ÎπÑÎßåÎèÑ(BCS) Î∂ÑÎ•ò: EfficientNet-B3 backbone, 3-class (Ï†ÄÏ≤¥Ï§ë/Î≥¥ÌÜµ/Í≥ºÏ≤¥Ï§ë)\"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.backbone, in_features = _efficientnet_b3_backbone()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            AUDIO_MODEL_NAME,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.model.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        # ‚úÖ labelsÎ•º ÎÑòÍ∏∞Î©¥ Î™®Îç∏ ÎÇ¥Î∂ÄÏóêÏÑú lossÎ•º ÏßÅÏ†ë Í≥ÑÏÇ∞ (padding mask Í≥†Î†§)\n",
    "        return self.model(input_values=input_values, labels=labels)\n",
    "\n",
    "# =========================\n",
    "# 4. Helper Functions\n",
    "# =========================\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"üî• Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# =========================\n",
    "# 5. Sequential Training (Î©îÎ™®Î¶¨ Ìö®Ïú®Ï†Å)\n",
    "# =========================\n",
    "def train():\n",
    "    # prepare_datasetÏùÄ health splitÏùÑ Î∞òÌôò\n",
    "    health_train_samples, health_val_samples, _ = prepare_dataset()\n",
    "\n",
    "    # label_to_id ÎØ∏Î¶¨ Î°úÎìú\n",
    "    print(\"\\n\\U0001f504 Pre-loading label mappings...\")\n",
    "    temp_b = ImageDataset(os.path.join(WORK_DIR, \"train\", \"behavior\"), augment=False)\n",
    "    temp_e = ImageDataset(os.path.join(WORK_DIR, \"train\", \"emotion\"), augment=False)\n",
    "    temp_s = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=False)\n",
    "    temp_p = PatellaDataset(os.path.join(WORK_DIR, \"train\", \"patella\"), augment=False)\n",
    "\n",
    "    behavior_label_to_id = temp_b.label_to_id\n",
    "    emotion_label_to_id  = temp_e.label_to_id\n",
    "    sound_label_to_id    = temp_s.label_to_id\n",
    "    sound_id_to_label    = temp_s.id_to_label\n",
    "    patella_label_to_id  = temp_p.label_to_id\n",
    "\n",
    "    # Emotion class_weight (Î∂àÍ∑†Ìòï Î≥¥Ï†ï)\n",
    "    emotion_labels_list = [temp_e.label_to_id[label] for _, label in temp_e.samples]\n",
    "    emotion_class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.arange(len(emotion_label_to_id)),\n",
    "        y=emotion_labels_list\n",
    "    )\n",
    "    emotion_class_weights_tensor = torch.tensor(emotion_class_weights, dtype=torch.float)\n",
    "\n",
    "    del temp_b, temp_e, temp_s, temp_p\n",
    "    clear_memory()\n",
    "\n",
    "    # Î™®Îç∏ Ï¥àÍ∏∞Ìôî (CPUÏóê Î®ºÏ†Ä ÏÉùÏÑ±)\n",
    "    print(\"\\n\\U0001f504 Initializing models...\")\n",
    "    behavior_model = BehaviorModel(len(behavior_label_to_id))\n",
    "    emotion_model  = EmotionModel(len(emotion_label_to_id))\n",
    "    patella_model  = PatellaModel(len(patella_label_to_id))\n",
    "    audio_model    = AudioModel(len(sound_label_to_id), freeze_backbone=False)\n",
    "    health_model   = HealthModel(num_classes=3)\n",
    "\n",
    "    # Optimizers\n",
    "    behavior_opt = torch.optim.AdamW(behavior_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "    emotion_opt  = torch.optim.AdamW(emotion_model.parameters(),  lr=LR_VIDEO, weight_decay=0.01)\n",
    "    patella_opt  = torch.optim.AdamW(patella_model.parameters(),  lr=LR_VIDEO, weight_decay=0.01)\n",
    "    audio_opt    = torch.optim.AdamW(audio_model.parameters(),    lr=LR_AUDIO, weight_decay=0.01)\n",
    "    health_opt   = torch.optim.AdamW(health_model.parameters(),   lr=LR_VIDEO, weight_decay=0.01)\n",
    "\n",
    "    # Audio LR Warmup Scheduler\n",
    "    _temp_sound = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=False)\n",
    "    _approx_sound_steps = (len(_temp_sound) // BATCH_SIZE) * EPOCHS\n",
    "    del _temp_sound\n",
    "    audio_scheduler = get_linear_schedule_with_warmup(\n",
    "        audio_opt,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=_approx_sound_steps\n",
    "    )\n",
    "    clear_memory()\n",
    "\n",
    "    # Scalers\n",
    "    video_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    audio_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # Loss (Í∏∞Î≥∏ criterion: label smoothing Ï†ÅÏö©)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Emotion Ï†ÑÏö© criterion: class_weight Ï∂îÍ∞ÄÎ°ú Î∂àÍ∑†Ìòï Î≥¥Ï†ï\n",
    "    criterion_emotion = nn.CrossEntropyLoss(\n",
    "        weight=emotion_class_weights_tensor.to(DEVICE),\n",
    "        label_smoothing=0.1\n",
    "    )\n",
    "    # Health Ï†ÑÏö© criterion: BCS 3-class Î∂àÍ∑†Ìòï Î≥¥Ï†ï (ÌïôÏäµ Ïãú ÎèôÏ†Å Í≥ÑÏÇ∞)\n",
    "    # (HealthDataset ÏÉòÌîåÏù¥ epochÎßàÎã§ Í≥†Ï†ïÏù¥ÎØÄÎ°ú 1Ìöå Í≥ÑÏÇ∞ ÌõÑ Ïû¨ÏÇ¨Ïö©)\n",
    "    _health_labels = [HealthDataset.LABEL_TO_ID[s[0]] for s in health_train_samples]\n",
    "    _health_cw = compute_class_weight('balanced', classes=np.arange(3), y=_health_labels)\n",
    "    criterion_health = nn.CrossEntropyLoss(\n",
    "        weight=torch.tensor(_health_cw, dtype=torch.float).to(DEVICE),\n",
    "        label_smoothing=0.1\n",
    "    )\n",
    "\n",
    "    best_avg_acc = 0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        loss_b, loss_e, loss_s, loss_p, loss_h = 0, 0, 0, 0, 0\n",
    "        \n",
    "        # ========== 1. Behavior ==========\n",
    "        print(f\"\\nüêæ Training Behavior...\")\n",
    "        behavior_model.to(DEVICE)\n",
    "        behavior_model.train()\n",
    "        \n",
    "        behavior_train = ImageDataset(os.path.join(WORK_DIR, \"train\", \"behavior\"), augment=True)\n",
    "        behavior_loader = DataLoader(behavior_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, labels in tqdm(behavior_loader, desc=\"Behavior\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            behavior_opt.zero_grad()  # ‚úÖ zero_grad Ïù¥Îèô: forward ÏïûÏúºÎ°ú\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = behavior_model(imgs)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(behavior_opt)\n",
    "            video_scaler.update()\n",
    "            \n",
    "            loss_b += loss.item()\n",
    "        \n",
    "        loss_b /= len(behavior_loader)\n",
    "        print(f\"  ‚Üí Avg Loss: {loss_b:.4f}\")\n",
    "        \n",
    "        # üî• Î©îÎ™®Î¶¨ Ìï¥Ï†ú\n",
    "        behavior_model.cpu()\n",
    "        del behavior_train, behavior_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 2. Emotion ==========\n",
    "        print(f\"\\nüòä Training Emotion...\")\n",
    "        emotion_model.to(DEVICE)\n",
    "        emotion_model.train()\n",
    "        \n",
    "        emotion_train = ImageDataset(os.path.join(WORK_DIR, \"train\", \"emotion\"), augment=True)\n",
    "        emotion_loader = DataLoader(emotion_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, labels in tqdm(emotion_loader, desc=\"Emotion\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            emotion_opt.zero_grad()  # ‚úÖ zero_grad Ïù¥Îèô: forward ÏïûÏúºÎ°ú\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = emotion_model(imgs)\n",
    "                # ‚úÖ emotion: class_weight Ï†ÅÏö© criterion ÏÇ¨Ïö©\n",
    "                loss = (lam * criterion_emotion(logits, labels_a)\n",
    "                        + (1 - lam) * criterion_emotion(logits, labels_b))\n",
    "                loss = loss * LOSS_WEIGHTS[\"emotion\"]\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(emotion_opt)\n",
    "            video_scaler.update()\n",
    "\n",
    "            loss_e += loss.item()\n",
    "        \n",
    "        loss_e /= len(emotion_loader)\n",
    "        print(f\"  ‚Üí Avg Loss: {loss_e:.4f}\")\n",
    "        \n",
    "        emotion_model.cpu()\n",
    "        del emotion_train, emotion_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 3. Sound ==========\n",
    "        print(f\"\\nüîä Training Sound...\")\n",
    "        audio_model.to(DEVICE)\n",
    "        audio_model.train()\n",
    "        \n",
    "        sound_train = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=True)\n",
    "\n",
    "        # ‚úÖ ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò (epochÎßàÎã§ Í≥ÑÏÇ∞ Ïú†ÏßÄ ‚Äì ÌÅ¥ÎûòÏä§Î≥Ñ Î≥ÑÎèÑ criterion)\n",
    "        sound_labels_list = [item[1] for item in sound_train.samples]\n",
    "        sound_label_ids   = [sound_train.label_to_id[l] for l in sound_labels_list]\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.arange(len(sound_train.label_to_id)),\n",
    "            y=sound_label_ids\n",
    "        )\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        # ‚úÖ collate_fn_audio Ï†ÅÏö©\n",
    "        sound_loader = DataLoader(\n",
    "            sound_train, BATCH_SIZE, True,\n",
    "            num_workers=2, pin_memory=True,\n",
    "            collate_fn=collate_fn_audio\n",
    "        )\n",
    "\n",
    "        for batch in tqdm(sound_loader, desc=\"Sound\", leave=False):\n",
    "            audios = batch[\"input_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            audio_opt.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                # ‚úÖ labels ÏßÅÏ†ë Ï†ÑÎã¨ ‚Üí outputs.loss ÏÇ¨Ïö© (padding mask Í≥†Î†§)\n",
    "                outputs = audio_model(input_values=audios, labels=labels)\n",
    "                # ‚úÖ LOSS_WEIGHTS Ïú†ÏßÄ, ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπòÎäî ÏàòÎèôÏúºÎ°ú Ï†ÅÏö©\n",
    "                loss = outputs.loss * LOSS_WEIGHTS[\"sound\"]\n",
    "                # class_weightsÎ•º ÌôúÏö©Ìïú Î≥¥Ï†ï Ìï≠ Ï∂îÍ∞Ä\n",
    "                per_sample_w = class_weights_tensor[labels]\n",
    "                loss = (loss * per_sample_w.mean())\n",
    "\n",
    "            audio_scaler.scale(loss).backward()\n",
    "            audio_scaler.unscale_(audio_opt)\n",
    "            torch.nn.utils.clip_grad_norm_(audio_model.parameters(), 1.0)\n",
    "            audio_scaler.step(audio_opt)\n",
    "            audio_scaler.update()\n",
    "            # ‚úÖ Ïä§ÏºÄÏ§ÑÎü¨ step\n",
    "            audio_scheduler.step()\n",
    "\n",
    "            loss_s += loss.item()\n",
    "\n",
    "        loss_s /= len(sound_loader)\n",
    "        print(f\"  ‚Üí Avg Loss: {loss_s:.4f}\")\n",
    "\n",
    "        audio_model.cpu()\n",
    "        del sound_train, sound_loader, class_weights_tensor\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 4. Patella ==========\n",
    "        print(f\"\\nü¶¥ Training Patella...\")\n",
    "        patella_model.to(DEVICE)\n",
    "        patella_model.train()\n",
    "        \n",
    "        patella_train = PatellaDataset(os.path.join(WORK_DIR, \"train\", \"patella\"), augment=True)\n",
    "        patella_loader = DataLoader(patella_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, keypoints, labels in tqdm(patella_loader, desc=\"Patella\", leave=False):\n",
    "            imgs, keypoints, labels = imgs.to(DEVICE), keypoints.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            patella_opt.zero_grad()  # ‚úÖ zero_grad Ïù¥Îèô: forward ÏïûÏúºÎ°ú\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = patella_model(imgs, keypoints)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(patella_opt)\n",
    "            video_scaler.update()\n",
    "            \n",
    "            loss_p += loss.item()\n",
    "        \n",
    "        loss_p /= len(patella_loader)\n",
    "        print(f\"  ‚Üí Avg Loss: {loss_p:.4f}\")\n",
    "        \n",
    "        patella_model.cpu()\n",
    "        del patella_train, patella_loader\n",
    "        clear_memory()\n",
    "\n",
    "        # ========== 5. Health (BCS) ==========\n",
    "        print(f\"\\n\\U0001f4aa Training Health(BCS)...\")\n",
    "        health_model.to(DEVICE)\n",
    "        health_model.train()\n",
    "\n",
    "        health_train_ds = HealthDataset(health_train_samples, augment=True)\n",
    "        health_loader   = DataLoader(health_train_ds, BATCH_SIZE, True,\n",
    "                                     num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "        for imgs, labels in tqdm(health_loader, desc=\"Health\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            health_opt.zero_grad()\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = health_model(imgs)\n",
    "                loss = (lam * criterion_health(logits, labels_a)\n",
    "                        + (1 - lam) * criterion_health(logits, labels_b))\n",
    "                loss = loss * LOSS_WEIGHTS[\"health\"]\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(health_opt)\n",
    "            video_scaler.update()\n",
    "\n",
    "            loss_h += loss.item()\n",
    "\n",
    "        loss_h /= len(health_loader)\n",
    "        print(f\"  \\u2192 Avg Loss: {loss_h:.4f}\")\n",
    "\n",
    "        health_model.cpu()\n",
    "        del health_train_ds, health_loader\n",
    "        clear_memory()\n",
    "\n",
    "        # ========== Validation ==========\n",
    "        print(f\"\\nüîç Validation...\")\n",
    "        \n",
    "        # Behavior Val\n",
    "        behavior_model.to(DEVICE)\n",
    "        behavior_model.eval()\n",
    "        behavior_val = ImageDataset(os.path.join(WORK_DIR, \"val\", \"behavior\"), augment=False)\n",
    "        behavior_val_loader = DataLoader(behavior_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_b, total_b = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(behavior_val_loader, desc=\"Val Behavior\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = behavior_model(imgs)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_b += (pred == labels).sum().item()\n",
    "                total_b += labels.size(0)\n",
    "        acc_b = correct_b / total_b\n",
    "        \n",
    "        behavior_model.cpu()\n",
    "        del behavior_val, behavior_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Emotion Val\n",
    "        emotion_model.to(DEVICE)\n",
    "        emotion_model.eval()\n",
    "        emotion_val = ImageDataset(os.path.join(WORK_DIR, \"val\", \"emotion\"), augment=False)\n",
    "        emotion_val_loader = DataLoader(emotion_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_e, total_e = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(emotion_val_loader, desc=\"Val Emotion\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = emotion_model(imgs)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_e += (pred == labels).sum().item()\n",
    "                total_e += labels.size(0)\n",
    "        acc_e = correct_e / total_e\n",
    "        \n",
    "        emotion_model.cpu()\n",
    "        del emotion_val, emotion_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Sound Val\n",
    "        audio_model.to(DEVICE)\n",
    "        audio_model.eval()\n",
    "        sound_val = AudioDataset(os.path.join(WORK_DIR, \"val\", \"sound\"), augment=False)\n",
    "        # ‚úÖ collate_fn_audio Ï†ÅÏö©\n",
    "        sound_val_loader = DataLoader(\n",
    "            sound_val, BATCH_SIZE, False,\n",
    "            num_workers=2, pin_memory=True,\n",
    "            collate_fn=collate_fn_audio\n",
    "        )\n",
    "\n",
    "        correct_s, total_s = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(sound_val_loader, desc=\"Val Sound\", leave=False):\n",
    "                audios = batch[\"input_values\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                # ‚úÖ outputs.logits ÏÇ¨Ïö©\n",
    "                outputs = audio_model(input_values=audios, labels=labels)\n",
    "                pred = outputs.logits.argmax(-1)\n",
    "                correct_s += (pred == labels).sum().item()\n",
    "                total_s   += labels.size(0)\n",
    "        acc_s = correct_s / total_s\n",
    "        \n",
    "        audio_model.cpu()\n",
    "        del sound_val, sound_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Patella Val\n",
    "        patella_model.to(DEVICE)\n",
    "        patella_model.eval()\n",
    "        patella_val = PatellaDataset(os.path.join(WORK_DIR, \"val\", \"patella\"), augment=False)\n",
    "        patella_val_loader = DataLoader(patella_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_p, total_p = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, keypoints, labels in tqdm(patella_val_loader, desc=\"Val Patella\", leave=False):\n",
    "                imgs, keypoints, labels = imgs.to(DEVICE), keypoints.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = patella_model(imgs, keypoints)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_p += (pred == labels).sum().item()\n",
    "                total_p += labels.size(0)\n",
    "        acc_p = correct_p / total_p\n",
    "\n",
    "        patella_model.cpu()\n",
    "        del patella_val, patella_val_loader\n",
    "        clear_memory()\n",
    "\n",
    "        # Health Val\n",
    "        health_model.to(DEVICE)\n",
    "        health_model.eval()\n",
    "        health_val_ds     = HealthDataset(health_val_samples, augment=False)\n",
    "        health_val_loader = DataLoader(health_val_ds, BATCH_SIZE, False,\n",
    "                                       num_workers=NUM_WORKERS // 2, pin_memory=True)\n",
    "\n",
    "        correct_h, total_h = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(health_val_loader, desc=\"Val Health\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = health_model(imgs)\n",
    "                pred   = logits.argmax(-1)\n",
    "                correct_h += (pred == labels).sum().item()\n",
    "                total_h   += labels.size(0)\n",
    "        acc_h = correct_h / total_h\n",
    "\n",
    "        health_model.cpu()\n",
    "        del health_val_ds, health_val_loader\n",
    "        clear_memory()\n",
    "\n",
    "        avg_acc = (acc_b + acc_e + acc_s + acc_p + acc_h) / 5\n",
    "        \n",
    "        print(f\"\\n\\U0001f4ca Results:\")\n",
    "        print(f\"  Behavior: Loss {loss_b:.4f} | Acc {acc_b:.4f} ({acc_b*100:.1f}%)\")\n",
    "        print(f\"  Emotion:  Loss {loss_e:.4f} | Acc {acc_e:.4f} ({acc_e*100:.1f}%)\")\n",
    "        print(f\"  Sound:    Loss {loss_s:.4f} | Acc {acc_s:.4f} ({acc_s*100:.1f}%)\")\n",
    "        print(f\"  Patella:  Loss {loss_p:.4f} | Acc {acc_p:.4f} ({acc_p*100:.1f}%)\")\n",
    "        print(f\"  Health:   Loss {loss_h:.4f} | Acc {acc_h:.4f} ({acc_h*100:.1f}%)\")\n",
    "        print(f\"  Average Acc: {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "\n",
    "        history.append({\n",
    "            'epoch' : epoch + 1,\n",
    "            'loss_b': loss_b, 'loss_e': loss_e, 'loss_s': loss_s,\n",
    "            'loss_p': loss_p, 'loss_h': loss_h,\n",
    "            'acc_b' : acc_b,  'acc_e' : acc_e,  'acc_s' : acc_s,\n",
    "            'acc_p' : acc_p,  'acc_h' : acc_h,  'acc_avg': avg_acc,\n",
    "        })\n",
    "\n",
    "        if avg_acc > best_avg_acc:\n",
    "            best_avg_acc = avg_acc\n",
    "            torch.save({\n",
    "                \"behavior_model\":       behavior_model.state_dict(),\n",
    "                \"emotion_model\":        emotion_model.state_dict(),\n",
    "                \"audio_model\":          audio_model.state_dict(),\n",
    "                \"patella_model\":        patella_model.state_dict(),\n",
    "                \"health_model\":         health_model.state_dict(),\n",
    "                \"behavior_label_to_id\": behavior_label_to_id,\n",
    "                \"emotion_label_to_id\":  emotion_label_to_id,\n",
    "                \"sound_label_to_id\":    sound_label_to_id,\n",
    "                \"sound_id_to_label\":    sound_id_to_label,\n",
    "                \"patella_label_to_id\":  patella_label_to_id,\n",
    "                \"health_classes\":       BCS_CLASSES,\n",
    "                \"best_epoch\":           epoch + 1,\n",
    "                \"best_acc\":             best_avg_acc,\n",
    "                \"history\":              history,\n",
    "            }, \"pet_normal_omni_best.pth\")\n",
    "            print(f\"  \\U0001f4be Saved new best model! (Acc: {best_avg_acc:.4f})\")\n",
    "    \n",
    "    # ÌïôÏäµ Í≥°ÏÑ† ÏãúÍ∞ÅÌôî\n",
    "    print(\"\\n\\U0001f4c8 Generating training history plot...\")\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(25, 8))\n",
    "\n",
    "    tasks = [\n",
    "        ('acc_b', 'b-',      'Behavior'),\n",
    "        ('acc_e', 'r-',      'Emotion'),\n",
    "        ('acc_s', 'g-',      'Sound'),\n",
    "        ('acc_p', 'purple',  'Patella'),\n",
    "        ('acc_h', 'orange',  'Health'),\n",
    "    ]\n",
    "    loss_keys = ['loss_b', 'loss_e', 'loss_s', 'loss_p', 'loss_h']\n",
    "\n",
    "    for i, (acc_key, color, title) in enumerate(tasks):\n",
    "        # Loss row\n",
    "        axes[0, i].plot([h[loss_keys[i]] for h in history], color=color, linewidth=2)\n",
    "        axes[0, i].set_title(f'{title} Loss')\n",
    "        axes[0, i].set_xlabel('Epoch'); axes[0, i].set_ylabel('Loss')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        # Accuracy row\n",
    "        axes[1, i].plot([h[acc_key] for h in history], color=color, linewidth=2)\n",
    "        axes[1, i].set_title(f'{title} Accuracy')\n",
    "        axes[1, i].set_xlabel('Epoch'); axes[1, i].set_ylabel('Accuracy')\n",
    "        axes[1, i].set_ylim(0, 1); axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Pet Normal Omni Model Training History', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pet_normal_omni_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"  \\u2705 Saved: pet_normal_omni_history.png\")\n",
    "\n",
    "    print(f\"\\n\\U0001f389 Training Finished!\")\n",
    "    print(f\"  Best Average Acc: {best_avg_acc:.4f} ({best_avg_acc*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb70b40",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd408c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Loading best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211/211 [00:00<00:00, 658.49it/s, Materializing param=wav2vec2.masked_spec_embed]                                            \n",
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "project_hid.weight           | UNEXPECTED | \n",
      "project_q.weight             | UNEXPECTED | \n",
      "quantizer.codevectors        | UNEXPECTED | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED | \n",
      "quantizer.weight_proj.weight | UNEXPECTED | \n",
      "project_hid.bias             | UNEXPECTED | \n",
      "project_q.bias               | UNEXPECTED | \n",
      "projector.weight             | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "projector.bias               | MISSING    | \n",
      "classifier.weight            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading TEST datasets...\n",
      "\n",
      "üìä TEST Results:\n",
      "  Behavior Acc: 0.7273 (72.7%)\n",
      "  Emotion Acc:  0.7525 (75.2%)\n",
      "  Sound Acc:    0.9138 (91.4%)\n",
      "  Average Acc:  0.7979 (79.8%)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, get_linear_schedule_with_warmup\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import resnet34, ResNet34_Weights\n",
    "# from PIL import Image\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# from collections import defaultdict, Counter\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "# FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "# class VideoMultiBackbone(nn.Module):\n",
    "#     def __init__(self, num_b, num_e):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         backbone_b = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_b = backbone_b.fc.in_features\n",
    "#         backbone_b.fc = nn.Identity()\n",
    "#         self.behavior_backbone = backbone_b\n",
    "#         self.behavior_head = nn.Linear(in_features_b, num_b)\n",
    "        \n",
    "#         backbone_e = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_e = backbone_e.fc.in_features\n",
    "#         backbone_e.fc = nn.Identity()\n",
    "#         self.emotion_backbone = backbone_e\n",
    "#         self.emotion_head = nn.Linear(in_features_e, num_e)\n",
    "    \n",
    "#     def forward(self, x, task):\n",
    "#         if task == \"behavior\":\n",
    "#             feat = self.behavior_backbone(x)\n",
    "#             return self.behavior_head(feat)\n",
    "#         elif task == \"emotion\":\n",
    "#             feat = self.emotion_backbone(x)\n",
    "#             return self.emotion_head(feat)\n",
    "#         else:\n",
    "#             raise ValueError(\"Task must be 'behavior' or 'emotion'\")\n",
    "        \n",
    "# class AudioModel(nn.Module):\n",
    "#     def __init__(self, num_classes, freeze_backbone=False):  # üî• Í∏∞Î≥∏Í∞í False\n",
    "#         super().__init__()\n",
    "#         self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "#             AUDIO_MODEL_NAME,\n",
    "#             num_labels=num_classes,\n",
    "#             ignore_mismatched_sizes=True\n",
    "#         )\n",
    "        \n",
    "#         # üî• Freeze ÏòµÏÖò (Í∏∞Î≥∏: Ï†ÑÏ≤¥ ÌïôÏäµ)\n",
    "#         if freeze_backbone:\n",
    "#             for param in self.model.wav2vec2.parameters():\n",
    "#                 param.requires_grad = False\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(input_values=x).logits\n",
    "\n",
    "# def test():\n",
    "#     from transformers import Wav2Vec2FeatureExtractor\n",
    "#     FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "#         \"facebook/wav2vec2-base\"\n",
    "#     )\n",
    "\n",
    "#     DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     BATCH_SIZE = 16\n",
    "#     SR = 16000\n",
    "#     MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "#     print(\"üîé Loading best model...\")\n",
    "#     checkpoint = torch.load(\"pet_omni_best.pth\", map_location=DEVICE)\n",
    "\n",
    "#     behavior_label_to_id = checkpoint[\"behavior_label_to_id\"]\n",
    "#     emotion_label_to_id = checkpoint[\"emotion_label_to_id\"]\n",
    "#     sound_label_to_id = checkpoint[\"sound_label_to_id\"]\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Î™®Îç∏ Î≥µÏõê\n",
    "#     # -----------------------------\n",
    "#     video_model = VideoMultiBackbone(\n",
    "#         len(behavior_label_to_id),\n",
    "#         len(emotion_label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     audio_model = AudioModel(\n",
    "#         len(sound_label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     video_model.load_state_dict(checkpoint[\"video_model\"])\n",
    "#     audio_model.load_state_dict(checkpoint[\"audio_model\"])\n",
    "\n",
    "#     video_model.eval()\n",
    "#     audio_model.eval()\n",
    "\n",
    "#     print(\"üì¶ Loading TEST datasets...\")\n",
    "\n",
    "#     TEST_DIR = os.path.join(\"files\", \"work\", \"omni_dataset\", \"test\")\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Train ÏùòÏ°¥ ÏóÜÎäî Dataset Ï†ïÏùò\n",
    "#     # -----------------------------\n",
    "#     class TestImageDataset(Dataset):\n",
    "#         def __init__(self, task_dir, label_to_id):\n",
    "#             self.samples = []\n",
    "#             self.label_to_id = label_to_id\n",
    "\n",
    "#             for label in os.listdir(task_dir):\n",
    "#                 if label not in label_to_id:\n",
    "#                     continue\n",
    "\n",
    "#                 label_dir = os.path.join(task_dir, label)\n",
    "#                 for file in os.listdir(label_dir):\n",
    "#                     if file.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "#                         self.samples.append(\n",
    "#                             (os.path.join(label_dir,file),\n",
    "#                              label_to_id[label])\n",
    "#                         )\n",
    "\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((224,224)),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(\n",
    "#                     [0.485,0.456,0.406],\n",
    "#                     [0.229,0.224,0.225]\n",
    "#                 )\n",
    "#             ])\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.samples)\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             path, label_id = self.samples[idx]\n",
    "#             img = Image.open(path).convert(\"RGB\")\n",
    "#             img = self.transform(img)\n",
    "#             return img, label_id\n",
    "\n",
    "\n",
    "#     class TestAudioDataset(Dataset):\n",
    "#         def __init__(self, task_dir, label_to_id):\n",
    "#             self.samples = []\n",
    "#             self.label_to_id = label_to_id\n",
    "\n",
    "#             for label in os.listdir(task_dir):\n",
    "#                 if label not in label_to_id:\n",
    "#                     continue\n",
    "\n",
    "#                 label_dir = os.path.join(task_dir, label)\n",
    "#                 for file in os.listdir(label_dir):\n",
    "#                     if file.lower().endswith(('.wav','.mp3','.m4a')):\n",
    "#                         self.samples.append(\n",
    "#                             (os.path.join(label_dir,file),\n",
    "#                              label_to_id[label])\n",
    "#                         )\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.samples)\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             path, label_id = self.samples[idx]\n",
    "#             waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "\n",
    "#             if len(waveform) > MAX_AUDIO_LEN:\n",
    "#                 waveform = waveform[:MAX_AUDIO_LEN]\n",
    "#             else:\n",
    "#                 waveform = np.pad(\n",
    "#                     waveform,\n",
    "#                     (0, MAX_AUDIO_LEN - len(waveform))\n",
    "#                 )\n",
    "\n",
    "#             inputs = FEATURE_EXTRACTOR(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=SR,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "\n",
    "#             return inputs.input_values.squeeze(0), label_id\n",
    "\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Loader\n",
    "#     # -----------------------------\n",
    "#     behavior_loader = DataLoader(\n",
    "#         TestImageDataset(\n",
    "#             os.path.join(TEST_DIR,\"behavior\"),\n",
    "#             behavior_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     emotion_loader = DataLoader(\n",
    "#         TestImageDataset(\n",
    "#             os.path.join(TEST_DIR,\"emotion\"),\n",
    "#             emotion_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     sound_loader = DataLoader(\n",
    "#         TestAudioDataset(\n",
    "#             os.path.join(TEST_DIR,\"sound\"),\n",
    "#             sound_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Evaluation\n",
    "#     # -----------------------------\n",
    "#     def evaluate(loader, task):\n",
    "#         correct, total = 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for x, y in loader:\n",
    "#                 x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "#                 if task in [\"behavior\",\"emotion\"]:\n",
    "#                     logits = video_model(x, task)\n",
    "#                 else:\n",
    "#                     logits = audio_model(x)\n",
    "\n",
    "#                 pred = logits.argmax(-1)\n",
    "#                 correct += (pred == y).sum().item()\n",
    "#                 total += y.size(0)\n",
    "\n",
    "#         return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "#     acc_b = evaluate(behavior_loader, \"behavior\")\n",
    "#     acc_e = evaluate(emotion_loader, \"emotion\")\n",
    "#     acc_s = evaluate(sound_loader, \"sound\")\n",
    "\n",
    "#     avg_acc = (acc_b + acc_e + acc_s) / 3\n",
    "\n",
    "#     print(\"\\nüìä TEST Results:\")\n",
    "#     print(f\"  Behavior Acc: {acc_b:.4f} ({acc_b*100:.1f}%)\")\n",
    "#     print(f\"  Emotion Acc:  {acc_e:.4f} ({acc_e*100:.1f}%)\")\n",
    "#     print(f\"  Sound Acc:    {acc_s:.4f} ({acc_s*100:.1f}%)\")\n",
    "#     print(f\"  Average Acc:  {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
