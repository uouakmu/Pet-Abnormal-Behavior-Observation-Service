{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2747e39f",
   "metadata": {},
   "source": [
    "## ìŠ¬ê°œê³¨ í¬í•¨ver. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cap/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Device: cuda:0\n",
      "\n",
      "ğŸ“¦ Collecting behavior...\n",
      "  â†’ 757113 samples, 25 classes\n",
      "  ğŸ¯ Target: 100000 samples\n",
      "  ğŸ“Š 25 classes â†’ max 4000 per class\n",
      "    CAT_ARCH: 2296/2296\n",
      "    CAT_ARMSTRETCH: 4000/38483\n",
      "    CAT_FOOTPUSH: 4000/9517\n",
      "    CAT_GETDOWN: 4000/13421\n",
      "    CAT_GROOMING: 4000/65029\n",
      "    CAT_HEADING: 4000/11237\n",
      "    CAT_LAYDOWN: 4000/21474\n",
      "    CAT_LYING: 4000/12119\n",
      "    CAT_ROLL: 4000/8513\n",
      "    CAT_SITDOWN: 4000/18401\n",
      "    CAT_TAILING: 4000/36960\n",
      "    CAT_WALKRUN: 4000/30498\n",
      "    DOG_BODYLOWER: 4000/79772\n",
      "    DOG_BODYSCRATCH: 4000/15783\n",
      "    DOG_BODYSHAKE: 4000/15296\n",
      "    DOG_FEETUP: 4000/34365\n",
      "    DOG_FOOTUP: 4000/52506\n",
      "    DOG_HEADING: 4000/19052\n",
      "    DOG_LYING: 4000/32129\n",
      "    DOG_MOUNTING: 4000/5211\n",
      "    DOG_SIT: 4000/79182\n",
      "    DOG_TAILING: 4000/35824\n",
      "    DOG_TAILLOW: 4000/8376\n",
      "    DOG_TURN: 4000/21554\n",
      "    DOG_WALKRUN: 4000/90115\n",
      "  âœ… Total sampled: 98296\n",
      "  ğŸ“‹ Splitting & Copying behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Collecting emotion...\n",
      "  â†’ 69113 samples, 10 classes\n",
      "  ğŸ¯ Target: 100000 samples\n",
      "  ğŸ“Š 10 classes â†’ max 10000 per class\n",
      "    cat_attentive: 997/997\n",
      "    cat_happy: 1221/1221\n",
      "    cat_relaxed: 2999/2999\n",
      "    cat_sad: 171/171\n",
      "    dog_angry: 8589/8589\n",
      "    dog_anxious : 10000/11590\n",
      "    dog_confused: 3286/3286\n",
      "    dog_happy: 10000/17355\n",
      "    dog_relaxed: 8699/8699\n",
      "    dog_sad: 10000/14206\n",
      "  âœ… Total sampled: 55962\n",
      "  ğŸ“‹ Splitting & Copying emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Collecting sound...\n",
      "  â†’ 1248 samples, 14 classes\n",
      "  ğŸ¯ Min samples per class: 50\n",
      "    cat_aggressive: 39\n",
      "    cat_huntingMind: 10\n",
      "    cat_mating: 10\n",
      "    cat_paining: 10\n",
      "    cat_positive: 30\n",
      "    dog_bark: 316\n",
      "    dog_breath: 62\n",
      "    dog_cough: 115\n",
      "    dog_growl: 65\n",
      "    dog_howling: 151\n",
      "    dog_playing: 91\n",
      "    dog_sneeze: 110\n",
      "    dog_tracheal_collapse: 89\n",
      "    dog_whining: 150\n",
      "  âœ… Total sampled: 1248\n",
      "  ğŸ“‹ Splitting & Copying sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Collecting patella luxation...\n",
      "  â†’ 100873 samples, 5 classes\n",
      "  â„¹ï¸  Patella: Using all samples\n",
      "  ğŸ“‹ Splitting & Copying patella...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset preparation complete.\n",
      "\n",
      "ğŸ”„ Pre-loading label mappings...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=False\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n",
      "\n",
      "ğŸ”„ Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211/211 [00:00<00:00, 546.70it/s, Materializing param=wav2vec2.masked_spec_embed]                                            \n",
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "project_q.bias               | UNEXPECTED | \n",
      "project_hid.bias             | UNEXPECTED | \n",
      "quantizer.weight_proj.weight | UNEXPECTED | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED | \n",
      "quantizer.codevectors        | UNEXPECTED | \n",
      "project_hid.weight           | UNEXPECTED | \n",
      "project_q.weight             | UNEXPECTED | \n",
      "projector.weight             | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "projector.bias               | MISSING    | \n",
      "classifier.weight            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=False\n",
      "\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 2.6111\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4937\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sound:   0%|          | 0/32 [00:00<?, ?it/s]/tmp/ipykernel_994311/3516768661.py:712: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  audio_scheduler.step()\n",
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5670\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1043\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 2.6111 | Acc 0.4579 (45.8%)\n",
      "  Emotion:  Loss 1.4937 | Acc 0.6477 (64.8%)\n",
      "  Sound:    Loss 1.5670 | Acc 0.2066 (20.7%)\n",
      "  Patella:  Loss 1.1043 | Acc 0.8072 (80.7%)\n",
      "  Average Acc: 0.5298 (53.0%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5298)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 2.0962\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3883\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5122\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8965\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 2.0962 | Acc 0.5472 (54.7%)\n",
      "  Emotion:  Loss 1.3883 | Acc 0.6726 (67.3%)\n",
      "  Sound:    Loss 1.5122 | Acc 0.2562 (25.6%)\n",
      "  Patella:  Loss 0.8965 | Acc 0.8866 (88.7%)\n",
      "  Average Acc: 0.5906 (59.1%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5906)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.8676\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3309\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4512\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8193\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.8676 | Acc 0.5940 (59.4%)\n",
      "  Emotion:  Loss 1.3309 | Acc 0.6931 (69.3%)\n",
      "  Sound:    Loss 1.4512 | Acc 0.2562 (25.6%)\n",
      "  Patella:  Loss 0.8193 | Acc 0.9094 (90.9%)\n",
      "  Average Acc: 0.6132 (61.3%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6132)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.7872\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2912\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3728\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7712\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.7872 | Acc 0.6192 (61.9%)\n",
      "  Emotion:  Loss 1.2912 | Acc 0.7148 (71.5%)\n",
      "  Sound:    Loss 1.3728 | Acc 0.3223 (32.2%)\n",
      "  Patella:  Loss 0.7712 | Acc 0.9249 (92.5%)\n",
      "  Average Acc: 0.6453 (64.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6453)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.6643\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2694\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2622\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7564\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.6643 | Acc 0.6378 (63.8%)\n",
      "  Emotion:  Loss 1.2694 | Acc 0.7010 (70.1%)\n",
      "  Sound:    Loss 1.2622 | Acc 0.3636 (36.4%)\n",
      "  Patella:  Loss 0.7564 | Acc 0.9293 (92.9%)\n",
      "  Average Acc: 0.6579 (65.8%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6579)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5383\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2308\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2335\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7252\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.5383 | Acc 0.6677 (66.8%)\n",
      "  Emotion:  Loss 1.2308 | Acc 0.7124 (71.2%)\n",
      "  Sound:    Loss 1.2335 | Acc 0.4050 (40.5%)\n",
      "  Patella:  Loss 0.7252 | Acc 0.9536 (95.4%)\n",
      "  Average Acc: 0.6847 (68.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6847)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5958\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2159\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1762\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7177\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.5958 | Acc 0.6698 (67.0%)\n",
      "  Emotion:  Loss 1.2159 | Acc 0.7142 (71.4%)\n",
      "  Sound:    Loss 1.1762 | Acc 0.4463 (44.6%)\n",
      "  Patella:  Loss 0.7177 | Acc 0.9561 (95.6%)\n",
      "  Average Acc: 0.6966 (69.7%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6966)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5329\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1984\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1428\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7010\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.5329 | Acc 0.6804 (68.0%)\n",
      "  Emotion:  Loss 1.1984 | Acc 0.7266 (72.7%)\n",
      "  Sound:    Loss 1.1428 | Acc 0.5041 (50.4%)\n",
      "  Patella:  Loss 0.7010 | Acc 0.9600 (96.0%)\n",
      "  Average Acc: 0.7178 (71.8%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7178)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4929\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1916\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2921\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6970\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4929 | Acc 0.6838 (68.4%)\n",
      "  Emotion:  Loss 1.1916 | Acc 0.7278 (72.8%)\n",
      "  Sound:    Loss 1.2921 | Acc 0.5372 (53.7%)\n",
      "  Patella:  Loss 0.6970 | Acc 0.9558 (95.6%)\n",
      "  Average Acc: 0.7261 (72.6%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7261)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4343\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1784\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1791\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6929\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4343 | Acc 0.6891 (68.9%)\n",
      "  Emotion:  Loss 1.1784 | Acc 0.7328 (73.3%)\n",
      "  Sound:    Loss 1.1791 | Acc 0.5372 (53.7%)\n",
      "  Patella:  Loss 0.6929 | Acc 0.9578 (95.8%)\n",
      "  Average Acc: 0.7292 (72.9%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7292)\n",
      "\n",
      "============================================================\n",
      "Epoch 11/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4172\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1626\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0418\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6865\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4172 | Acc 0.7004 (70.0%)\n",
      "  Emotion:  Loss 1.1626 | Acc 0.7357 (73.6%)\n",
      "  Sound:    Loss 1.0418 | Acc 0.6116 (61.2%)\n",
      "  Patella:  Loss 0.6865 | Acc 0.9694 (96.9%)\n",
      "  Average Acc: 0.7543 (75.4%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7543)\n",
      "\n",
      "============================================================\n",
      "Epoch 12/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4223\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1494\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9880\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6878\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4223 | Acc 0.6953 (69.5%)\n",
      "  Emotion:  Loss 1.1494 | Acc 0.7486 (74.9%)\n",
      "  Sound:    Loss 0.9880 | Acc 0.5702 (57.0%)\n",
      "  Patella:  Loss 0.6878 | Acc 0.9659 (96.6%)\n",
      "  Average Acc: 0.7450 (74.5%)\n",
      "\n",
      "============================================================\n",
      "Epoch 13/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3880\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1472\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1041\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6831\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3880 | Acc 0.7038 (70.4%)\n",
      "  Emotion:  Loss 1.1472 | Acc 0.7416 (74.2%)\n",
      "  Sound:    Loss 1.1041 | Acc 0.6033 (60.3%)\n",
      "  Patella:  Loss 0.6831 | Acc 0.9705 (97.0%)\n",
      "  Average Acc: 0.7548 (75.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7548)\n",
      "\n",
      "============================================================\n",
      "Epoch 14/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3690\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1400\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9525\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6814\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3690 | Acc 0.7107 (71.1%)\n",
      "  Emotion:  Loss 1.1400 | Acc 0.7428 (74.3%)\n",
      "  Sound:    Loss 0.9525 | Acc 0.5868 (58.7%)\n",
      "  Patella:  Loss 0.6814 | Acc 0.9641 (96.4%)\n",
      "  Average Acc: 0.7511 (75.1%)\n",
      "\n",
      "============================================================\n",
      "Epoch 15/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3925\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1221\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9243\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6782\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3925 | Acc 0.7054 (70.5%)\n",
      "  Emotion:  Loss 1.1221 | Acc 0.7432 (74.3%)\n",
      "  Sound:    Loss 0.9243 | Acc 0.5620 (56.2%)\n",
      "  Patella:  Loss 0.6782 | Acc 0.9781 (97.8%)\n",
      "  Average Acc: 0.7472 (74.7%)\n",
      "\n",
      "============================================================\n",
      "Epoch 16/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4008\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1198\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9139\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6800\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4008 | Acc 0.7093 (70.9%)\n",
      "  Emotion:  Loss 1.1198 | Acc 0.7371 (73.7%)\n",
      "  Sound:    Loss 0.9139 | Acc 0.6198 (62.0%)\n",
      "  Patella:  Loss 0.6800 | Acc 0.9754 (97.5%)\n",
      "  Average Acc: 0.7604 (76.0%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7604)\n",
      "\n",
      "============================================================\n",
      "Epoch 17/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3693\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1110\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8509\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6776\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3693 | Acc 0.7118 (71.2%)\n",
      "  Emotion:  Loss 1.1110 | Acc 0.7369 (73.7%)\n",
      "  Sound:    Loss 0.8509 | Acc 0.6612 (66.1%)\n",
      "  Patella:  Loss 0.6776 | Acc 0.9778 (97.8%)\n",
      "  Average Acc: 0.7719 (77.2%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7719)\n",
      "\n",
      "============================================================\n",
      "Epoch 18/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3369\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1073\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8261\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6678\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3369 | Acc 0.7078 (70.8%)\n",
      "  Emotion:  Loss 1.1073 | Acc 0.7321 (73.2%)\n",
      "  Sound:    Loss 0.8261 | Acc 0.6529 (65.3%)\n",
      "  Patella:  Loss 0.6678 | Acc 0.9734 (97.3%)\n",
      "  Average Acc: 0.7666 (76.7%)\n",
      "\n",
      "============================================================\n",
      "Epoch 19/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.4254\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1097\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9297\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6707\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.4254 | Acc 0.7054 (70.5%)\n",
      "  Emotion:  Loss 1.1097 | Acc 0.7409 (74.1%)\n",
      "  Sound:    Loss 0.9297 | Acc 0.6612 (66.1%)\n",
      "  Patella:  Loss 0.6707 | Acc 0.9756 (97.6%)\n",
      "  Average Acc: 0.7708 (77.1%)\n",
      "\n",
      "============================================================\n",
      "Epoch 20/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3324\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0966\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7954\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6760\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3324 | Acc 0.7211 (72.1%)\n",
      "  Emotion:  Loss 1.0966 | Acc 0.7502 (75.0%)\n",
      "  Sound:    Loss 0.7954 | Acc 0.6529 (65.3%)\n",
      "  Patella:  Loss 0.6760 | Acc 0.9772 (97.7%)\n",
      "  Average Acc: 0.7753 (77.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.7753)\n",
      "\n",
      "============================================================\n",
      "Epoch 21/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3531\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0999\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7702\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6654\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3531 | Acc 0.7186 (71.9%)\n",
      "  Emotion:  Loss 1.0999 | Acc 0.7538 (75.4%)\n",
      "  Sound:    Loss 0.7702 | Acc 0.6446 (64.5%)\n",
      "  Patella:  Loss 0.6654 | Acc 0.9760 (97.6%)\n",
      "  Average Acc: 0.7733 (77.3%)\n",
      "\n",
      "============================================================\n",
      "Epoch 22/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3485\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0858\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7576\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6626\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3485 | Acc 0.7190 (71.9%)\n",
      "  Emotion:  Loss 1.0858 | Acc 0.7486 (74.9%)\n",
      "  Sound:    Loss 0.7576 | Acc 0.6446 (64.5%)\n",
      "  Patella:  Loss 0.6626 | Acc 0.9757 (97.6%)\n",
      "  Average Acc: 0.7720 (77.2%)\n",
      "\n",
      "============================================================\n",
      "Epoch 23/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3635\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0796\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7225\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6566\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3635 | Acc 0.7182 (71.8%)\n",
      "  Emotion:  Loss 1.0796 | Acc 0.7318 (73.2%)\n",
      "  Sound:    Loss 0.7225 | Acc 0.6364 (63.6%)\n",
      "  Patella:  Loss 0.6566 | Acc 0.9784 (97.8%)\n",
      "  Average Acc: 0.7662 (76.6%)\n",
      "\n",
      "============================================================\n",
      "Epoch 24/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3452\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0690\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7000\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6640\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3452 | Acc 0.7171 (71.7%)\n",
      "  Emotion:  Loss 1.0690 | Acc 0.7398 (74.0%)\n",
      "  Sound:    Loss 0.7000 | Acc 0.6446 (64.5%)\n",
      "  Patella:  Loss 0.6640 | Acc 0.9794 (97.9%)\n",
      "  Average Acc: 0.7702 (77.0%)\n",
      "\n",
      "============================================================\n",
      "Epoch 25/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3904\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0820\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6902\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6637\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3904 | Acc 0.7156 (71.6%)\n",
      "  Emotion:  Loss 1.0820 | Acc 0.7325 (73.2%)\n",
      "  Sound:    Loss 0.6902 | Acc 0.6612 (66.1%)\n",
      "  Patella:  Loss 0.6637 | Acc 0.9782 (97.8%)\n",
      "  Average Acc: 0.7719 (77.2%)\n",
      "\n",
      "============================================================\n",
      "Epoch 26/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3012\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0823\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6590\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6571\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3012 | Acc 0.7215 (72.1%)\n",
      "  Emotion:  Loss 1.0823 | Acc 0.7364 (73.6%)\n",
      "  Sound:    Loss 0.6590 | Acc 0.6446 (64.5%)\n",
      "  Patella:  Loss 0.6571 | Acc 0.9788 (97.9%)\n",
      "  Average Acc: 0.7703 (77.0%)\n",
      "\n",
      "============================================================\n",
      "Epoch 27/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3184\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0738\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6632\n",
      "\n",
      "ğŸ¦´ Training Patella...\n",
      "  ğŸ“Š patella: 80696 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6615\n",
      "\n",
      "ğŸ” Validation...\n",
      "  ğŸ“Š behavior: 5281 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š sound: 121 samples, 14 classes, augment=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ğŸ“Š patella: 10086 samples, 5 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.3184 | Acc 0.7215 (72.1%)\n",
      "  Emotion:  Loss 1.0738 | Acc 0.7305 (73.1%)\n",
      "  Sound:    Loss 0.6632 | Acc 0.6446 (64.5%)\n",
      "  Patella:  Loss 0.6615 | Acc 0.9801 (98.0%)\n",
      "  Average Acc: 0.7692 (76.9%)\n",
      "\n",
      "============================================================\n",
      "Epoch 28/100\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n",
      "  ğŸ“Š behavior: 11843 samples, 25 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3733\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0826\n",
      "\n",
      "ğŸ”Š Training Sound...\n",
      "  ğŸ“Š sound: 995 samples, 14 classes, augment=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 830.25 MiB is free. Including non-PyTorch memory, this process has 7.05 GiB memory in use. Of the allocated memory 4.88 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 911\u001b[39m\n\u001b[32m    908\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Best Average Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_avg_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_avg_acc*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m911\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 706\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    703\u001b[39m     per_sample_w = class_weights_tensor[labels]\n\u001b[32m    704\u001b[39m     loss = (loss * per_sample_w.mean())\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m \u001b[43maudio_scaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m audio_scaler.unscale_(audio_opt)\n\u001b[32m    708\u001b[39m torch.nn.utils.clip_grad_norm_(audio_model.parameters(), \u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/cap/lib/python3.11/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/cap/lib/python3.11/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/cap/lib/python3.11/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 830.25 MiB is free. Including non-PyTorch memory, this process has 7.05 GiB memory in use. Of the allocated memory 4.88 GiB is allocated by PyTorch, and 1.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, get_linear_schedule_with_warmup\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# =========================\n",
    "# 0. ì„¤ì •\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BEHAVIOR_ROOT = \"files/1_Animal_Behavior\"\n",
    "EMOTION_ROOT = \"files/2_Animal_emotions\"\n",
    "SOUND_ROOT = \"files/3_Animal_Sound\"\n",
    "PATELLA_ROOT = \"files/6_Animal_Patella\"\n",
    "WORK_DIR = \"files/work/omni_dataset\"\n",
    "\n",
    "MAX_SAMPLES_BEHAVIOR = 100000\n",
    "MAX_SAMPLES_EMOTION = 100000\n",
    "MIN_SAMPLES_PER_SOUND_CLASS = 50\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR_VIDEO = 5e-5\n",
    "LR_AUDIO = 1e-5\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 24\n",
    "SR = 16000\n",
    "MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "LOSS_WEIGHTS = {\n",
    "    \"behavior\": 1.0,\n",
    "    \"emotion\": 0.8,\n",
    "    \"sound\": 0.6,\n",
    "    \"patella\": 1.0\n",
    "}\n",
    "\n",
    "AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "print(f\"ğŸ¯ Device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# ğŸ”¥ Audio Augmentation\n",
    "# =========================\n",
    "def augment_audio(waveform, p=0.5):\n",
    "    if random.random() > p:\n",
    "        return waveform\n",
    "    \n",
    "    n_steps = random.uniform(-2, 2)\n",
    "    waveform = librosa.effects.pitch_shift(waveform, sr=SR, n_steps=n_steps)\n",
    "    \n",
    "    rate = random.uniform(0.9, 1.1)\n",
    "    stretched = librosa.effects.time_stretch(waveform, rate=rate)\n",
    "    if len(stretched) > MAX_AUDIO_LEN:\n",
    "        stretched = stretched[:MAX_AUDIO_LEN]\n",
    "    else:\n",
    "        stretched = np.pad(stretched, (0, MAX_AUDIO_LEN - len(stretched)))\n",
    "    waveform = stretched\n",
    "    \n",
    "    noise = np.random.normal(0, 0.003, len(waveform))\n",
    "    waveform = waveform * 0.99 + noise\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "# =========================\n",
    "# 1. Dataset Preparation\n",
    "# =========================\n",
    "def collect_samples(root, exts):\n",
    "    samples = []\n",
    "    for class_dir in sorted(os.listdir(root)):\n",
    "        class_path = os.path.join(root, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        \n",
    "        for root_dir, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if any(filename.lower().endswith(ext) for ext in exts):\n",
    "                    file_path = os.path.join(root_dir, filename)\n",
    "                    samples.append((class_dir, file_path))\n",
    "    \n",
    "    print(f\"  â†’ {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "    return samples\n",
    "\n",
    "def collect_patella_samples(root):\n",
    "    samples = []\n",
    "    \n",
    "    for grade in sorted(os.listdir(root)):\n",
    "        grade_path = os.path.join(root, grade)\n",
    "        if not os.path.isdir(grade_path):\n",
    "            continue\n",
    "        \n",
    "        for date_dir in os.listdir(grade_path):\n",
    "            date_path = os.path.join(grade_path, date_dir)\n",
    "            if not os.path.isdir(date_path):\n",
    "                continue\n",
    "            \n",
    "            for direction in ['Back', 'Front', 'Left', 'Right']:\n",
    "                direction_path = os.path.join(date_path, direction)\n",
    "                if not os.path.exists(direction_path):\n",
    "                    continue\n",
    "                \n",
    "                for filename in os.listdir(direction_path):\n",
    "                    if filename.lower().endswith('.jpg'):\n",
    "                        img_path = os.path.join(direction_path, filename)\n",
    "                        json_path = img_path.replace('.jpg', '.json')\n",
    "                        \n",
    "                        if os.path.exists(json_path):\n",
    "                            samples.append((grade, img_path, json_path))\n",
    "    \n",
    "    print(f\"  â†’ {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "    return samples\n",
    "\n",
    "def sample_balanced(samples, max_total_samples):\n",
    "    class_samples = defaultdict(list)\n",
    "    for label, path in samples:\n",
    "        class_samples[label].append(path)\n",
    "    \n",
    "    num_classes = len(class_samples)\n",
    "    max_per_class = max_total_samples // num_classes\n",
    "    \n",
    "    print(f\"  ğŸ¯ Target: {max_total_samples} samples\")\n",
    "    print(f\"  ğŸ“Š {num_classes} classes â†’ max {max_per_class} per class\")\n",
    "    \n",
    "    sampled = []\n",
    "    for label, paths in class_samples.items():\n",
    "        n_samples = min(len(paths), max_per_class)\n",
    "        selected = random.sample(paths, n_samples)\n",
    "        sampled.extend([(label, p) for p in selected])\n",
    "        print(f\"    {label}: {n_samples}/{len(paths)}\")\n",
    "    \n",
    "    print(f\"  âœ… Total sampled: {len(sampled)}\")\n",
    "    return sampled\n",
    "\n",
    "def sample_balanced_audio(samples, min_per_class):\n",
    "    # í´ë˜ìŠ¤ë³„ ì§‘ê³„ (ë³‘í•© ì—†ì´ ì›ë³¸ í´ë˜ìŠ¤ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
    "    class_samples = defaultdict(list)\n",
    "    for label, path in samples:\n",
    "        class_samples[label].append(path)\n",
    "\n",
    "    print(f\"  ğŸ¯ Min samples per class: {min_per_class}\")\n",
    "\n",
    "    sampled = []\n",
    "    for label, paths in sorted(class_samples.items()):\n",
    "        sampled.extend([(label, p) for p in paths])\n",
    "        print(f\"    {label}: {len(paths)}\")\n",
    "\n",
    "    print(f\"  âœ… Total sampled: {len(sampled)}\")\n",
    "    return sampled\n",
    "\n",
    "def split_and_copy(samples, task_name, is_patella=False, original_samples=None):\n",
    "    \"\"\"\n",
    "    original_samples: sound task ì „ìš©. test setì„ ì˜¤ë²„ìƒ˜í”Œ ì´ì „ ì›ë³¸ì—ì„œ ë¶„ë¦¬í•  ë•Œ ì‚¬ìš©.\n",
    "                      ë²„ê·¸ 1 ìˆ˜ì • - ì˜¤ë²„ìƒ˜í”Œëœ poolê³¼ testê°€ ê²¹ì¹˜ëŠ” data leakage ë°©ì§€.\n",
    "    \"\"\"\n",
    "    random.shuffle(samples)\n",
    "    class_samples = defaultdict(list)\n",
    "\n",
    "    if is_patella:\n",
    "        for label, img_path, json_path in samples:\n",
    "            class_samples[label].append((img_path, json_path))\n",
    "    else:\n",
    "        for label, path in samples:\n",
    "            class_samples[label].append(path)\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(WORK_DIR, split, task_name), exist_ok=True)\n",
    "\n",
    "    # âœ… sound: test setì€ ì˜¤ë²„ìƒ˜í”Œ ì´ì „ ì›ë³¸(original_samples)ì—ì„œ ë³„ë„ ì¶”ì¶œ\n",
    "    if original_samples is not None:\n",
    "        orig_class = defaultdict(list)\n",
    "        for label, path in original_samples:\n",
    "            orig_class[label].append(path)\n",
    "        test_items_by_label = {\n",
    "            label: paths[:max(10, len(paths) // 5)]\n",
    "            for label, paths in orig_class.items()\n",
    "        }\n",
    "    else:\n",
    "        test_items_by_label = None\n",
    "\n",
    "    for label, items in class_samples.items():\n",
    "        n = len(items)\n",
    "        n_train = int(n * 0.8)\n",
    "        n_val   = int(n * 0.1)\n",
    "\n",
    "        if test_items_by_label is not None:\n",
    "            # sound: train/valì€ ì˜¤ë²„ìƒ˜í”Œ pool, testëŠ” ì›ë³¸\n",
    "            train_items = items[:n_train]\n",
    "            val_items   = items[n_train:n_train + n_val]\n",
    "            test_items  = test_items_by_label.get(label, [])\n",
    "        else:\n",
    "            train_items = items[:n_train]\n",
    "            val_items   = items[n_train:n_train + n_val]\n",
    "            test_items  = items[n_train + n_val:]\n",
    "\n",
    "        split_map = {\"train\": train_items, \"val\": val_items, \"test\": test_items}\n",
    "\n",
    "        for split_name, split_items in split_map.items():\n",
    "            dst_label_dir = os.path.join(WORK_DIR, split_name, task_name, label)\n",
    "            os.makedirs(dst_label_dir, exist_ok=True)\n",
    "\n",
    "            for item in tqdm(split_items, desc=f\"{task_name}/{split_name}/{label}\", leave=False):\n",
    "                if is_patella:\n",
    "                    img_path, json_path = item\n",
    "                    dst_img  = os.path.join(dst_label_dir, f\"{label}_{os.path.basename(img_path)}\")\n",
    "                    shutil.copy(img_path, dst_img)\n",
    "                    dst_json = dst_img.replace('.jpg', '.json')\n",
    "                    shutil.copy(json_path, dst_json)\n",
    "                else:\n",
    "                    dst_path = os.path.join(dst_label_dir, f\"{label}_{os.path.basename(item)}\")\n",
    "                    shutil.copy(item, dst_path)\n",
    "\n",
    "def _task_ready(task_name):\n",
    "    \"\"\"í•´ë‹¹ taskì˜ train í´ë”ê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ True\"\"\"\n",
    "    task_train = os.path.join(WORK_DIR, \"train\", task_name)\n",
    "    return os.path.isdir(task_train) and len(os.listdir(task_train)) > 0\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    # âœ… taskë³„ ë…ë¦½ ì²´í¬: ì—†ëŠ” taskë§Œ ì„ íƒì ìœ¼ë¡œ ì¤€ë¹„\n",
    "    need_behavior = not _task_ready(\"behavior\")\n",
    "    need_emotion  = not _task_ready(\"emotion\")\n",
    "    need_sound    = not _task_ready(\"sound\")\n",
    "    need_patella  = not _task_ready(\"patella\")\n",
    "\n",
    "    if not any([need_behavior, need_emotion, need_sound, need_patella]):\n",
    "        print(\"âœ… All tasks already prepared, skipping.\")\n",
    "        return\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(WORK_DIR, split), exist_ok=True)\n",
    "\n",
    "    if need_behavior:\n",
    "        print(\"\\nğŸ“¦ Collecting behavior...\")\n",
    "        behavior_all = collect_samples(BEHAVIOR_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "        behavior = sample_balanced(behavior_all, MAX_SAMPLES_BEHAVIOR)\n",
    "        print(\"  ğŸ“‹ Splitting & Copying behavior...\")\n",
    "        split_and_copy(behavior, \"behavior\")\n",
    "    else:\n",
    "        print(\"âœ… behavior already prepared, skipping.\")\n",
    "\n",
    "    if need_emotion:\n",
    "        print(\"\\nğŸ“¦ Collecting emotion...\")\n",
    "        emotion_all = collect_samples(EMOTION_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "        emotion = sample_balanced(emotion_all, MAX_SAMPLES_EMOTION)\n",
    "        print(\"  ğŸ“‹ Splitting & Copying emotion...\")\n",
    "        split_and_copy(emotion, \"emotion\")\n",
    "    else:\n",
    "        print(\"âœ… emotion already prepared, skipping.\")\n",
    "\n",
    "    if need_sound:\n",
    "        print(\"\\nğŸ“¦ Collecting sound...\")\n",
    "        sound_all = collect_samples(SOUND_ROOT, ['.wav', '.mp3', '.m4a'])\n",
    "        sound = sample_balanced_audio(sound_all, MIN_SAMPLES_PER_SOUND_CLASS)\n",
    "        print(\"  ğŸ“‹ Splitting & Copying sound...\")\n",
    "        # âœ… original_samples ì „ë‹¬ë¡œ test set leakage ë°©ì§€ (ë³‘í•© ì—†ì´ ì›ë³¸ ë¼ë²¨ ê·¸ëŒ€ë¡œ)\n",
    "        split_and_copy(sound, \"sound\", original_samples=sound_all)\n",
    "    else:\n",
    "        print(\"âœ… sound already prepared, skipping.\")\n",
    "\n",
    "    if need_patella:\n",
    "        print(\"\\nğŸ“¦ Collecting patella luxation...\")\n",
    "        patella_all = collect_patella_samples(PATELLA_ROOT)\n",
    "        print(\"  â„¹ï¸  Patella: Using all samples\")\n",
    "        print(\"  ğŸ“‹ Splitting & Copying patella...\")\n",
    "        split_and_copy(patella_all, \"patella\", is_patella=True)\n",
    "    else:\n",
    "        print(\"âœ… patella already prepared, skipping.\")\n",
    "\n",
    "    print(\"\\nâœ… Dataset preparation complete.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset Classes\n",
    "# =========================\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        \n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            \n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(label_dir, file), label))\n",
    "        \n",
    "        print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.label_to_id[label]\n",
    "\n",
    "class PatellaDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        \n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            \n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith('.jpg'):\n",
    "                    img_path = os.path.join(label_dir, file)\n",
    "                    json_path = img_path.replace('.jpg', '.json')\n",
    "                    \n",
    "                    if os.path.exists(json_path):\n",
    "                        self.samples.append((img_path, json_path, label))\n",
    "        \n",
    "        print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "        \n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, json_path, label = self.samples[idx]\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        \n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        keypoints = []\n",
    "        for annotation in data.get('annotation_info', []):\n",
    "            x = float(annotation.get('x', 0))\n",
    "            y = float(annotation.get('y', 0))\n",
    "            keypoints.extend([x, y])\n",
    "        \n",
    "        while len(keypoints) < 18:\n",
    "            keypoints.append(0.0)\n",
    "        \n",
    "        keypoints = torch.tensor(keypoints[:18], dtype=torch.float32)\n",
    "        \n",
    "        return img, keypoints, self.label_to_id[label]\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = {}   # âœ… ì—­ë°©í–¥ ë§¤í•‘ ì¶”ê°€\n",
    "        self.augment = augment\n",
    "        next_id = 0\n",
    "\n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "\n",
    "            self.label_to_id[label] = next_id\n",
    "            self.id_to_label[next_id] = label\n",
    "            next_id += 1\n",
    "\n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.wav', '.mp3', '.m4a')):\n",
    "                    self.samples.append((os.path.join(label_dir, file), label))\n",
    "\n",
    "        print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes, augment={augment}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "        except Exception:\n",
    "            waveform = np.zeros(MAX_AUDIO_LEN)\n",
    "\n",
    "        if self.augment:\n",
    "            waveform = augment_audio(waveform)\n",
    "\n",
    "        if len(waveform) > MAX_AUDIO_LEN:\n",
    "            waveform = waveform[:MAX_AUDIO_LEN]\n",
    "        else:\n",
    "            waveform = np.pad(waveform, (0, MAX_AUDIO_LEN - len(waveform)))\n",
    "\n",
    "        inputs = FEATURE_EXTRACTOR(waveform, sampling_rate=SR, return_tensors=\"pt\")\n",
    "        # âœ… dict ë°˜í™˜: collate_fnì´ ì•ˆì „í•˜ê²Œ ìŠ¤íƒí•  ìˆ˜ ìˆë„ë¡\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            \"labels\": torch.tensor(self.label_to_id[label], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn_audio(batch):\n",
    "    \"\"\"AudioDatasetì˜ dict ë°°ì¹˜ë¥¼ ì•ˆì „í•˜ê²Œ í…Œì„ ì„œë¡œ ë³´í˜ˆ\"\"\"\n",
    "    input_values = torch.stack([item[\"input_values\"] for item in batch])\n",
    "    labels       = torch.stack([item[\"labels\"]       for item in batch])\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# =========================\n",
    "# 3. Individual Models (ë…ë¦½ ëª¨ë¸)\n",
    "# =========================\n",
    "class BehaviorModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(feat)\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return self.head(feat)\n",
    "\n",
    "class PatellaModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "        in_features = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features + 18, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, keypoints):\n",
    "        feat = self.backbone(x)\n",
    "        combined = torch.cat([feat, keypoints], dim=1)\n",
    "        return self.head(combined)\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            AUDIO_MODEL_NAME,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.model.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        # âœ… labelsë¥¼ ë„˜ê¸°ë©´ ëª¨ë¸ ë‚´ë¶€ì—ì„œ lossë¥¼ ì§ì ‘ ê³„ì‚° (padding mask ê³ ë ¤)\n",
    "        return self.model(input_values=input_values, labels=labels)\n",
    "\n",
    "# =========================\n",
    "# 4. Helper Functions\n",
    "# =========================\n",
    "def mixup_data(x, y, alpha=0.4):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"ğŸ”¥ ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# =========================\n",
    "# 5. Sequential Training (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )\n",
    "# =========================\n",
    "def train():\n",
    "    prepare_dataset()\n",
    "    \n",
    "    # ğŸ”¥ label_to_id ë¯¸ë¦¬ ë¡œë“œ (ë‚˜ì¤‘ì— ì‚¬ìš©)\n",
    "    print(\"\\nğŸ”„ Pre-loading label mappings...\")\n",
    "    temp_b = ImageDataset(os.path.join(WORK_DIR, \"train\", \"behavior\"), augment=False)\n",
    "    temp_e = ImageDataset(os.path.join(WORK_DIR, \"train\", \"emotion\"), augment=False)\n",
    "    temp_s = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=False)\n",
    "    temp_p = PatellaDataset(os.path.join(WORK_DIR, \"train\", \"patella\"), augment=False)\n",
    "    \n",
    "    behavior_label_to_id = temp_b.label_to_id\n",
    "    emotion_label_to_id  = temp_e.label_to_id\n",
    "    sound_label_to_id    = temp_s.label_to_id\n",
    "    sound_id_to_label    = temp_s.id_to_label    # âœ… ì—­ë°©í–¥ ë§¤í•‘ ì €ì¥\n",
    "    patella_label_to_id  = temp_p.label_to_id\n",
    "    \n",
    "    del temp_b, temp_e, temp_s, temp_p\n",
    "    clear_memory()\n",
    "    \n",
    "    # ğŸ”¥ ëª¨ë¸ ì´ˆê¸°í™” (CPUì— ë¨¼ì € ìƒì„±)\n",
    "    print(\"\\nğŸ”„ Initializing models...\")\n",
    "    behavior_model = BehaviorModel(len(behavior_label_to_id))\n",
    "    emotion_model = EmotionModel(len(emotion_label_to_id))\n",
    "    patella_model = PatellaModel(len(patella_label_to_id))\n",
    "    audio_model = AudioModel(len(sound_label_to_id), freeze_backbone=False)\n",
    "    \n",
    "    # Optimizers (ëª¨ë¸ì´ GPUë¡œ ê°€ê¸° ì „ì— ìƒì„±)\n",
    "    behavior_opt = torch.optim.AdamW(behavior_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "    emotion_opt = torch.optim.AdamW(emotion_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "    patella_opt = torch.optim.AdamW(patella_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "    audio_opt = torch.optim.AdamW(audio_model.parameters(), lr=LR_AUDIO, weight_decay=0.01)\n",
    "\n",
    "    # âœ… Audio LR Warmup Scheduler\n",
    "    # sound loader í¬ê¸°ë¥¼ ë¯¸ë¦¬ ì¶”ì • (epochë‹¹ í•©ì‚°ìœ¼ë¡œ ê²°ì •ë¨)\n",
    "    _temp_sound = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=False)\n",
    "    _approx_sound_steps = (len(_temp_sound) // BATCH_SIZE) * EPOCHS\n",
    "    del _temp_sound\n",
    "    audio_scheduler = get_linear_schedule_with_warmup(\n",
    "        audio_opt,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=_approx_sound_steps\n",
    "    )\n",
    "    clear_memory()\n",
    "\n",
    "    # Scalers\n",
    "    video_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    audio_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    best_avg_acc = 0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        loss_b, loss_e, loss_s, loss_p = 0, 0, 0, 0\n",
    "        \n",
    "        # ========== 1. Behavior ==========\n",
    "        print(f\"\\nğŸ¾ Training Behavior...\")\n",
    "        behavior_model.to(DEVICE)\n",
    "        behavior_model.train()\n",
    "        \n",
    "        behavior_train = ImageDataset(os.path.join(WORK_DIR, \"train\", \"behavior\"), augment=True)\n",
    "        behavior_loader = DataLoader(behavior_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, labels in tqdm(behavior_loader, desc=\"Behavior\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            behavior_opt.zero_grad()  # âœ… zero_grad ì´ë™: forward ì•ìœ¼ë¡œ\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = behavior_model(imgs)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(behavior_opt)\n",
    "            video_scaler.update()\n",
    "            \n",
    "            loss_b += loss.item()\n",
    "        \n",
    "        loss_b /= len(behavior_loader)\n",
    "        print(f\"  â†’ Avg Loss: {loss_b:.4f}\")\n",
    "        \n",
    "        # ğŸ”¥ ë©”ëª¨ë¦¬ í•´ì œ\n",
    "        behavior_model.cpu()\n",
    "        del behavior_train, behavior_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 2. Emotion ==========\n",
    "        print(f\"\\nğŸ˜Š Training Emotion...\")\n",
    "        emotion_model.to(DEVICE)\n",
    "        emotion_model.train()\n",
    "        \n",
    "        emotion_train = ImageDataset(os.path.join(WORK_DIR, \"train\", \"emotion\"), augment=True)\n",
    "        emotion_loader = DataLoader(emotion_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, labels in tqdm(emotion_loader, desc=\"Emotion\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            emotion_opt.zero_grad()  # âœ… zero_grad ì´ë™: forward ì•ìœ¼ë¡œ\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = emotion_model(imgs)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(emotion_opt)\n",
    "            video_scaler.update()\n",
    "            \n",
    "            loss_e += loss.item()\n",
    "        \n",
    "        loss_e /= len(emotion_loader)\n",
    "        print(f\"  â†’ Avg Loss: {loss_e:.4f}\")\n",
    "        \n",
    "        emotion_model.cpu()\n",
    "        del emotion_train, emotion_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 3. Sound ==========\n",
    "        print(f\"\\nğŸ”Š Training Sound...\")\n",
    "        audio_model.to(DEVICE)\n",
    "        audio_model.train()\n",
    "        \n",
    "        sound_train = AudioDataset(os.path.join(WORK_DIR, \"train\", \"sound\"), augment=True)\n",
    "\n",
    "        # âœ… í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (epochë§ˆë‹¤ ê³„ì‚° ìœ ì§€ â€“ í´ë˜ìŠ¤ë³„ ë³„ë„ criterion)\n",
    "        sound_labels_list = [item[1] for item in sound_train.samples]\n",
    "        sound_label_ids   = [sound_train.label_to_id[l] for l in sound_labels_list]\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.arange(len(sound_train.label_to_id)),\n",
    "            y=sound_label_ids\n",
    "        )\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        # âœ… collate_fn_audio ì ìš©\n",
    "        sound_loader = DataLoader(\n",
    "            sound_train, BATCH_SIZE, True,\n",
    "            num_workers=2, pin_memory=True,\n",
    "            collate_fn=collate_fn_audio\n",
    "        )\n",
    "\n",
    "        for batch in tqdm(sound_loader, desc=\"Sound\", leave=False):\n",
    "            audios = batch[\"input_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            audio_opt.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                # âœ… labels ì§ì ‘ ì „ë‹¬ â†’ outputs.loss ì‚¬ìš© (padding mask ê³ ë ¤)\n",
    "                outputs = audio_model(input_values=audios, labels=labels)\n",
    "                # âœ… LOSS_WEIGHTS ìœ ì§€, í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì ìš©\n",
    "                loss = outputs.loss * LOSS_WEIGHTS[\"sound\"]\n",
    "                # class_weightsë¥¼ í™œìš©í•œ ë³´ì • í•­ ì¶”ê°€\n",
    "                per_sample_w = class_weights_tensor[labels]\n",
    "                loss = (loss * per_sample_w.mean())\n",
    "\n",
    "            audio_scaler.scale(loss).backward()\n",
    "            audio_scaler.unscale_(audio_opt)\n",
    "            torch.nn.utils.clip_grad_norm_(audio_model.parameters(), 1.0)\n",
    "            audio_scaler.step(audio_opt)\n",
    "            audio_scaler.update()\n",
    "            # âœ… ìŠ¤ì¼€ì¤„ëŸ¬ step\n",
    "            audio_scheduler.step()\n",
    "\n",
    "            loss_s += loss.item()\n",
    "\n",
    "        loss_s /= len(sound_loader)\n",
    "        print(f\"  â†’ Avg Loss: {loss_s:.4f}\")\n",
    "\n",
    "        audio_model.cpu()\n",
    "        del sound_train, sound_loader, class_weights_tensor\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== 4. Patella ==========\n",
    "        print(f\"\\nğŸ¦´ Training Patella...\")\n",
    "        patella_model.to(DEVICE)\n",
    "        patella_model.train()\n",
    "        \n",
    "        patella_train = PatellaDataset(os.path.join(WORK_DIR, \"train\", \"patella\"), augment=True)\n",
    "        patella_loader = DataLoader(patella_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "        \n",
    "        for imgs, keypoints, labels in tqdm(patella_loader, desc=\"Patella\", leave=False):\n",
    "            imgs, keypoints, labels = imgs.to(DEVICE), keypoints.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            patella_opt.zero_grad()  # âœ… zero_grad ì´ë™: forward ì•ìœ¼ë¡œ\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                imgs, labels_a, labels_b, lam = mixup_data(imgs, labels)\n",
    "                logits = patella_model(imgs, keypoints)\n",
    "                loss = lam * criterion(logits, labels_a) + (1 - lam) * criterion(logits, labels_b)\n",
    "\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(patella_opt)\n",
    "            video_scaler.update()\n",
    "            \n",
    "            loss_p += loss.item()\n",
    "        \n",
    "        loss_p /= len(patella_loader)\n",
    "        print(f\"  â†’ Avg Loss: {loss_p:.4f}\")\n",
    "        \n",
    "        patella_model.cpu()\n",
    "        del patella_train, patella_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # ========== Validation ==========\n",
    "        print(f\"\\nğŸ” Validation...\")\n",
    "        \n",
    "        # Behavior Val\n",
    "        behavior_model.to(DEVICE)\n",
    "        behavior_model.eval()\n",
    "        behavior_val = ImageDataset(os.path.join(WORK_DIR, \"val\", \"behavior\"), augment=False)\n",
    "        behavior_val_loader = DataLoader(behavior_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_b, total_b = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(behavior_val_loader, desc=\"Val Behavior\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = behavior_model(imgs)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_b += (pred == labels).sum().item()\n",
    "                total_b += labels.size(0)\n",
    "        acc_b = correct_b / total_b\n",
    "        \n",
    "        behavior_model.cpu()\n",
    "        del behavior_val, behavior_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Emotion Val\n",
    "        emotion_model.to(DEVICE)\n",
    "        emotion_model.eval()\n",
    "        emotion_val = ImageDataset(os.path.join(WORK_DIR, \"val\", \"emotion\"), augment=False)\n",
    "        emotion_val_loader = DataLoader(emotion_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_e, total_e = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(emotion_val_loader, desc=\"Val Emotion\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = emotion_model(imgs)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_e += (pred == labels).sum().item()\n",
    "                total_e += labels.size(0)\n",
    "        acc_e = correct_e / total_e\n",
    "        \n",
    "        emotion_model.cpu()\n",
    "        del emotion_val, emotion_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Sound Val\n",
    "        audio_model.to(DEVICE)\n",
    "        audio_model.eval()\n",
    "        sound_val = AudioDataset(os.path.join(WORK_DIR, \"val\", \"sound\"), augment=False)\n",
    "        # âœ… collate_fn_audio ì ìš©\n",
    "        sound_val_loader = DataLoader(\n",
    "            sound_val, BATCH_SIZE, False,\n",
    "            num_workers=2, pin_memory=True,\n",
    "            collate_fn=collate_fn_audio\n",
    "        )\n",
    "\n",
    "        correct_s, total_s = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(sound_val_loader, desc=\"Val Sound\", leave=False):\n",
    "                audios = batch[\"input_values\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "                # âœ… outputs.logits ì‚¬ìš©\n",
    "                outputs = audio_model(input_values=audios, labels=labels)\n",
    "                pred = outputs.logits.argmax(-1)\n",
    "                correct_s += (pred == labels).sum().item()\n",
    "                total_s   += labels.size(0)\n",
    "        acc_s = correct_s / total_s\n",
    "        \n",
    "        audio_model.cpu()\n",
    "        del sound_val, sound_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        # Patella Val\n",
    "        patella_model.to(DEVICE)\n",
    "        patella_model.eval()\n",
    "        patella_val = PatellaDataset(os.path.join(WORK_DIR, \"val\", \"patella\"), augment=False)\n",
    "        patella_val_loader = DataLoader(patella_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "        \n",
    "        correct_p, total_p = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, keypoints, labels in tqdm(patella_val_loader, desc=\"Val Patella\", leave=False):\n",
    "                imgs, keypoints, labels = imgs.to(DEVICE), keypoints.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = patella_model(imgs, keypoints)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_p += (pred == labels).sum().item()\n",
    "                total_p += labels.size(0)\n",
    "        acc_p = correct_p / total_p\n",
    "        \n",
    "        patella_model.cpu()\n",
    "        del patella_val, patella_val_loader\n",
    "        clear_memory()\n",
    "        \n",
    "        avg_acc = (acc_b + acc_e + acc_s + acc_p) / 4\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"  Behavior: Loss {loss_b:.4f} | Acc {acc_b:.4f} ({acc_b*100:.1f}%)\")\n",
    "        print(f\"  Emotion:  Loss {loss_e:.4f} | Acc {acc_e:.4f} ({acc_e*100:.1f}%)\")\n",
    "        print(f\"  Sound:    Loss {loss_s:.4f} | Acc {acc_s:.4f} ({acc_s*100:.1f}%)\")\n",
    "        print(f\"  Patella:  Loss {loss_p:.4f} | Acc {acc_p:.4f} ({acc_p*100:.1f}%)\")\n",
    "        print(f\"  Average Acc: {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss_b': loss_b, 'loss_e': loss_e, 'loss_s': loss_s, 'loss_p': loss_p,\n",
    "            'acc_b': acc_b, 'acc_e': acc_e, 'acc_s': acc_s, 'acc_p': acc_p,\n",
    "            'acc_avg': avg_acc\n",
    "        })\n",
    "        \n",
    "        if avg_acc > best_avg_acc:\n",
    "            best_avg_acc = avg_acc\n",
    "            \n",
    "            # ğŸ”¥ ëª¨ë¸ë“¤ì„ CPUë¡œ ì˜®ê¸´ í›„ ì €ì¥\n",
    "            torch.save({\n",
    "                \"behavior_model\":       behavior_model.state_dict(),\n",
    "                \"emotion_model\":        emotion_model.state_dict(),\n",
    "                \"audio_model\":          audio_model.state_dict(),\n",
    "                \"patella_model\":        patella_model.state_dict(),\n",
    "                \"behavior_label_to_id\": behavior_label_to_id,\n",
    "                \"emotion_label_to_id\":  emotion_label_to_id,\n",
    "                \"sound_label_to_id\":    sound_label_to_id,\n",
    "                \"sound_id_to_label\":    sound_id_to_label,    # âœ… ì—­ë°©í–¥ ë§¤í•‘ ì¶”ê°€\n",
    "                \"patella_label_to_id\":  patella_label_to_id,\n",
    "                \"best_epoch\":           epoch + 1,\n",
    "                \"best_acc\":             best_avg_acc,\n",
    "                \"history\":              history\n",
    "            }, \"pet_normal_omni_best.pth\")\n",
    "            print(f\"  ğŸ’¾ Saved new best model! (Acc: {best_avg_acc:.4f})\")\n",
    "    \n",
    "    # ê·¸ë˜í”„\n",
    "    print(\"\\nğŸ“ˆ Generating training history plot...\")\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    plt.subplot(141)\n",
    "    plt.plot([h['acc_b'] for h in history], 'b-', label='Behavior', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Behavior Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    \n",
    "    plt.subplot(142)\n",
    "    plt.plot([h['acc_e'] for h in history], 'r-', label='Emotion', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Emotion Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    \n",
    "    plt.subplot(143)\n",
    "    plt.plot([h['acc_s'] for h in history], 'g-', label='Sound', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Sound Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    \n",
    "    plt.subplot(144)\n",
    "    plt.plot([h['acc_p'] for h in history], 'purple', label='Patella', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Patella Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pet_omni_sequential_history.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"  âœ… Saved: pet_omni_sequential_history.png\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Training Finished!\")\n",
    "    print(f\"  Best Average Acc: {best_avg_acc:.4f} ({best_avg_acc*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb70b40",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd408c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Loading best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211/211 [00:00<00:00, 658.49it/s, Materializing param=wav2vec2.masked_spec_embed]                                            \n",
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "project_hid.weight           | UNEXPECTED | \n",
      "project_q.weight             | UNEXPECTED | \n",
      "quantizer.codevectors        | UNEXPECTED | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED | \n",
      "quantizer.weight_proj.weight | UNEXPECTED | \n",
      "project_hid.bias             | UNEXPECTED | \n",
      "project_q.bias               | UNEXPECTED | \n",
      "projector.weight             | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "projector.bias               | MISSING    | \n",
      "classifier.weight            | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loading TEST datasets...\n",
      "\n",
      "ğŸ“Š TEST Results:\n",
      "  Behavior Acc: 0.7273 (72.7%)\n",
      "  Emotion Acc:  0.7525 (75.2%)\n",
      "  Sound Acc:    0.9138 (91.4%)\n",
      "  Average Acc:  0.7979 (79.8%)\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, get_linear_schedule_with_warmup\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import resnet34, ResNet34_Weights\n",
    "# from PIL import Image\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# from collections import defaultdict, Counter\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "# FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "# class VideoMultiBackbone(nn.Module):\n",
    "#     def __init__(self, num_b, num_e):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         backbone_b = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_b = backbone_b.fc.in_features\n",
    "#         backbone_b.fc = nn.Identity()\n",
    "#         self.behavior_backbone = backbone_b\n",
    "#         self.behavior_head = nn.Linear(in_features_b, num_b)\n",
    "        \n",
    "#         backbone_e = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "#         in_features_e = backbone_e.fc.in_features\n",
    "#         backbone_e.fc = nn.Identity()\n",
    "#         self.emotion_backbone = backbone_e\n",
    "#         self.emotion_head = nn.Linear(in_features_e, num_e)\n",
    "    \n",
    "#     def forward(self, x, task):\n",
    "#         if task == \"behavior\":\n",
    "#             feat = self.behavior_backbone(x)\n",
    "#             return self.behavior_head(feat)\n",
    "#         elif task == \"emotion\":\n",
    "#             feat = self.emotion_backbone(x)\n",
    "#             return self.emotion_head(feat)\n",
    "#         else:\n",
    "#             raise ValueError(\"Task must be 'behavior' or 'emotion'\")\n",
    "        \n",
    "# class AudioModel(nn.Module):\n",
    "#     def __init__(self, num_classes, freeze_backbone=False):  # ğŸ”¥ ê¸°ë³¸ê°’ False\n",
    "#         super().__init__()\n",
    "#         self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "#             AUDIO_MODEL_NAME,\n",
    "#             num_labels=num_classes,\n",
    "#             ignore_mismatched_sizes=True\n",
    "#         )\n",
    "        \n",
    "#         # ğŸ”¥ Freeze ì˜µì…˜ (ê¸°ë³¸: ì „ì²´ í•™ìŠµ)\n",
    "#         if freeze_backbone:\n",
    "#             for param in self.model.wav2vec2.parameters():\n",
    "#                 param.requires_grad = False\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(input_values=x).logits\n",
    "\n",
    "# def test():\n",
    "#     from transformers import Wav2Vec2FeatureExtractor\n",
    "#     FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "#         \"facebook/wav2vec2-base\"\n",
    "#     )\n",
    "\n",
    "#     DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     BATCH_SIZE = 16\n",
    "#     SR = 16000\n",
    "#     MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "#     print(\"ğŸ” Loading best model...\")\n",
    "#     checkpoint = torch.load(\"pet_omni_best.pth\", map_location=DEVICE)\n",
    "\n",
    "#     behavior_label_to_id = checkpoint[\"behavior_label_to_id\"]\n",
    "#     emotion_label_to_id = checkpoint[\"emotion_label_to_id\"]\n",
    "#     sound_label_to_id = checkpoint[\"sound_label_to_id\"]\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # ëª¨ë¸ ë³µì›\n",
    "#     # -----------------------------\n",
    "#     video_model = VideoMultiBackbone(\n",
    "#         len(behavior_label_to_id),\n",
    "#         len(emotion_label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     audio_model = AudioModel(\n",
    "#         len(sound_label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     video_model.load_state_dict(checkpoint[\"video_model\"])\n",
    "#     audio_model.load_state_dict(checkpoint[\"audio_model\"])\n",
    "\n",
    "#     video_model.eval()\n",
    "#     audio_model.eval()\n",
    "\n",
    "#     print(\"ğŸ“¦ Loading TEST datasets...\")\n",
    "\n",
    "#     TEST_DIR = os.path.join(\"files\", \"work\", \"omni_dataset\", \"test\")\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Train ì˜ì¡´ ì—†ëŠ” Dataset ì •ì˜\n",
    "#     # -----------------------------\n",
    "#     class TestImageDataset(Dataset):\n",
    "#         def __init__(self, task_dir, label_to_id):\n",
    "#             self.samples = []\n",
    "#             self.label_to_id = label_to_id\n",
    "\n",
    "#             for label in os.listdir(task_dir):\n",
    "#                 if label not in label_to_id:\n",
    "#                     continue\n",
    "\n",
    "#                 label_dir = os.path.join(task_dir, label)\n",
    "#                 for file in os.listdir(label_dir):\n",
    "#                     if file.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "#                         self.samples.append(\n",
    "#                             (os.path.join(label_dir,file),\n",
    "#                              label_to_id[label])\n",
    "#                         )\n",
    "\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((224,224)),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize(\n",
    "#                     [0.485,0.456,0.406],\n",
    "#                     [0.229,0.224,0.225]\n",
    "#                 )\n",
    "#             ])\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.samples)\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             path, label_id = self.samples[idx]\n",
    "#             img = Image.open(path).convert(\"RGB\")\n",
    "#             img = self.transform(img)\n",
    "#             return img, label_id\n",
    "\n",
    "\n",
    "#     class TestAudioDataset(Dataset):\n",
    "#         def __init__(self, task_dir, label_to_id):\n",
    "#             self.samples = []\n",
    "#             self.label_to_id = label_to_id\n",
    "\n",
    "#             for label in os.listdir(task_dir):\n",
    "#                 if label not in label_to_id:\n",
    "#                     continue\n",
    "\n",
    "#                 label_dir = os.path.join(task_dir, label)\n",
    "#                 for file in os.listdir(label_dir):\n",
    "#                     if file.lower().endswith(('.wav','.mp3','.m4a')):\n",
    "#                         self.samples.append(\n",
    "#                             (os.path.join(label_dir,file),\n",
    "#                              label_to_id[label])\n",
    "#                         )\n",
    "\n",
    "#         def __len__(self):\n",
    "#             return len(self.samples)\n",
    "\n",
    "#         def __getitem__(self, idx):\n",
    "#             path, label_id = self.samples[idx]\n",
    "#             waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "\n",
    "#             if len(waveform) > MAX_AUDIO_LEN:\n",
    "#                 waveform = waveform[:MAX_AUDIO_LEN]\n",
    "#             else:\n",
    "#                 waveform = np.pad(\n",
    "#                     waveform,\n",
    "#                     (0, MAX_AUDIO_LEN - len(waveform))\n",
    "#                 )\n",
    "\n",
    "#             inputs = FEATURE_EXTRACTOR(\n",
    "#                 waveform,\n",
    "#                 sampling_rate=SR,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "\n",
    "#             return inputs.input_values.squeeze(0), label_id\n",
    "\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Loader\n",
    "#     # -----------------------------\n",
    "#     behavior_loader = DataLoader(\n",
    "#         TestImageDataset(\n",
    "#             os.path.join(TEST_DIR,\"behavior\"),\n",
    "#             behavior_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     emotion_loader = DataLoader(\n",
    "#         TestImageDataset(\n",
    "#             os.path.join(TEST_DIR,\"emotion\"),\n",
    "#             emotion_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     sound_loader = DataLoader(\n",
    "#         TestAudioDataset(\n",
    "#             os.path.join(TEST_DIR,\"sound\"),\n",
    "#             sound_label_to_id\n",
    "#         ),\n",
    "#         BATCH_SIZE, False\n",
    "#     )\n",
    "\n",
    "#     # -----------------------------\n",
    "#     # Evaluation\n",
    "#     # -----------------------------\n",
    "#     def evaluate(loader, task):\n",
    "#         correct, total = 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for x, y in loader:\n",
    "#                 x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "#                 if task in [\"behavior\",\"emotion\"]:\n",
    "#                     logits = video_model(x, task)\n",
    "#                 else:\n",
    "#                     logits = audio_model(x)\n",
    "\n",
    "#                 pred = logits.argmax(-1)\n",
    "#                 correct += (pred == y).sum().item()\n",
    "#                 total += y.size(0)\n",
    "\n",
    "#         return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "#     acc_b = evaluate(behavior_loader, \"behavior\")\n",
    "#     acc_e = evaluate(emotion_loader, \"emotion\")\n",
    "#     acc_s = evaluate(sound_loader, \"sound\")\n",
    "\n",
    "#     avg_acc = (acc_b + acc_e + acc_s) / 3\n",
    "\n",
    "#     print(\"\\nğŸ“Š TEST Results:\")\n",
    "#     print(f\"  Behavior Acc: {acc_b:.4f} ({acc_b*100:.1f}%)\")\n",
    "#     print(f\"  Emotion Acc:  {acc_e:.4f} ({acc_e*100:.1f}%)\")\n",
    "#     print(f\"  Sound Acc:    {acc_s:.4f} ({acc_s*100:.1f}%)\")\n",
    "#     print(f\"  Average Acc:  {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
