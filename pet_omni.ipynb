{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b8343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "# import torchvision.transforms as transforms\n",
    "# from torchvision.models import resnet18, ResNet18_Weights\n",
    "# from PIL import Image\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# import json\n",
    "\n",
    "# # =========================\n",
    "# # 0. ì„¤ì •\n",
    "# # =========================\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# # ê²½ë¡œ\n",
    "# BEHAVIOR_ROOT = \"files/1_Animal_Behavior\"\n",
    "# EMOTION_ROOT = \"files/2_Animal_emotions\"\n",
    "# SOUND_ROOT = \"files/3_Animal_Sound\"\n",
    "# WORK_DIR = \"files/work/omni_dataset\"\n",
    "\n",
    "# # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "# BATCH_SIZE = 16\n",
    "# EPOCHS = 100\n",
    "# LR_VIDEO = 1e-4\n",
    "# LR_AUDIO = 1e-5\n",
    "# DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "# NUM_WORKERS = 4\n",
    "# SR = 16000\n",
    "# MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "# # Loss Weighting\n",
    "# LOSS_WEIGHTS = {\n",
    "#     'behavior': 1.0,\n",
    "#     'emotion': 0.8,\n",
    "#     'sound': 0.6\n",
    "# }\n",
    "\n",
    "# print(f\"ðŸŽ¯ Device: {DEVICE}, Workers: {NUM_WORKERS}\")\n",
    "\n",
    "# # Audio Feature Extractor\n",
    "# AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "# FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "# # =========================\n",
    "# # 1. ë°ì´í„°ì…‹ ì¤€ë¹„ (ê¸°ì¡´ ë™ì¼)\n",
    "# # =========================\n",
    "# def prepare_omni_dataset():\n",
    "#     \"\"\"behavior + emotion ì´ë¯¸ì§€, sound ì˜¤ë””ì˜¤ë¥¼ í†µí•© ë””ë ‰í† ë¦¬ë¡œ êµ¬ì„±\"\"\"\n",
    "#     if os.path.exists(WORK_DIR):\n",
    "#         shutil.rmtree(WORK_DIR)\n",
    "    \n",
    "#     for split in ['train', 'val', 'test']:\n",
    "#         os.makedirs(os.path.join(WORK_DIR, split, 'behavior'), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(WORK_DIR, split, 'emotion'), exist_ok=True)\n",
    "#         os.makedirs(os.path.join(WORK_DIR, split, 'sound'), exist_ok=True)\n",
    "    \n",
    "#     print(\"ðŸ“¦ Collecting behavior data...\")\n",
    "#     behavior_samples = collect_samples(BEHAVIOR_ROOT, exts=['.jpg', '.png', '.jpeg'])\n",
    "    \n",
    "#     print(\"ðŸ“¦ Collecting emotion data...\")\n",
    "#     emotion_samples = collect_samples(EMOTION_ROOT, exts=['.jpg', '.png', '.jpeg'])\n",
    "    \n",
    "#     print(\"ðŸ“¦ Collecting sound data...\")\n",
    "#     sound_samples = collect_samples(SOUND_ROOT, exts=['.wav', '.mp3', '.m4a'])\n",
    "    \n",
    "#     # 8:1:1 split\n",
    "#     split_and_copy(behavior_samples, 'behavior', img=True)\n",
    "#     split_and_copy(emotion_samples, 'emotion', img=True)\n",
    "#     split_and_copy(sound_samples, 'sound', img=False)\n",
    "    \n",
    "#     print(\"âœ… Omni dataset ready!\")\n",
    "\n",
    "# def collect_samples(root, exts):\n",
    "#     samples = []\n",
    "#     for class_dir in os.listdir(root):\n",
    "#         class_path = os.path.join(root, class_dir)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "        \n",
    "#         for filename in os.listdir(class_path):\n",
    "#             if any(filename.lower().endswith(ext) for ext in exts):\n",
    "#                 samples.append((class_dir, os.path.join(class_path, filename)))\n",
    "    \n",
    "#     print(f\"  â†’ {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "#     return samples\n",
    "\n",
    "# def split_and_copy(samples, task_name, img=True):\n",
    "#     random.shuffle(samples)\n",
    "    \n",
    "#     class_samples = defaultdict(list)\n",
    "#     for label, path in samples:\n",
    "#         class_samples[label].append(path)\n",
    "    \n",
    "#     train_files, val_files, test_files = [], [], []\n",
    "#     for label, paths in class_samples.items():\n",
    "#         n = len(paths)\n",
    "#         n_train = int(n * 0.8)\n",
    "#         n_val = int(n * 0.1)\n",
    "        \n",
    "#         train_files.extend([(label, p) for p in paths[:n_train]])\n",
    "#         val_files.extend([(label, p) for p in paths[n_train:n_train+n_val]])\n",
    "#         test_files.extend([(label, p) for p in paths[n_train+n_val:]])\n",
    "    \n",
    "#     for split_name, files in [('train', train_files), ('val', val_files), ('test', test_files)]:\n",
    "#         print(f\"  Copying {task_name}/{split_name}: {len(files)}\")\n",
    "#         for label, src in tqdm(files, desc=f\"{task_name}/{split_name}\"):\n",
    "#             dst_dir = os.path.join(WORK_DIR, split_name, task_name, label)\n",
    "#             os.makedirs(dst_dir, exist_ok=True)\n",
    "#             shutil.copy(src, os.path.join(dst_dir, os.path.basename(src)))\n",
    "\n",
    "# # =========================\n",
    "# # 2. íƒœìŠ¤í¬ë³„ Dataset (ë¶„ë¦¬)\n",
    "# # =========================\n",
    "# class ImageDataset(Dataset):\n",
    "#     \"\"\"Behavior / Emotionìš© ì´ë¯¸ì§€ ë°ì´í„°ì…‹\"\"\"\n",
    "#     def __init__(self, task_dir, augment=False):\n",
    "#         self.samples = []\n",
    "#         self.label_to_id = {}\n",
    "        \n",
    "#         for label in sorted(os.listdir(task_dir)):\n",
    "#             label_dir = os.path.join(task_dir, label)\n",
    "#             if not os.path.isdir(label_dir):\n",
    "#                 continue\n",
    "            \n",
    "#             if label not in self.label_to_id:\n",
    "#                 self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "#             for filename in os.listdir(label_dir):\n",
    "#                 if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "#                     self.samples.append((os.path.join(label_dir, filename), label))\n",
    "        \n",
    "#         # ì „ì²˜ë¦¬\n",
    "#         if augment:\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((256, 256)),\n",
    "#                 transforms.RandomCrop(224),\n",
    "#                 transforms.RandomHorizontalFlip(),\n",
    "#                 transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#             ])\n",
    "#         else:\n",
    "#             self.transform = transforms.Compose([\n",
    "#                 transforms.Resize((224, 224)),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#             ])\n",
    "        \n",
    "#         print(f\"  ðŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         path, label = self.samples[idx]\n",
    "#         img = Image.open(path).convert('RGB')\n",
    "#         img_tensor = self.transform(img)\n",
    "#         label_id = self.label_to_id[label]\n",
    "#         return img_tensor, label_id\n",
    "\n",
    "# class AudioDataset(Dataset):\n",
    "#     \"\"\"Soundìš© ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹\"\"\"\n",
    "#     def __init__(self, task_dir, augment=False):\n",
    "#         self.samples = []\n",
    "#         self.label_to_id = {}\n",
    "#         self.augment = augment\n",
    "        \n",
    "#         for label in sorted(os.listdir(task_dir)):\n",
    "#             label_dir = os.path.join(task_dir, label)\n",
    "#             if not os.path.isdir(label_dir):\n",
    "#                 continue\n",
    "            \n",
    "#             if label not in self.label_to_id:\n",
    "#                 self.label_to_id[label] = len(self.label_to_id)\n",
    "            \n",
    "#             for filename in os.listdir(label_dir):\n",
    "#                 if filename.lower().endswith(('.wav', '.mp3', '.m4a')):\n",
    "#                     self.samples.append((os.path.join(label_dir, filename), label))\n",
    "        \n",
    "#         print(f\"  ðŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         path, label = self.samples[idx]\n",
    "        \n",
    "#         try:\n",
    "#             waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "#         except:\n",
    "#             waveform = np.zeros(MAX_AUDIO_LEN)\n",
    "        \n",
    "#         # ì¦ê°• (ì„ íƒ)\n",
    "#         if self.augment and random.random() > 0.5:\n",
    "#             # Pitch shift\n",
    "#             n_steps = random.uniform(-2, 2)\n",
    "#             waveform = librosa.effects.pitch_shift(waveform, sr=SR, n_steps=n_steps)\n",
    "        \n",
    "#         # Padding/Truncate\n",
    "#         if len(waveform) > MAX_AUDIO_LEN:\n",
    "#             waveform = waveform[:MAX_AUDIO_LEN]\n",
    "#         else:\n",
    "#             waveform = np.pad(waveform, (0, MAX_AUDIO_LEN - len(waveform)))\n",
    "        \n",
    "#         inputs = FEATURE_EXTRACTOR(waveform, sampling_rate=SR, return_tensors=\"pt\")\n",
    "#         audio_tensor = inputs.input_values.squeeze(0)\n",
    "#         label_id = self.label_to_id[label]\n",
    "        \n",
    "#         return audio_tensor, label_id\n",
    "\n",
    "# # =========================\n",
    "# # 3. ëª¨ë¸ ì •ì˜ (ê°œì„ )\n",
    "# # =========================\n",
    "# class VideoMultiHeadModel(nn.Module):\n",
    "#     \"\"\"Vision Backbone + Behavior/Emotion Heads\"\"\"\n",
    "#     def __init__(self, num_behavior_classes, num_emotion_classes):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Shared Backbone\n",
    "#         weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "#         backbone = resnet18(weights=weights)\n",
    "#         self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # 512-dim\n",
    "        \n",
    "#         # Task Heads\n",
    "#         self.behavior_head = nn.Linear(512, num_behavior_classes)\n",
    "#         self.emotion_head = nn.Linear(512, num_emotion_classes)\n",
    "    \n",
    "#     def forward(self, x, task='behavior'):\n",
    "#         \"\"\"taskë³„ë¡œ ì ì ˆí•œ head ì„ íƒ\"\"\"\n",
    "#         feat = self.backbone(x).squeeze(-1).squeeze(-1)  # [B, 512]\n",
    "        \n",
    "#         if task == 'behavior':\n",
    "#             return self.behavior_head(feat)\n",
    "#         elif task == 'emotion':\n",
    "#             return self.emotion_head(feat)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown task: {task}\")\n",
    "\n",
    "# class AudioModel(nn.Module):\n",
    "#     \"\"\"ë…ë¦½ Audio ëª¨ë¸\"\"\"\n",
    "#     def __init__(self, num_sound_classes):\n",
    "#         super().__init__()\n",
    "#         self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "#             AUDIO_MODEL_NAME,\n",
    "#             num_labels=num_sound_classes,\n",
    "#             ignore_mismatched_sizes=True\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, audio):\n",
    "#         return self.model(input_values=audio).logits\n",
    "\n",
    "# # =========================\n",
    "# # 4. Task Alternating í•™ìŠµ\n",
    "# # =========================\n",
    "# def train():\n",
    "#     \"\"\"Taskë³„ë¡œ ë²ˆê°ˆì•„ê°€ë©° í•™ìŠµ\"\"\"\n",
    "#     prepare_omni_dataset()\n",
    "    \n",
    "#     # ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "#     print(\"\\nðŸ”„ Loading datasets...\")\n",
    "#     behavior_train = ImageDataset(os.path.join(WORK_DIR, 'train', 'behavior'), augment=True)\n",
    "#     behavior_val = ImageDataset(os.path.join(WORK_DIR, 'val', 'behavior'), augment=False)\n",
    "    \n",
    "#     emotion_train = ImageDataset(os.path.join(WORK_DIR, 'train', 'emotion'), augment=True)\n",
    "#     emotion_val = ImageDataset(os.path.join(WORK_DIR, 'val', 'emotion'), augment=False)\n",
    "    \n",
    "#     sound_train = AudioDataset(os.path.join(WORK_DIR, 'train', 'sound'), augment=True)\n",
    "#     sound_val = AudioDataset(os.path.join(WORK_DIR, 'val', 'sound'), augment=False)\n",
    "    \n",
    "#     # DataLoader\n",
    "#     behavior_train_loader = DataLoader(behavior_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "#                                       num_workers=NUM_WORKERS, pin_memory=True)\n",
    "#     behavior_val_loader = DataLoader(behavior_val, batch_size=BATCH_SIZE, shuffle=False, \n",
    "#                                     num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "    \n",
    "#     emotion_train_loader = DataLoader(emotion_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "#                                      num_workers=NUM_WORKERS, pin_memory=True)\n",
    "#     emotion_val_loader = DataLoader(emotion_val, batch_size=BATCH_SIZE, shuffle=False, \n",
    "#                                    num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "    \n",
    "#     sound_train_loader = DataLoader(sound_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "#                                    num_workers=2, pin_memory=True)\n",
    "#     sound_val_loader = DataLoader(sound_val, batch_size=BATCH_SIZE, shuffle=False, \n",
    "#                                  num_workers=1, pin_memory=True)\n",
    "    \n",
    "#     # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#     video_model = VideoMultiHeadModel(\n",
    "#         num_behavior_classes=len(behavior_train.label_to_id),\n",
    "#         num_emotion_classes=len(emotion_train.label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "    \n",
    "#     audio_model = AudioModel(\n",
    "#         num_sound_classes=len(sound_train.label_to_id)\n",
    "#     ).to(DEVICE)\n",
    "    \n",
    "#     # Optimizer (ë¶„ë¦¬)\n",
    "#     video_optimizer = torch.optim.AdamW(video_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "#     audio_optimizer = torch.optim.AdamW(audio_model.parameters(), lr=LR_AUDIO, weight_decay=0.01)\n",
    "    \n",
    "#     # Loss\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     scaler = torch.amp.GradScaler('cuda')\n",
    "    \n",
    "#     # Task Iteration Tracker\n",
    "#     task_iters = {\n",
    "#         'behavior': iter(behavior_train_loader),\n",
    "#         'emotion': iter(emotion_train_loader),\n",
    "#         'sound': iter(sound_train_loader)\n",
    "#     }\n",
    "    \n",
    "#     # í•™ìŠµ ë£¨í”„\n",
    "#     best_avg_acc = 0\n",
    "#     history = []\n",
    "    \n",
    "#     # Epochë‹¹ ì´ iteration (ê°€ìž¥ ê¸´ íƒœìŠ¤í¬ ê¸°ì¤€)\n",
    "#     max_iters = max(len(behavior_train_loader), len(emotion_train_loader), len(sound_train_loader))\n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "#         video_model.train()\n",
    "#         audio_model.train()\n",
    "        \n",
    "#         # Taskë³„ loss ëˆ„ì \n",
    "#         task_losses = {'behavior': 0, 'emotion': 0, 'sound': 0}\n",
    "#         task_counts = {'behavior': 0, 'emotion': 0, 'sound': 0}\n",
    "        \n",
    "#         pbar = tqdm(range(max_iters), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "#         for step in pbar:\n",
    "#             # Task ìˆœí™˜: behavior â†’ emotion â†’ sound\n",
    "#             current_task = ['behavior', 'emotion', 'sound'][step % 3]\n",
    "            \n",
    "#             # í•´ë‹¹ taskì˜ batch ê°€ì ¸ì˜¤ê¸°\n",
    "#             try:\n",
    "#                 if current_task == 'behavior':\n",
    "#                     imgs, labels = next(task_iters['behavior'])\n",
    "#                 elif current_task == 'emotion':\n",
    "#                     imgs, labels = next(task_iters['emotion'])\n",
    "#                 else:  # sound\n",
    "#                     audios, labels = next(task_iters['sound'])\n",
    "#             except StopIteration:\n",
    "#                 # Iterator ìž¬ìƒì„±\n",
    "#                 if current_task == 'behavior':\n",
    "#                     task_iters['behavior'] = iter(behavior_train_loader)\n",
    "#                     imgs, labels = next(task_iters['behavior'])\n",
    "#                 elif current_task == 'emotion':\n",
    "#                     task_iters['emotion'] = iter(emotion_train_loader)\n",
    "#                     imgs, labels = next(task_iters['emotion'])\n",
    "#                 else:\n",
    "#                     task_iters['sound'] = iter(sound_train_loader)\n",
    "#                     audios, labels = next(task_iters['sound'])\n",
    "            \n",
    "#             # GPU ì´ë™\n",
    "#             labels = labels.to(DEVICE)\n",
    "            \n",
    "#             # Forward + Loss\n",
    "#             with torch.amp.autocast('cuda'):\n",
    "#                 if current_task in ['behavior', 'emotion']:\n",
    "#                     imgs = imgs.to(DEVICE)\n",
    "#                     logits = video_model(imgs, task=current_task)\n",
    "#                     loss = criterion(logits, labels) * LOSS_WEIGHTS[current_task]\n",
    "                    \n",
    "#                     video_optimizer.zero_grad()\n",
    "#                     scaler.scale(loss).backward()\n",
    "#                     scaler.step(video_optimizer)\n",
    "#                     scaler.update()\n",
    "                    \n",
    "#                 else:  # sound\n",
    "#                     audios = audios.to(DEVICE)\n",
    "#                     logits = audio_model(audios)\n",
    "#                     loss = criterion(logits, labels) * LOSS_WEIGHTS['sound']\n",
    "                    \n",
    "#                     audio_optimizer.zero_grad()\n",
    "#                     scaler.scale(loss).backward()\n",
    "#                     scaler.step(audio_optimizer)\n",
    "#                     scaler.update()\n",
    "            \n",
    "#             task_losses[current_task] += loss.item()\n",
    "#             task_counts[current_task] += 1\n",
    "            \n",
    "#             pbar.set_postfix({\n",
    "#                 'task': current_task,\n",
    "#                 'loss': loss.item()\n",
    "#             })\n",
    "        \n",
    "#         # Validation\n",
    "#         print(\"\\nðŸ” Validation...\")\n",
    "#         video_model.eval()\n",
    "#         audio_model.eval()\n",
    "        \n",
    "#         # Behavior Val\n",
    "#         correct_b, total_b = 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for imgs, labels in behavior_val_loader:\n",
    "#                 imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "#                 logits = video_model(imgs, task='behavior')\n",
    "#                 pred = logits.argmax(-1)\n",
    "#                 correct_b += (pred == labels).sum().item()\n",
    "#                 total_b += labels.size(0)\n",
    "#         acc_b = correct_b / total_b\n",
    "        \n",
    "#         # Emotion Val\n",
    "#         correct_e, total_e = 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for imgs, labels in emotion_val_loader:\n",
    "#                 imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "#                 logits = video_model(imgs, task='emotion')\n",
    "#                 pred = logits.argmax(-1)\n",
    "#                 correct_e += (pred == labels).sum().item()\n",
    "#                 total_e += labels.size(0)\n",
    "#         acc_e = correct_e / total_e\n",
    "        \n",
    "#         # Sound Val\n",
    "#         correct_s, total_s = 0, 0\n",
    "#         with torch.no_grad():\n",
    "#             for audios, labels in sound_val_loader:\n",
    "#                 audios, labels = audios.to(DEVICE), labels.to(DEVICE)\n",
    "#                 logits = audio_model(audios)\n",
    "#                 pred = logits.argmax(-1)\n",
    "#                 correct_s += (pred == labels).sum().item()\n",
    "#                 total_s += labels.size(0)\n",
    "#         acc_s = correct_s / total_s\n",
    "        \n",
    "#         avg_acc = (acc_b + acc_e + acc_s) / 3\n",
    "        \n",
    "#         print(f\"ðŸ“Š E{epoch+1}: Behavior {acc_b:.4f}, Emotion {acc_e:.4f}, Sound {acc_s:.4f}, Avg {avg_acc:.4f}\")\n",
    "        \n",
    "#         history.append({\n",
    "#             'epoch': epoch+1,\n",
    "#             'loss_behavior': task_losses['behavior'] / task_counts['behavior'],\n",
    "#             'loss_emotion': task_losses['emotion'] / task_counts['emotion'],\n",
    "#             'loss_sound': task_losses['sound'] / task_counts['sound'],\n",
    "#             'acc_behavior': acc_b,\n",
    "#             'acc_emotion': acc_e,\n",
    "#             'acc_sound': acc_s,\n",
    "#             'acc_avg': avg_acc\n",
    "#         })\n",
    "        \n",
    "#         # ì €ìž¥\n",
    "#         if avg_acc > best_avg_acc:\n",
    "#             best_avg_acc = avg_acc\n",
    "#             torch.save({\n",
    "#                 'video_model_state_dict': video_model.state_dict(),\n",
    "#                 'audio_model_state_dict': audio_model.state_dict(),\n",
    "#                 'behavior_label_to_id': behavior_train.label_to_id,\n",
    "#                 'emotion_label_to_id': emotion_train.label_to_id,\n",
    "#                 'sound_label_to_id': sound_train.label_to_id,\n",
    "#                 'best_epoch': epoch+1,\n",
    "#                 'best_acc': best_avg_acc,\n",
    "#                 'history': history\n",
    "#             }, 'pet_omni_v2_best.pth')\n",
    "#             print(f\"ðŸ’¾ New best: {best_avg_acc:.4f}\")\n",
    "    \n",
    "#     # ê·¸ëž˜í”„\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "    \n",
    "#     plt.subplot(131)\n",
    "#     plt.plot([h['acc_behavior'] for h in history], 'b-', label='Behavior', linewidth=2)\n",
    "#     plt.legend(); plt.title('Behavior Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.subplot(132)\n",
    "#     plt.plot([h['acc_emotion'] for h in history], 'r-', label='Emotion', linewidth=2)\n",
    "#     plt.legend(); plt.title('Emotion Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.subplot(133)\n",
    "#     plt.plot([h['acc_sound'] for h in history], 'g-', label='Sound', linewidth=2)\n",
    "#     plt.legend(); plt.title('Sound Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('pet_omni_v2_history.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "    \n",
    "#     print(f\"\\nðŸŽ‰ Best Avg Acc: {best_avg_acc:.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
