{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790f7a53",
   "metadata": {},
   "source": [
    "## í´ë˜ìŠ¤ë³„ íŒŒì¼ ê°œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a2acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               JPG Count\n",
      "dog_happy          17355\n",
      "dog_sad            14206\n",
      "dog_anxious        11590\n",
      "dog_relaxed         8699\n",
      "dog_angry           8564\n",
      "dog_confused        3286\n",
      "cat_relaxed         2999\n",
      "cat_happy           1221\n",
      "cat_attentive        997\n",
      "cat_sad              171\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd  # í…Œì´ë¸” ë³´ê¸° í¸í•¨\n",
    "\n",
    "root_dir = Path('./files/2_Animal_emotions')\n",
    "jpg_counts = Counter()\n",
    "\n",
    "# 1_Animal_Behavior ë°”ë¡œ ì•„ë˜ í´ë”ë§Œ ëŒ€ìƒ\n",
    "for subdir in root_dir.iterdir():\n",
    "    if subdir.is_dir():\n",
    "        jpg_count = len(list(subdir.rglob('*.jpg')))\n",
    "        jpg_counts[subdir.name] = jpg_count\n",
    "\n",
    "# í…Œì´ë¸” ì¶œë ¥\n",
    "df = pd.DataFrame.from_dict(jpg_counts, orient='index', columns=['JPG Count']).sort_values('JPG Count', ascending=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7a330",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Device: cuda:1\n",
      "\n",
      "ğŸ“¦ Collecting behavior...\n",
      "  â†’ 757113 samples, 25 classes\n",
      "  ğŸ¯ Target: 100000 samples\n",
      "  ğŸ“Š 25 classes â†’ max 4000 per class\n",
      "    CAT_ARCH: 2296/2296\n",
      "    CAT_ARMSTRETCH: 4000/38483\n",
      "    CAT_FOOTPUSH: 4000/9517\n",
      "    CAT_GETDOWN: 4000/13421\n",
      "    CAT_GROOMING: 4000/65029\n",
      "    CAT_HEADING: 4000/11237\n",
      "    CAT_LAYDOWN: 4000/21474\n",
      "    CAT_LYING: 4000/12119\n",
      "    CAT_ROLL: 4000/8513\n",
      "    CAT_SITDOWN: 4000/18401\n",
      "    CAT_TAILING: 4000/36960\n",
      "    CAT_WALKRUN: 4000/30498\n",
      "    DOG_BODYLOWER: 4000/79772\n",
      "    DOG_BODYSCRATCH: 4000/15783\n",
      "    DOG_BODYSHAKE: 4000/15296\n",
      "    DOG_FEETUP: 4000/34365\n",
      "    DOG_FOOTUP: 4000/52506\n",
      "    DOG_HEADING: 4000/19052\n",
      "    DOG_LYING: 4000/32129\n",
      "    DOG_MOUNTING: 4000/5211\n",
      "    DOG_SIT: 4000/79182\n",
      "    DOG_TAILING: 4000/35824\n",
      "    DOG_TAILLOW: 4000/8376\n",
      "    DOG_TURN: 4000/21554\n",
      "    DOG_WALKRUN: 4000/90115\n",
      "  âœ… Total sampled: 98296\n",
      "\n",
      "ğŸ“¦ Collecting emotion...\n",
      "  â†’ 69113 samples, 10 classes\n",
      "  ğŸ¯ Target: 100000 samples\n",
      "  ğŸ“Š 10 classes â†’ max 10000 per class\n",
      "    cat_attentive: 997/997\n",
      "    cat_happy: 1221/1221\n",
      "    cat_relaxed: 2999/2999\n",
      "    cat_sad: 171/171\n",
      "    dog_angry: 8589/8589\n",
      "    dog_anxious : 10000/11590\n",
      "    dog_confused: 3286/3286\n",
      "    dog_happy: 10000/17355\n",
      "    dog_relaxed: 8699/8699\n",
      "    dog_sad: 10000/14206\n",
      "  âœ… Total sampled: 55962\n",
      "\n",
      "ğŸ“¦ Collecting sound...\n",
      "  â†’ 1248 samples, 16 classes\n",
      "  â„¹ï¸  Sound: Using all samples (no limit)\n",
      "\n",
      "ğŸ“‹ Splitting & Copying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Dataset ready\n",
      "\n",
      "ğŸ”„ Loading datasets...\n",
      "  ğŸ“Š behavior: 11864 samples, 25 classes\n",
      "  ğŸ“Š behavior: 5301 samples, 25 classes\n",
      "  ğŸ“Š emotion: 44766 samples, 10 classes\n",
      "  ğŸ“Š emotion: 5592 samples, 10 classes\n",
      "  ğŸ“Š sound: 995 samples, 16 classes\n",
      "  ğŸ“Š sound: 121 samples, 16 classes\n",
      "\n",
      "ğŸ“¦ DataLoaders:\n",
      "  Behavior: 742 train batches, 332 val batches\n",
      "  Emotion: 2798 train batches, 350 val batches\n",
      "  Sound: 63 train batches, 8 val batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211/211 [00:00<00:00, 476.34it/s, Materializing param=wav2vec2.masked_spec_embed]                                            \n",
      "\u001b[1mWav2Vec2ForSequenceClassification LOAD REPORT\u001b[0m from: facebook/wav2vec2-base\n",
      "Key                          | Status     | \n",
      "-----------------------------+------------+-\n",
      "quantizer.weight_proj.weight | UNEXPECTED | \n",
      "quantizer.codevectors        | UNEXPECTED | \n",
      "project_hid.bias             | UNEXPECTED | \n",
      "project_q.weight             | UNEXPECTED | \n",
      "project_q.bias               | UNEXPECTED | \n",
      "quantizer.weight_proj.bias   | UNEXPECTED | \n",
      "project_hid.weight           | UNEXPECTED | \n",
      "projector.bias               | MISSING    | \n",
      "classifier.weight            | MISSING    | \n",
      "classifier.bias              | MISSING    | \n",
      "projector.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 2.0376\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8022\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5678\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 2.0376 | Acc 0.1519 (15.2%)\n",
      "  Emotion:  Loss 0.8022 | Acc 0.6345 (63.4%)\n",
      "  Sound:    Loss 1.5678 | Acc 0.2562 (25.6%)\n",
      "  Average Acc: 0.3475 (34.8%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.3475)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.5044\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6758\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.3802\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.5044 | Acc 0.2635 (26.4%)\n",
      "  Emotion:  Loss 0.6758 | Acc 0.6615 (66.1%)\n",
      "  Sound:    Loss 1.3802 | Acc 0.4132 (41.3%)\n",
      "  Average Acc: 0.4461 (44.6%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.4461)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.1222\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6140\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.2275\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 1.1222 | Acc 0.3228 (32.3%)\n",
      "  Emotion:  Loss 0.6140 | Acc 0.6786 (67.9%)\n",
      "  Sound:    Loss 1.2275 | Acc 0.5537 (55.4%)\n",
      "  Average Acc: 0.5184 (51.8%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5184)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8633\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.5621\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 1.0982\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.8633 | Acc 0.3765 (37.7%)\n",
      "  Emotion:  Loss 0.5621 | Acc 0.6883 (68.8%)\n",
      "  Sound:    Loss 1.0982 | Acc 0.5455 (54.5%)\n",
      "  Average Acc: 0.5368 (53.7%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5368)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6780\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.5196\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.9934\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.6780 | Acc 0.4256 (42.6%)\n",
      "  Emotion:  Loss 0.5196 | Acc 0.6942 (69.4%)\n",
      "  Sound:    Loss 0.9934 | Acc 0.5950 (59.5%)\n",
      "  Average Acc: 0.5716 (57.2%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5716)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.5440\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.4834\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8960\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.5440 | Acc 0.4554 (45.5%)\n",
      "  Emotion:  Loss 0.4834 | Acc 0.6924 (69.2%)\n",
      "  Sound:    Loss 0.8960 | Acc 0.6364 (63.6%)\n",
      "  Average Acc: 0.5947 (59.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.5947)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.4418\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.4532\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.8220\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.4418 | Acc 0.4790 (47.9%)\n",
      "  Emotion:  Loss 0.4532 | Acc 0.7065 (70.7%)\n",
      "  Sound:    Loss 0.8220 | Acc 0.6446 (64.5%)\n",
      "  Average Acc: 0.6100 (61.0%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6100)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.3689\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.4270\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.7481\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.3689 | Acc 0.4743 (47.4%)\n",
      "  Emotion:  Loss 0.4270 | Acc 0.7117 (71.2%)\n",
      "  Sound:    Loss 0.7481 | Acc 0.6612 (66.1%)\n",
      "  Average Acc: 0.6157 (61.6%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6157)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.3067\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.4016\n",
      "\n",
      "ğŸ”Š Training Sound...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.6675\n",
      "\n",
      "ğŸ” Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Results:\n",
      "  Behavior: Loss 0.3067 | Acc 0.5127 (51.3%)\n",
      "  Emotion:  Loss 0.4016 | Acc 0.7121 (71.2%)\n",
      "  Sound:    Loss 0.6675 | Acc 0.7107 (71.1%)\n",
      "  Average Acc: 0.6452 (64.5%)\n",
      "  ğŸ’¾ Saved new best model! (Acc: 0.6452)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/30\n",
      "============================================================\n",
      "\n",
      "ğŸ¾ Training Behavior...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  â†’ Avg Loss: 0.2727\n",
      "\n",
      "ğŸ˜Š Training Emotion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emotion:  18%|â–ˆâ–Š        | 503/2798 [00:15<01:01, 37.17it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import librosa\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# =========================\n",
    "# 0. ì„¤ì •\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BEHAVIOR_ROOT = \"files/1_Animal_Behavior\"\n",
    "EMOTION_ROOT = \"files/2_Animal_emotions\"\n",
    "SOUND_ROOT = \"files/3_Animal_Sound\"\n",
    "WORK_DIR = \"files/work/omni_dataset\"\n",
    "\n",
    "# ğŸ”¥ ìƒ˜í”Œ ì œí•œ ì„¤ì •\n",
    "MAX_SAMPLES_BEHAVIOR = 100000  # Behavior ì´ ìƒ˜í”Œ ìˆ˜\n",
    "MAX_SAMPLES_EMOTION = 100000    # Emotion ì´ ìƒ˜í”Œ ìˆ˜\n",
    "# SoundëŠ” ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR_VIDEO = 1e-4\n",
    "LR_AUDIO = 1e-5\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_WORKERS = 8\n",
    "SR = 16000\n",
    "MAX_AUDIO_LEN = SR * 5\n",
    "\n",
    "LOSS_WEIGHTS = {\n",
    "    \"behavior\": 1.0,\n",
    "    \"emotion\": 0.8,\n",
    "    \"sound\": 0.6\n",
    "}\n",
    "\n",
    "AUDIO_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)\n",
    "\n",
    "print(f\"ğŸ¯ Device: {DEVICE}\")\n",
    "\n",
    "# =========================\n",
    "# 1. Dataset Preparation (ê°œì„ )\n",
    "# =========================\n",
    "def collect_samples(root, exts):\n",
    "    \"\"\"ëª¨ë“  ìƒ˜í”Œ ìˆ˜ì§‘\"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for class_dir in sorted(os.listdir(root)):\n",
    "        class_path = os.path.join(root, class_dir)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for root_dir, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if any(filename.lower().endswith(ext) for ext in exts):\n",
    "                    file_path = os.path.join(root_dir, filename)\n",
    "                    samples.append((class_dir, file_path))\n",
    "\n",
    "    print(f\"  â†’ {len(samples)} samples, {len(set(s[0] for s in samples))} classes\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "def sample_balanced(samples, max_total_samples):\n",
    "    \"\"\"\n",
    "    í´ë˜ìŠ¤ë³„ ê· ë“± ìƒ˜í”Œë§\n",
    "    ê° í´ë˜ìŠ¤ë‹¹ max_samples_per_class = max_total_samples / num_classes\n",
    "    \"\"\"\n",
    "    # í´ë˜ìŠ¤ë³„ ê·¸ë£¹í™”\n",
    "    class_samples = defaultdict(list)\n",
    "    for label, path in samples:\n",
    "        class_samples[label].append(path)\n",
    "    \n",
    "    num_classes = len(class_samples)\n",
    "    max_per_class = max_total_samples // num_classes\n",
    "    \n",
    "    print(f\"  ğŸ¯ Target: {max_total_samples} samples\")\n",
    "    print(f\"  ğŸ“Š {num_classes} classes â†’ max {max_per_class} per class\")\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œë§\n",
    "    sampled = []\n",
    "    for label, paths in class_samples.items():\n",
    "        n_samples = min(len(paths), max_per_class)\n",
    "        selected = random.sample(paths, n_samples)\n",
    "        sampled.extend([(label, p) for p in selected])\n",
    "        print(f\"    {label}: {n_samples}/{len(paths)}\")\n",
    "    \n",
    "    print(f\"  âœ… Total sampled: {len(sampled)}\")\n",
    "    return sampled\n",
    "\n",
    "\n",
    "def split_and_copy(samples, task_name):\n",
    "    \"\"\"8:1:1 split í›„ ë³µì‚¬\"\"\"\n",
    "    random.shuffle(samples)\n",
    "    class_samples = defaultdict(list)\n",
    "\n",
    "    for label, path in samples:\n",
    "        class_samples[label].append(path)\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(WORK_DIR, split, task_name), exist_ok=True)\n",
    "\n",
    "    for label, paths in class_samples.items():\n",
    "        n = len(paths)\n",
    "        n_train = int(n * 0.8)\n",
    "        n_val = int(n * 0.1)\n",
    "\n",
    "        splits = {\n",
    "            \"train\": paths[:n_train],\n",
    "            \"val\": paths[n_train:n_train+n_val],\n",
    "            \"test\": paths[n_train+n_val:]\n",
    "        }\n",
    "\n",
    "        for split_name, split_paths in splits.items():\n",
    "            for src in tqdm(split_paths, desc=f\"{task_name}/{split_name}/{label}\", leave=False):\n",
    "                dst_dir = os.path.join(WORK_DIR, split_name, task_name, label)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "                dst_path = os.path.join(\n",
    "                    dst_dir,\n",
    "                    f\"{label}_{os.path.basename(src)}\"\n",
    "                )\n",
    "                shutil.copy(src, dst_path)\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"ë°ì´í„°ì…‹ ì¤€ë¹„ (ìƒ˜í”Œ ì œí•œ ì ìš©)\"\"\"\n",
    "    if os.path.exists(WORK_DIR):\n",
    "        shutil.rmtree(WORK_DIR)\n",
    "\n",
    "    print(\"\\nğŸ“¦ Collecting behavior...\")\n",
    "    behavior_all = collect_samples(BEHAVIOR_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "    behavior = sample_balanced(behavior_all, MAX_SAMPLES_BEHAVIOR)\n",
    "\n",
    "    print(\"\\nğŸ“¦ Collecting emotion...\")\n",
    "    emotion_all = collect_samples(EMOTION_ROOT, ['.jpg', '.png', '.jpeg'])\n",
    "    emotion = sample_balanced(emotion_all, MAX_SAMPLES_EMOTION)\n",
    "\n",
    "    print(\"\\nğŸ“¦ Collecting sound...\")\n",
    "    sound = collect_samples(SOUND_ROOT, ['.wav', '.mp3', '.m4a'])\n",
    "    print(\"  â„¹ï¸  Sound: Using all samples (no limit)\")\n",
    "\n",
    "    print(\"\\nğŸ“‹ Splitting & Copying...\")\n",
    "    split_and_copy(behavior, \"behavior\")\n",
    "    split_and_copy(emotion, \"emotion\")\n",
    "    split_and_copy(sound, \"sound\")\n",
    "\n",
    "    print(\"\\nâœ… Dataset ready\")\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset Classes\n",
    "# =========================\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, task_dir, augment=False):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "\n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "\n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "\n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                    self.samples.append((os.path.join(label_dir, file), label))\n",
    "\n",
    "        print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "\n",
    "        if augment:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((224,224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.label_to_id[label]\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, task_dir):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "\n",
    "        for label in sorted(os.listdir(task_dir)):\n",
    "            label_dir = os.path.join(task_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "\n",
    "            self.label_to_id[label] = len(self.label_to_id)\n",
    "\n",
    "            for file in os.listdir(label_dir):\n",
    "                if file.lower().endswith(('.wav','.mp3','.m4a')):\n",
    "                    self.samples.append((os.path.join(label_dir,file), label))\n",
    "\n",
    "        print(f\"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            waveform, _ = librosa.load(path, sr=SR, mono=True)\n",
    "        except:\n",
    "            waveform = np.zeros(MAX_AUDIO_LEN)\n",
    "\n",
    "        if len(waveform) > MAX_AUDIO_LEN:\n",
    "            waveform = waveform[:MAX_AUDIO_LEN]\n",
    "        else:\n",
    "            waveform = np.pad(waveform,(0,MAX_AUDIO_LEN-len(waveform)))\n",
    "\n",
    "        inputs = FEATURE_EXTRACTOR(waveform, sampling_rate=SR, return_tensors=\"pt\")\n",
    "        return inputs.input_values.squeeze(0), self.label_to_id[label]\n",
    "\n",
    "# =========================\n",
    "# 3. Models\n",
    "# =========================\n",
    "class VideoMultiHead(nn.Module):\n",
    "    def __init__(self, num_b, num_e):\n",
    "        super().__init__()\n",
    "        backbone = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.behavior_head = nn.Linear(512, num_b)\n",
    "        self.emotion_head = nn.Linear(512, num_e)\n",
    "\n",
    "    def forward(self, x, task):\n",
    "        feat = self.backbone(x).squeeze(-1).squeeze(-1)\n",
    "        if task == \"behavior\":\n",
    "            return self.behavior_head(feat)\n",
    "        else:\n",
    "            return self.emotion_head(feat)\n",
    "\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            AUDIO_MODEL_NAME,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.model.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(input_values=x).logits\n",
    "\n",
    "# =========================\n",
    "# 4. Training (ì§„í–‰ ìƒí™© ì¶”ì  ì¶”ê°€)\n",
    "# =========================\n",
    "def train():\n",
    "    prepare_dataset()\n",
    "\n",
    "    print(\"\\nğŸ”„ Loading datasets...\")\n",
    "    behavior_train = ImageDataset(os.path.join(WORK_DIR,\"train\",\"behavior\"), augment=True)\n",
    "    behavior_val = ImageDataset(os.path.join(WORK_DIR,\"val\",\"behavior\"), augment=False)\n",
    "    \n",
    "    emotion_train = ImageDataset(os.path.join(WORK_DIR,\"train\",\"emotion\"), augment=True)\n",
    "    emotion_val = ImageDataset(os.path.join(WORK_DIR,\"val\",\"emotion\"), augment=False)\n",
    "    \n",
    "    sound_train = AudioDataset(os.path.join(WORK_DIR,\"train\",\"sound\"))\n",
    "    sound_val = AudioDataset(os.path.join(WORK_DIR,\"val\",\"sound\"))\n",
    "\n",
    "    behavior_train_loader = DataLoader(behavior_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    behavior_val_loader = DataLoader(behavior_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "    \n",
    "    emotion_train_loader = DataLoader(emotion_train, BATCH_SIZE, True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    emotion_val_loader = DataLoader(emotion_val, BATCH_SIZE, False, num_workers=NUM_WORKERS//2, pin_memory=True)\n",
    "    \n",
    "    sound_train_loader = DataLoader(sound_train, BATCH_SIZE, True, num_workers=2, pin_memory=True)\n",
    "    sound_val_loader = DataLoader(sound_val, BATCH_SIZE, False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    print(f\"\\nğŸ“¦ DataLoaders:\")\n",
    "    print(f\"  Behavior: {len(behavior_train_loader)} train batches, {len(behavior_val_loader)} val batches\")\n",
    "    print(f\"  Emotion: {len(emotion_train_loader)} train batches, {len(emotion_val_loader)} val batches\")\n",
    "    print(f\"  Sound: {len(sound_train_loader)} train batches, {len(sound_val_loader)} val batches\")\n",
    "\n",
    "    # ëª¨ë¸\n",
    "    video_model = VideoMultiHead(\n",
    "        len(behavior_train.label_to_id),\n",
    "        len(emotion_train.label_to_id)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    audio_model = AudioModel(\n",
    "        len(sound_train.label_to_id),\n",
    "        # freeze_backbone=True\n",
    "        freeze_backbone=False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Optimizer\n",
    "    video_opt = torch.optim.AdamW(video_model.parameters(), lr=LR_VIDEO, weight_decay=0.01)\n",
    "    audio_opt = torch.optim.AdamW(audio_model.parameters(), lr=LR_AUDIO, weight_decay=0.01)\n",
    "\n",
    "    # Scaler\n",
    "    video_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    audio_scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_avg_acc = 0\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        video_model.train()\n",
    "        audio_model.train()\n",
    "\n",
    "        # Taskë³„ loss ëˆ„ì \n",
    "        loss_b_total, loss_e_total, loss_s_total = 0, 0, 0\n",
    "\n",
    "        # -------- 1. Behavior --------\n",
    "        print(f\"\\nğŸ¾ Training Behavior...\")\n",
    "        for imgs, labels in tqdm(behavior_train_loader, desc=\"Behavior\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = video_model(imgs, \"behavior\")\n",
    "                loss = criterion(logits, labels) * LOSS_WEIGHTS[\"behavior\"]\n",
    "\n",
    "            video_opt.zero_grad()\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(video_opt)\n",
    "            video_scaler.update()\n",
    "\n",
    "            loss_b_total += loss.item()\n",
    "\n",
    "        avg_loss_b = loss_b_total / len(behavior_train_loader)\n",
    "        print(f\"  â†’ Avg Loss: {avg_loss_b:.4f}\")\n",
    "\n",
    "        # -------- 2. Emotion --------\n",
    "        print(f\"\\nğŸ˜Š Training Emotion...\")\n",
    "        for imgs, labels in tqdm(emotion_train_loader, desc=\"Emotion\", leave=False):\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = video_model(imgs, \"emotion\")\n",
    "                loss = criterion(logits, labels) * LOSS_WEIGHTS[\"emotion\"]\n",
    "\n",
    "            video_opt.zero_grad()\n",
    "            video_scaler.scale(loss).backward()\n",
    "            video_scaler.step(video_opt)\n",
    "            video_scaler.update()\n",
    "\n",
    "            loss_e_total += loss.item()\n",
    "\n",
    "        avg_loss_e = loss_e_total / len(emotion_train_loader)\n",
    "        print(f\"  â†’ Avg Loss: {avg_loss_e:.4f}\")\n",
    "\n",
    "        # -------- 3. Sound --------\n",
    "        print(f\"\\nğŸ”Š Training Sound...\")\n",
    "        for audios, labels in tqdm(sound_train_loader, desc=\"Sound\", leave=False):\n",
    "            audios, labels = audios.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = audio_model(audios)\n",
    "                loss = criterion(logits, labels) * LOSS_WEIGHTS[\"sound\"]\n",
    "\n",
    "            audio_opt.zero_grad()\n",
    "            audio_scaler.scale(loss).backward()\n",
    "            audio_scaler.step(audio_opt)\n",
    "            audio_scaler.update()\n",
    "\n",
    "            loss_s_total += loss.item()\n",
    "\n",
    "        avg_loss_s = loss_s_total / len(sound_train_loader)\n",
    "        print(f\"  â†’ Avg Loss: {avg_loss_s:.4f}\")\n",
    "\n",
    "        # -------- Validation --------\n",
    "        print(f\"\\nğŸ” Validation...\")\n",
    "        video_model.eval()\n",
    "        audio_model.eval()\n",
    "\n",
    "        # Behavior Val\n",
    "        correct_b, total_b = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(behavior_val_loader, desc=\"Val Behavior\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = video_model(imgs, \"behavior\")\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_b += (pred == labels).sum().item()\n",
    "                total_b += labels.size(0)\n",
    "        acc_b = correct_b / total_b\n",
    "\n",
    "        # Emotion Val\n",
    "        correct_e, total_e = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(emotion_val_loader, desc=\"Val Emotion\", leave=False):\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = video_model(imgs, \"emotion\")\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_e += (pred == labels).sum().item()\n",
    "                total_e += labels.size(0)\n",
    "        acc_e = correct_e / total_e\n",
    "\n",
    "        # Sound Val\n",
    "        correct_s, total_s = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for audios, labels in tqdm(sound_val_loader, desc=\"Val Sound\", leave=False):\n",
    "                audios, labels = audios.to(DEVICE), labels.to(DEVICE)\n",
    "                logits = audio_model(audios)\n",
    "                pred = logits.argmax(-1)\n",
    "                correct_s += (pred == labels).sum().item()\n",
    "                total_s += labels.size(0)\n",
    "        acc_s = correct_s / total_s\n",
    "\n",
    "        avg_acc = (acc_b + acc_e + acc_s) / 3\n",
    "\n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"  Behavior: Loss {avg_loss_b:.4f} | Acc {acc_b:.4f} ({acc_b*100:.1f}%)\")\n",
    "        print(f\"  Emotion:  Loss {avg_loss_e:.4f} | Acc {acc_e:.4f} ({acc_e*100:.1f}%)\")\n",
    "        print(f\"  Sound:    Loss {avg_loss_s:.4f} | Acc {acc_s:.4f} ({acc_s*100:.1f}%)\")\n",
    "        print(f\"  Average Acc: {avg_acc:.4f} ({avg_acc*100:.1f}%)\")\n",
    "\n",
    "        # History ì €ì¥\n",
    "        history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'loss_b': avg_loss_b,\n",
    "            'loss_e': avg_loss_e,\n",
    "            'loss_s': avg_loss_s,\n",
    "            'acc_b': acc_b,\n",
    "            'acc_e': acc_e,\n",
    "            'acc_s': acc_s,\n",
    "            'acc_avg': avg_acc\n",
    "        })\n",
    "\n",
    "        # Best ëª¨ë¸ ì €ì¥\n",
    "        if avg_acc > best_avg_acc:\n",
    "            best_avg_acc = avg_acc\n",
    "            torch.save({\n",
    "                \"video_model\": video_model.state_dict(),\n",
    "                \"audio_model\": audio_model.state_dict(),\n",
    "                \"behavior_label_to_id\": behavior_train.label_to_id,\n",
    "                \"emotion_label_to_id\": emotion_train.label_to_id,\n",
    "                \"sound_label_to_id\": sound_train.label_to_id,\n",
    "                \"best_epoch\": epoch+1,\n",
    "                \"best_acc\": best_avg_acc,\n",
    "                \"history\": history\n",
    "            }, \"pet_omni_best.pth\")\n",
    "            print(f\"  ğŸ’¾ Saved new best model! (Acc: {best_avg_acc:.4f})\")\n",
    "\n",
    "    # ê·¸ë˜í”„\n",
    "    print(\"\\nğŸ“ˆ Generating training history plot...\")\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.plot([h['acc_b'] for h in history], 'b-', label='Behavior', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Behavior Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.plot([h['acc_e'] for h in history], 'r-', label='Emotion', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Emotion Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot([h['acc_s'] for h in history], 'g-', label='Sound', linewidth=2)\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
    "    plt.title('Sound Accuracy'); plt.ylim(0, 1); plt.grid(True, alpha=0.3); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pet_omni_history.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"  âœ… Saved: pet_omni_history.png\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ Training Finished!\")\n",
    "    print(f\"  Best Average Acc: {best_avg_acc:.4f} ({best_avg_acc*100:.1f}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
