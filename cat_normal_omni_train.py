"""
cat_normal_omni.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ê³ ì–‘ì´ ì •ìƒ í–‰ë™ ë¶„ë¥˜ ëª¨ë¸ (Behavior / Emotion / Sound)
- Backbone: EfficientNet-V2-S (ì´ë¯¸ì§€), wav2vec2-base (ì˜¤ë””ì˜¤)
- Patella ì—†ìŒ (ê³ ì–‘ì´ ì „ìš© ë°ì´í„° ì—†ìŒ)
- Sound: cat_aggressive / cat_positive 2í´ë˜ìŠ¤ (ìƒ˜í”Œ ê·¹ì†Œ, ì˜¤ë²„ìƒ˜í”Œë§ í•„ìˆ˜)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""

import os, gc, random, shutil, json
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import librosa

from PIL import Image, ImageFile
from torch.utils.data import Dataset, DataLoader
from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights
from transformers import (
    Wav2Vec2ForSequenceClassification,
    Wav2Vec2FeatureExtractor,
    get_cosine_schedule_with_warmup,
)
from collections import defaultdict
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm

ImageFile.LOAD_TRUNCATED_IMAGES = True

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SEED = 42
random.seed(SEED); torch.manual_seed(SEED); np.random.seed(SEED)

BEHAVIOR_ROOT = "files/1_Animal_Behavior"
EMOTION_ROOT  = "files/2_Animal_emotions"
SOUND_ROOT    = "files/3_Animal_Sound"
WORK_DIR      = "files/work/cat_normal_dataset"

DEVICE      = "cuda:0" if torch.cuda.is_available() else "cpu"
EPOCHS      = 100
BATCH_SIZE  = 32          # EfficientNet 384px â†’ VRAM ê³ ë ¤
NUM_WORKERS = 12
SR          = 16000
MAX_AUDIO_LEN = SR * 5

IMG_SIZE   = 384          # EfficientNet-V2-S ê¶Œì¥
IMG_RESIZE = 416

LR_BACKBONE = 2e-5
LR_HEAD     = 2e-4
LR_AUDIO    = 1e-5
FREEZE_EPOCHS = 5
LABEL_SMOOTHING = 0.1
AUDIO_MODEL_NAME = "facebook/wav2vec2-base"

print(f"ğŸ± Cat Normal Omni | Device: {DEVICE}")
FEATURE_EXTRACTOR = Wav2Vec2FeatureExtractor.from_pretrained(AUDIO_MODEL_NAME)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLASSES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì§„ë‹¨ ê²°ê³¼ í´ë˜ìŠ¤ ëª©ë¡ ê³ ì • (collect ì‹œ ì´ ëª©ë¡ë§Œ ìˆ˜ì§‘)
CAT_BEHAVIOR_CLASSES = [
    "CAT_ARCH", "CAT_ARMSTRETCH", "CAT_FOOTPUSH", "CAT_GETDOWN",
    "CAT_GROOMING", "CAT_HEADING", "CAT_LAYDOWN", "CAT_LYING",
    "CAT_ROLL", "CAT_SITDOWN", "CAT_TAILING", "CAT_WALKRUN",
]  # 12í´ë˜ìŠ¤, max/minâ‰ˆ1.7x (ê· í˜• ì–‘í˜¸)

CAT_EMOTION_CLASSES = [
    "cat_relaxed", "cat_happy", "cat_attentive", "cat_sad",
]  # 4í´ë˜ìŠ¤ â€” cat_sad ê·¹ì†Œ(136ê°œ) â†’ ì˜¤ë²„ìƒ˜í”Œë§ í•„ìˆ˜

CAT_SOUND_CLASSES = [
    "cat_aggressive", "cat_positive",
]  # 2í´ë˜ìŠ¤, í•©ê³„ 55ê°œ â†’ ê·¹ì†Œ, ì˜¤ë²„ìƒ˜í”Œë§ìœ¼ë¡œ ë³´ì™„

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AUGMENTATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def augment_audio(waveform, p=0.7):
    if random.random() > p:
        return waveform
    waveform = librosa.effects.pitch_shift(waveform, sr=SR, n_steps=random.uniform(-2, 2))
    rate = random.uniform(0.85, 1.15)
    stretched = librosa.effects.time_stretch(waveform, rate=rate)
    stretched = stretched[:MAX_AUDIO_LEN] if len(stretched) > MAX_AUDIO_LEN \
                else np.pad(stretched, (0, MAX_AUDIO_LEN - len(stretched)))
    waveform = stretched * 0.99 + np.random.normal(0, 0.003, len(stretched))
    return waveform

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRANSFORMS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRANSFORM_TRAIN = transforms.Compose([
    transforms.Resize((IMG_RESIZE, IMG_RESIZE)),
    transforms.RandomCrop(IMG_SIZE),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])
TRANSFORM_VAL = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATASETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ImageDataset(Dataset):
    def __init__(self, task_dir, class_list, augment=False):
        self.label_to_id = {c: i for i, c in enumerate(class_list)}
        self.samples = []
        for cls in class_list:
            d = os.path.join(task_dir, cls)
            if not os.path.isdir(d): continue
            for f in os.listdir(d):
                if f.lower().endswith(('.jpg', '.png', '.jpeg')):
                    self.samples.append((os.path.join(d, f), cls))
        self.transform = TRANSFORM_TRAIN if augment else TRANSFORM_VAL
        print(f"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes")

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        p, c = self.samples[idx]
        return self.transform(Image.open(p).convert("RGB")), self.label_to_id[c]


class AudioDataset(Dataset):
    def __init__(self, task_dir, class_list, augment=False):
        self.label_to_id = {c: i for i, c in enumerate(class_list)}
        self.id_to_label = {i: c for i, c in enumerate(class_list)}
        self.augment = augment
        self.samples = []
        for cls in class_list:
            d = os.path.join(task_dir, cls)
            if not os.path.isdir(d): continue
            for f in os.listdir(d):
                if f.lower().endswith(('.wav', '.mp3', '.m4a')):
                    self.samples.append((os.path.join(d, f), cls))
        print(f"  ğŸ“Š {os.path.basename(task_dir)}: {len(self.samples)} samples, {len(self.label_to_id)} classes, augment={augment}")

    def __len__(self): return len(self.samples)
    def __getitem__(self, idx):
        p, c = self.samples[idx]
        try:
            w, _ = librosa.load(p, sr=SR, mono=True)
        except:
            w = np.zeros(MAX_AUDIO_LEN)
        if self.augment: w = augment_audio(w)
        w = w[:MAX_AUDIO_LEN] if len(w) > MAX_AUDIO_LEN else np.pad(w, (0, MAX_AUDIO_LEN - len(w)))
        inp = FEATURE_EXTRACTOR(w, sampling_rate=SR, return_tensors="pt")
        return {"input_values": inp.input_values.squeeze(0),
                "labels": torch.tensor(self.label_to_id[c], dtype=torch.long)}

def collate_audio(batch):
    return {"input_values": torch.stack([b["input_values"] for b in batch]),
            "labels":       torch.stack([b["labels"]       for b in batch])}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MODELS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _efficientnet_backbone():
    b = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)
    feat = b.classifier[1].in_features  # 1280
    b.classifier = nn.Identity()
    return b, feat

class ImageModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.backbone, feat = _efficientnet_backbone()
        self.head = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(feat, 512),
            nn.BatchNorm1d(512),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes),
        )
    def forward(self, x): return self.head(self.backbone(x))

class AudioModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(
            AUDIO_MODEL_NAME, num_labels=num_classes, ignore_mismatched_sizes=True
        )
    def forward(self, input_values, labels=None):
        return self.model(input_values=input_values, labels=labels)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATA PREPARATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _task_ready(name, class_list=None,
                img_exts=('.jpg', '.png', '.jpeg', '.wav', '.mp3', '.m4a')):
    """
    [FIX] í´ë” ì¡´ì¬ ì—¬ë¶€ë§Œ ë³´ë˜ ê¸°ì¡´ ë¡œì§ì„ ê°œì„ .
    class_list ê°€ ì£¼ì–´ì§€ë©´ ê° í´ë˜ìŠ¤ í´ë” ì•ˆì˜ ì‹¤ì œ íŒŒì¼ ìˆ˜ë¥¼ í•©ì‚°í•˜ì—¬
    1ê°œ ì´ìƒì¼ ë•Œë§Œ True ë¥¼ ë°˜í™˜í•œë‹¤.
    (ë‚ ì§œ í•˜ìœ„ í´ë”ê°€ ë‚¨ì•„ ìˆì–´ í´ë”ë§Œ ì¡´ì¬í•˜ëŠ” ê²½ìš° False ì²˜ë¦¬)
    """
    p = os.path.join(WORK_DIR, "train", name)
    if not os.path.isdir(p):
        return False
    search_dirs = []
    if class_list:
        for cls in class_list:
            d = os.path.join(p, cls)
            if os.path.isdir(d):
                search_dirs.append(d)
    else:
        for sub in os.listdir(p):
            d = os.path.join(p, sub)
            if os.path.isdir(d):
                search_dirs.append(d)
    total = sum(
        1 for d in search_dirs
        for f in os.listdir(d)
        if f.lower().endswith(img_exts)
    )
    return total > 0

def collect_and_split(src_root, task_name, class_list, oversample_min=0):
    """í´ë˜ìŠ¤ë³„ stratified split â†’ WORK_DIR ë³µì‚¬. oversample_min>0 ì´ë©´ train ì˜¤ë²„ìƒ˜í”Œë§."""
    rng = random.Random(SEED)
    class_files = defaultdict(list)
    for cls in class_list:
        d = os.path.join(src_root, cls)
        if not os.path.isdir(d): continue
        # [FIX] os.listdir() â†’ os.walk() : ë‚ ì§œ/ID í•˜ìœ„ í´ë”ê°€ ìˆëŠ” ê²½ìš°
        #       (1_Animal_Behavior ë“±) ì¬ê·€ íƒìƒ‰ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë¹ ì§ì—†ì´ ìˆ˜ì§‘.
        for root, _, files in os.walk(d):
            for f in files:
                if f.lower().endswith(('.jpg', '.png', '.jpeg', '.wav', '.mp3', '.m4a')):
                    class_files[cls].append(os.path.join(root, f))

    for split in ["train","val","test"]:
        for cls in class_list:
            os.makedirs(os.path.join(WORK_DIR, split, task_name, cls), exist_ok=True)

    for cls, paths in class_files.items():
        rng.shuffle(paths)
        n = len(paths)
        n_val  = max(1, int(n * 0.1))
        n_test = max(1, int(n * 0.1))
        n_train = n - n_val - n_test

        splits = {"train": paths[:n_train], "val": paths[n_train:n_train+n_val], "test": paths[n_train+n_val:]}

        # ì˜¤ë²„ìƒ˜í”Œë§ (soundì²˜ëŸ¼ ì†Œìˆ˜ í´ë˜ìŠ¤ì— í•„ìš”)
        if oversample_min > 0 and len(splits["train"]) < oversample_min:
            splits["train"] = rng.choices(splits["train"], k=oversample_min)

        for sname, sfiles in splits.items():
            dst_dir = os.path.join(WORK_DIR, sname, task_name, cls)
            for i, src in enumerate(sfiles):
                dst = os.path.join(dst_dir, f"{cls}_{i:05d}{os.path.splitext(src)[1]}")
                if not os.path.exists(dst): shutil.copy2(src, dst)

    print(f"  âœ… {task_name} prepared â†’ {WORK_DIR}")

def prepare_datasets():
    if not _task_ready("behavior", CAT_BEHAVIOR_CLASSES):
        print("ğŸ“¦ Preparing behavior (cat)...")
        collect_and_split(BEHAVIOR_ROOT, "behavior", CAT_BEHAVIOR_CLASSES)
    else: print("âœ… behavior ready")

    if not _task_ready("emotion", CAT_EMOTION_CLASSES):
        print("ğŸ“¦ Preparing emotion (cat)...")
        # cat_sad: 136ê°œ â†’ ì˜¤ë²„ìƒ˜í”Œë§ìœ¼ë¡œ ìµœì†Œ 400ê°œ ë³´ì¥
        collect_and_split(EMOTION_ROOT, "emotion", CAT_EMOTION_CLASSES, oversample_min=400)
    else: print("âœ… emotion ready")

    if not _task_ready("sound", CAT_SOUND_CLASSES):
        print("ğŸ“¦ Preparing sound (cat)...")
        # cat ìŒì„± ê·¹ì†Œ (55ê°œ í•©ê³„) â†’ ê°•í•œ ì˜¤ë²„ìƒ˜í”Œë§
        collect_and_split(SOUND_ROOT, "sound", CAT_SOUND_CLASSES, oversample_min=150)
    else: print("âœ… sound ready")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_loader(ds, shuffle, is_audio=False, is_train=True):
    workers = 2 if is_audio else NUM_WORKERS
    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle,
                      num_workers=workers, pin_memory=True,
                      persistent_workers=(workers > 0), prefetch_factor=2,
                      multiprocessing_context="fork" if workers > 0 else None,
                      collate_fn=collate_audio if is_audio else None,
                      drop_last=is_train)   # [FIX 1] valì€ drop_last=False â†’ ZeroDivisionError ë°©ì§€

def get_class_weights(ds, class_list):
    labels = [ds.label_to_id[c] for _, c in ds.samples]
    w = compute_class_weight('balanced', classes=np.arange(len(class_list)), y=labels)
    return torch.tensor(w, dtype=torch.float).to(DEVICE)

def clear(): gc.collect(); torch.cuda.empty_cache()

def _save_history_plot(history, best_acc):
    """ë§¤ epoch í˜¸ì¶œ â†’ í•™ìŠµ ì¤‘ë‹¨ ì‹œì—ë„ ë§ˆì§€ë§‰ epochê¹Œì§€ì˜ ê³¡ì„  í™•ì¸ ê°€ëŠ¥"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    for ax, key, title, color in zip(axes,
            ["behavior_acc","emotion_acc","sound_acc"],
            ["Behavior","Emotion","Sound"],["b","g","r"]):
        ax.plot([h[key] for h in history], color=color, linewidth=2)
        ax.set_title(f"Cat {title} Val Acc"); ax.set_ylim(0,1); ax.grid(True, alpha=0.3)
    plt.suptitle(f"Cat Normal Omni | Best Avg {best_acc*100:.1f}%", fontweight="bold")
    plt.tight_layout()
    plt.savefig("cat_normal_omni_history.png", dpi=150, bbox_inches="tight")
    plt.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAINING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train():
    prepare_datasets()

    # â”€â”€ ëª¨ë¸ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    behavior_model = ImageModel(len(CAT_BEHAVIOR_CLASSES))
    emotion_model  = ImageModel(len(CAT_EMOTION_CLASSES))
    audio_model    = AudioModel(len(CAT_SOUND_CLASSES))

    # â”€â”€ Optimizers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def img_opt(m):
        return torch.optim.AdamW([
            {"params": m.backbone.parameters(), "lr": LR_BACKBONE, "weight_decay": 1e-4},
            {"params": m.head.parameters(),     "lr": LR_HEAD,     "weight_decay": 1e-4},
        ])

    behavior_opt = img_opt(behavior_model)
    emotion_opt  = img_opt(emotion_model)
    audio_opt    = torch.optim.AdamW(audio_model.parameters(), lr=LR_AUDIO, weight_decay=0.01)

    behavior_scaler = torch.amp.GradScaler("cuda")
    emotion_scaler  = torch.amp.GradScaler("cuda")
    audio_scaler    = torch.amp.GradScaler("cuda")

    # â”€â”€ [FIX 3] Dataset / DataLoader / criterionì„ ë£¨í”„ ë°–ì—ì„œ 1íšŒë§Œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€
    # Worker í”„ë¡œì„¸ìŠ¤ ìƒì„±/ì†Œë©¸ ë°˜ë³µ ì œê±° â†’ persistent_workers íš¨ê³¼ ì‹¤í˜„
    print("\nğŸ“‚ Building datasets & loaders (train)...")
    bds = ImageDataset(os.path.join(WORK_DIR,"train","behavior"), CAT_BEHAVIOR_CLASSES, augment=True)
    eds = ImageDataset(os.path.join(WORK_DIR,"train","emotion"),  CAT_EMOTION_CLASSES,  augment=True)
    sds = AudioDataset(os.path.join(WORK_DIR,"train","sound"),    CAT_SOUND_CLASSES,    augment=True)

    bl = make_loader(bds, shuffle=True,  is_audio=False, is_train=True)
    el = make_loader(eds, shuffle=True,  is_audio=False, is_train=True)
    sl = make_loader(sds, shuffle=True,  is_audio=True,  is_train=True)

    # criterionë„ 1íšŒë§Œ ê³„ì‚°
    criterion_b = nn.CrossEntropyLoss(weight=get_class_weights(bds, CAT_BEHAVIOR_CLASSES), label_smoothing=LABEL_SMOOTHING)
    criterion_e = nn.CrossEntropyLoss(weight=get_class_weights(eds, CAT_EMOTION_CLASSES),  label_smoothing=LABEL_SMOOTHING)
    criterion_s = nn.CrossEntropyLoss(weight=get_class_weights(sds, CAT_SOUND_CLASSES))

    # â”€â”€ Scheduler (cosine warmup) â€” Dataset ìƒì„± ì´í›„ len(ds) ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚° â”€â”€â”€
    def img_sched(opt, n_samples):
        steps = (n_samples // BATCH_SIZE) * EPOCHS
        return get_cosine_schedule_with_warmup(opt, num_warmup_steps=max(1, steps//50), num_training_steps=max(1, steps))

    behavior_sched = img_sched(behavior_opt, len(bds))
    emotion_sched  = img_sched(emotion_opt,  len(eds))
    audio_sched    = get_cosine_schedule_with_warmup(
        audio_opt,
        num_warmup_steps=max(1, (len(sds)//BATCH_SIZE)*2),
        num_training_steps=max(1, (len(sds)//BATCH_SIZE)*EPOCHS),
    )

    # [FIX 1] val ë¡œë”ëŠ” is_train=False â†’ drop_last=False â†’ ZeroDivisionError ë°©ì§€
    print("\nğŸ“‚ Building datasets & loaders (val)...")
    bds_val = ImageDataset(os.path.join(WORK_DIR,"val","behavior"), CAT_BEHAVIOR_CLASSES, augment=False)
    eds_val = ImageDataset(os.path.join(WORK_DIR,"val","emotion"),  CAT_EMOTION_CLASSES,  augment=False)
    sds_val = AudioDataset(os.path.join(WORK_DIR,"val","sound"),    CAT_SOUND_CLASSES,    augment=False)

    bl_val = make_loader(bds_val, shuffle=False, is_audio=False, is_train=False)
    el_val = make_loader(eds_val, shuffle=False, is_audio=False, is_train=False)
    sl_val = make_loader(sds_val, shuffle=False, is_audio=True,  is_train=False)

    # â”€â”€ Freeze backbone ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for m in [behavior_model, emotion_model]:
        for p in m.backbone.parameters(): p.requires_grad = False

    best_acc, history = 0.0, []

    for epoch in range(EPOCHS):
        print(f"\n{'='*55}\nEpoch {epoch+1}/{EPOCHS}\n{'='*55}")

        # Gradual unfreeze
        if epoch == FREEZE_EPOCHS:
            for m in [behavior_model, emotion_model]:
                for p in m.backbone.parameters(): p.requires_grad = True
            print(f"  ğŸ”“ Backbone unfrozen at epoch {epoch+1}")

        # â”€â”€ 1. Behavior â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ¾ Training Behavior (cat)...")
        behavior_model.to(DEVICE).train()
        loss_b, corr_b, tot_b = 0, 0, 0
        for imgs, labels in tqdm(bl, desc="Behavior", leave=False):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            behavior_opt.zero_grad()
            with torch.amp.autocast("cuda"):
                logits = behavior_model(imgs)
                loss = criterion_b(logits, labels)
            behavior_scaler.scale(loss).backward()
            behavior_scaler.unscale_(behavior_opt)
            torch.nn.utils.clip_grad_norm_(behavior_model.parameters(), 1.0)
            # [FIX 2] optimizerê°€ ì‹¤ì œë¡œ ì‹¤í–‰ëì„ ë•Œë§Œ scheduler.step()
            prev_scale = behavior_scaler.get_scale()
            behavior_scaler.step(behavior_opt); behavior_scaler.update()
            if behavior_scaler.get_scale() == prev_scale:
                behavior_sched.step()
            loss_b += loss.item(); corr_b += (logits.argmax(1)==labels).sum().item(); tot_b += labels.size(0)
        print(f"  â†’ Loss: {loss_b/len(bl):.4f} | Train Acc: {corr_b/tot_b*100:.1f}%")
        behavior_model.cpu(); clear()

        # â”€â”€ 2. Emotion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ˜Š Training Emotion (cat)...")
        emotion_model.to(DEVICE).train()
        loss_e, corr_e, tot_e = 0, 0, 0
        for imgs, labels in tqdm(el, desc="Emotion", leave=False):
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            emotion_opt.zero_grad()
            with torch.amp.autocast("cuda"):
                logits = emotion_model(imgs)
                loss = criterion_e(logits, labels)
            emotion_scaler.scale(loss).backward()
            emotion_scaler.unscale_(emotion_opt)
            torch.nn.utils.clip_grad_norm_(emotion_model.parameters(), 1.0)
            # [FIX 2] optimizerê°€ ì‹¤ì œë¡œ ì‹¤í–‰ëì„ ë•Œë§Œ scheduler.step()
            prev_scale = emotion_scaler.get_scale()
            emotion_scaler.step(emotion_opt); emotion_scaler.update()
            if emotion_scaler.get_scale() == prev_scale:
                emotion_sched.step()
            loss_e += loss.item(); corr_e += (logits.argmax(1)==labels).sum().item(); tot_e += labels.size(0)
        print(f"  â†’ Loss: {loss_e/len(el):.4f} | Train Acc: {corr_e/tot_e*100:.1f}%")
        emotion_model.cpu(); clear()

        # â”€â”€ 3. Sound â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ”Š Training Sound (cat)...")
        audio_model.to(DEVICE).train()
        loss_s, corr_s, tot_s = 0, 0, 0
        for batch in tqdm(sl, desc="Sound", leave=False):
            inp, labels = batch["input_values"].to(DEVICE), batch["labels"].to(DEVICE)
            audio_opt.zero_grad()
            with torch.amp.autocast("cuda"):
                out = audio_model(inp)
                loss = criterion_s(out.logits, labels)
            audio_scaler.scale(loss).backward()
            audio_scaler.unscale_(audio_opt)
            torch.nn.utils.clip_grad_norm_(audio_model.parameters(), 1.0)
            # [FIX 2] optimizerê°€ ì‹¤ì œë¡œ ì‹¤í–‰ëì„ ë•Œë§Œ scheduler.step()
            prev_scale = audio_scaler.get_scale()
            audio_scaler.step(audio_opt); audio_scaler.update()
            if audio_scaler.get_scale() == prev_scale:
                audio_sched.step()
            loss_s += loss.item(); corr_s += (out.logits.argmax(1)==labels).sum().item(); tot_s += labels.size(0)
        print(f"  â†’ Loss: {loss_s/len(sl):.4f} | Train Acc: {corr_s/tot_s*100:.1f}%")
        audio_model.cpu(); clear()

        # â”€â”€ Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ” Validation...")
        accs = {}
        for name, model, val_loader, is_audio in [
            ("behavior", behavior_model, bl_val, False),
            ("emotion",  emotion_model,  el_val, False),
            ("sound",    audio_model,    sl_val, True),
        ]:
            model.to(DEVICE).eval()
            c, t = 0, 0
            with torch.no_grad():
                for batch in val_loader:
                    if is_audio:
                        inp = batch["input_values"].to(DEVICE)
                        lbl = batch["labels"].to(DEVICE)
                        p = model(inp).logits.argmax(1)
                    else:
                        imgs, lbl = batch[0].to(DEVICE), batch[1].to(DEVICE)
                        p = model(imgs).argmax(1)
                    c += (p==lbl).sum().item(); t += lbl.size(0)
            accs[name] = c/t if t > 0 else 0.0   # [FIX 1] ì¶”ê°€ ë°©ì–´: val ìƒ˜í”Œ 0ê°œ ëŒ€ë¹„
            print(f"  {name:10s}: {accs[name]*100:.1f}%")
            model.cpu(); clear()

        avg = sum(accs.values()) / len(accs)
        print(f"  Average: {avg*100:.1f}%")
        history.append({"epoch": epoch+1, **{k+"_acc": v for k,v in accs.items()}, "avg_acc": avg})

        if avg > best_acc:
            best_acc = avg
            torch.save({
                "behavior_model": behavior_model.state_dict(),
                "emotion_model":  emotion_model.state_dict(),
                "audio_model":    audio_model.state_dict(),
                "cat_behavior_classes": CAT_BEHAVIOR_CLASSES,
                "cat_emotion_classes":  CAT_EMOTION_CLASSES,
                "cat_sound_classes":    CAT_SOUND_CLASSES,
                "best_epoch": epoch+1, "best_acc": best_acc, "history": history,
            }, "cat_normal_omni_best.pth")
            print(f"  ğŸ’¾ Saved! (Avg {best_acc*100:.1f}%)")

        # ë§¤ epoch í•™ìŠµ ê³¡ì„  ë®ì–´ì“°ê¸° ì €ì¥
        _save_history_plot(history, best_acc)

    print(f"\nğŸ‰ Done! Best Avg Acc: {best_acc*100:.1f}%")

if __name__ == "__main__":
    train()
