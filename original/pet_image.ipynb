{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf25c8d8",
   "metadata": {},
   "source": [
    "## 1. Pet Behaivor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe78184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêæ Pet Behavior | cuda:1 | 100,000 samples | Epochs: 100\n",
      "üîç Collecting all images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total: 757,113 images, 25 classes\n",
      "üéØ ÌÅ¥ÎûòÏä§Îãπ ÏÉòÌîå: 4,000Í∞ú\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# 0. ÏÑ§Ï†ï (ÌÜµÌï©)\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = \"files/1_Animal_Behavior\"\n",
    "WORK_DIR = \"files/work/behavior_dataset\"\n",
    "MAX_SAMPLES = 100_000  # Ï¥ù 10Îßå\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "# NUM_WORKERS ÌÜµÌï© Í¥ÄÎ¶¨\n",
    "NUM_WORKERS = 4\n",
    "PERSISTENT_WORKERS = True\n",
    "PIN_MEMORY = True\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üêæ Pet Behavior | {DEVICE} | {MAX_SAMPLES:,} samples | Epochs: {EPOCHS}\")\n",
    "\n",
    "# ResNet18 Ï†ÑÏ≤òÎ¶¨\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomErasing(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class BehaviorDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = []\n",
    "        self.label_to_id = {}\n",
    "        label_id = 0\n",
    "        \n",
    "        for label in sorted(os.listdir(root)):\n",
    "            label_dir = os.path.join(root, label)\n",
    "            if os.path.isdir(label_dir):\n",
    "                self.label_to_id[label] = label_id\n",
    "                label_id += 1\n",
    "                \n",
    "                for img_file in os.listdir(label_dir):\n",
    "                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.samples.append((os.path.join(label_dir, img_file), label))\n",
    "        \n",
    "        print(f\"üìÅ {os.path.basename(root)}: {len(self.samples):,} images, {len(self.label_to_id)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_name = self.samples[idx]\n",
    "        label_id = self.label_to_id[label_name]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (128, 128, 128))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label_id\n",
    "\n",
    "def split_dataset():\n",
    "    \"\"\"75Îßå ‚Üí 10Îßå Í∑†Îì± ÏÉòÌîåÎßÅ\"\"\"\n",
    "    if os.path.exists(WORK_DIR):\n",
    "        shutil.rmtree(WORK_DIR)\n",
    "    \n",
    "    os.makedirs(os.path.join(WORK_DIR, \"train\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(WORK_DIR, \"val\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(WORK_DIR, \"test\"), exist_ok=True)\n",
    "\n",
    "    print(\"üîç Collecting all images...\")\n",
    "    all_samples = []\n",
    "    class_dirs = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT, d))])\n",
    "    \n",
    "    for class_dir in tqdm(class_dirs, desc=\"Classes\"):\n",
    "        class_path = os.path.join(DATA_ROOT, class_dir)\n",
    "        imgs = []\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            imgs.extend([os.path.join(root, f) for f in files \n",
    "                        if f.lower().endswith(('.jpg','.jpeg','.png'))])\n",
    "        all_samples.extend([(class_dir, img) for img in imgs])\n",
    "    \n",
    "    print(f\"‚úÖ Total: {len(all_samples):,} images, {len(class_dirs)} classes\")\n",
    "\n",
    "    # ÌÅ¥ÎûòÏä§Î≥Ñ Í∑†Îì± ÏÉòÌîåÎßÅ\n",
    "    label_count = Counter(label for label, _ in all_samples)\n",
    "    samples_per_class = MAX_SAMPLES // len(class_dirs)\n",
    "    print(f\"üéØ ÌÅ¥ÎûòÏä§Îãπ ÏÉòÌîå: {samples_per_class:,}Í∞ú\")\n",
    "\n",
    "    balanced_samples = []\n",
    "    for label in class_dirs:\n",
    "        label_imgs = [img for l, img in all_samples if l == label]\n",
    "        n = min(samples_per_class, len(label_imgs))\n",
    "        balanced_samples.extend(random.sample(label_imgs, n))\n",
    "    \n",
    "    print(f\"üìä Í∑†Ìòï ÏÉòÌîåÎßÅ ÏôÑÎ£å: {len(balanced_samples):,}Í∞ú\")\n",
    "\n",
    "    # 8:1:1 ÌÅ¥ÎûòÏä§Î≥Ñ stratified split\n",
    "    label_samples = defaultdict(list)\n",
    "    for label, img in all_samples:\n",
    "        if img in balanced_samples:\n",
    "            label_samples[label].append(img)\n",
    "    \n",
    "    train_imgs, val_imgs, test_imgs = [], [], []\n",
    "    for label, imgs in label_samples.items():\n",
    "        random.shuffle(imgs)\n",
    "        n_train = int(len(imgs) * 0.8)\n",
    "        n_val = int(len(imgs) * 0.1)\n",
    "        \n",
    "        train_imgs.extend([(label, p) for p in imgs[:n_train]])\n",
    "        val_imgs.extend([(label, p) for p in imgs[n_train:n_train+n_val]])\n",
    "        test_imgs.extend([(label, p) for p in imgs[n_train+n_val:]])\n",
    "    \n",
    "    # ÌååÏùº Î≥µÏÇ¨\n",
    "    for split_name, files in [('train', train_imgs), ('val', val_imgs), ('test', test_imgs)]:\n",
    "        print(f\"üìÇ Copying {split_name}: {len(files):,}\")\n",
    "        for label, src_path in tqdm(files, desc=split_name):\n",
    "            dst_dir = os.path.join(WORK_DIR, split_name, label)\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            shutil.copy(src_path, os.path.join(dst_dir, os.path.basename(src_path)))\n",
    "    \n",
    "    print(\"‚úÖ Dataset split complete!\")\n",
    "\n",
    "def plot_history(history):\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    train_losses = [h['train_loss'] for h in history]\n",
    "    val_losses = [h['val_loss'] for h in history]\n",
    "    val_accs = [h['val_acc'] for h in history]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(epochs, train_losses, 'b-o', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, val_losses, 'r-s', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_title('Behavior Classification Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(epochs, val_accs, 'g-^', linewidth=3, markersize=8)\n",
    "    best_epoch = np.argmax(val_accs) + 1\n",
    "    best_acc = max(val_accs)\n",
    "    axes[1].axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_acc:.3f}')\n",
    "    axes[1].set_title('Validation Accuracy')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pet_behavior_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üìä Best Val Acc: {best_acc:.4f} (Epoch {best_epoch})\")\n",
    "\n",
    "def train_behavior():\n",
    "    split_dataset()\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ÏÖã\n",
    "    train_ds = BehaviorDataset(os.path.join(WORK_DIR, 'train'), transform_train)\n",
    "    val_ds = BehaviorDataset(os.path.join(WORK_DIR, 'val'), transform_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                            num_workers=NUM_WORKERS, persistent_workers=PERSISTENT_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS//2, persistent_workers=PERSISTENT_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    \n",
    "    num_classes = len(train_ds.label_to_id)\n",
    "    print(f\"\\nüöÄ ResNet18 | {num_classes} classes | {len(train_loader)} train batches\")\n",
    "    \n",
    "    # ResNet18\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_acc = 0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Train E{epoch+1:2d}\")\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            imgs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                imgs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                pred = outputs.argmax(1)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch+1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"\\nüìä E{epoch+1:3d} | Loss: {avg_train_loss:.4f}/{avg_val_loss:.4f} | \"\n",
    "              f\"Acc: {val_acc:.4f} | {epoch_time:.0f}s\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'label_to_id': train_ds.label_to_id,\n",
    "                'best_epoch': epoch+1,\n",
    "                'best_acc': best_acc,\n",
    "                'history': history\n",
    "            }, 'pet_behavior_best.pth')\n",
    "            print(f\"üíæ BEST: {best_acc:.4f}\")\n",
    "    \n",
    "    plot_history(history)\n",
    "    print(f\"\\nüéâ Pet Behavior ÌïôÏäµ ÏôÑÎ£å! pet_behavior_best_ex.pth Ï†ÄÏû•\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    train_behavior()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
